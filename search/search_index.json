{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"CCA/PLS Toolkit This is a MATLAB toolkit to incorporate Canonical Correlation Analysis (CCA) , Partial Least Squares (PLS) and their different variants to investigate multivariate associations between multiple modalities of data, e.g., brain imaging and behaviour. These models find pairs of weights (one weight for each data modality) such that the linear combination of the brain and behavioural variables maximise correlation (CCA) or covariance (PLS). The toolkit includes various options for CCA/PLS models (e.g., standard CCA, standard PLS, regularized CCA, sparse PLS) and analysis frameworks (e.g., statistical framework, machine learning framework). It can also perform Principal Component Analysis (PCA) to reduce the dimensionality of the data before entering them into standard CCA analysis ( PCA-CCA ). Although there are methods to estimate all weights (or associative effects ) for most CCA/PLS models at once, the toolkit uses an interative solution to be able to optimize the hyperparameters of the model (i.e., number of principal components or regularization parameters) for each associative effect independently. In such iterative solution, the CCA/PLS model estimates one pair of weights (one weight for each data modality) at a time. These associative effects are then removed from the data (by a process called deflation ) and the same process is repeated multiple times. The iterative solution also allows to estimate different PLS variants by choosing a specific deflation. For a short theoretical introduction to the CCA/PLS models, analytic frameworks and deflation methods used in the toolkit, see Background . For further reading, see: Shawe-Taylor J, Cristianini N (2004) Kernel Methods for Pattern Analysis. Cambridge: Cambridge University Press. Rosipal R, Kramer N (2006) Overview and Recent Advances in Partial Least Squares. In: Saunders, C., Grobelnik, M.m Gunn, S., Shawe-Taylor, J. (eds) Subspace, Latent Struct Featur Sel. Berlin, Heidelberg: Springer Berlin Heidelberg, pp 34-51. Krishnan A, Williams LJ, McIntosh AR, Abdi H (2011) Partial Least Squares (PLS) methods for neuroimaging: A tutorial and review. Neuroimage. 56: 455-475. Monteiro JM, Rao A, Shawe-Taylor J & Mourao-Miranda J (2016) A multiple hold-out framework for Sparse Partial Least Squares. J. Neurosci. Methods 271, 182-194. Mihalik A, Ferreira FS, Moutoussis M et al. (2020) Multiple Holdouts With Stability: Improving the Generalizability of Machine Learning Analyses of Brain-Behavior Relationships. Biol. Psychiatry 87, 368-376. Winkler AM, Renaud O, Smith SM, Nichols TE (2020) Permutation inference for canonical correlation analysis. Neuroimage 220, 117065 Mihalik A, Chapman J, Adams RA et al. (2022) Canonical Correlation Analysis and Partial Least Squares for identifying brain-behaviour associations: a tutorial and a comparative study. Biol. Psychiatry Cogn. Neurosci. Neuroimaging doi: https://doi.org/10.1016/j.bpsc.2022.07.012 Please use the menu on the left to get started with the toolkit. Contributors Agoston Mihalik - main developer (former at UCL, now at University of Cambridge, UK) Nils Winter (University of M\u00fcnster, Germany) Fabio Ferreira (former at UCL, now at Imperial College London, UK) James Chapman (UCL, UK) Janaina Mourao-Miranda - Principal Investigator (UCL, UK) Some of the code used in the toolkit was developed by Joao Monteiro who was a PhD student at UCL (currently a data scientist at Heni). We wish to thank members and collaborators of the Machine Learning & Neuroimaging Laboratory for testing the toolkit and providing invaluable feedback. We would particularly like to acknowledge Eliana Nicolaisen, Cemre Zor, Konstantinos Tsirlis, Taiane Ramos and Richard Nguyen. Feel free to report any bugs under cca-pls-toolkit@cs.ucl.ac.uk or by creating an issue here . Pull requests are also welcome to https://github.com/anaston/cca_pls_toolkit , however, unfortunately we don't have the resources to provide general user support. Acknowledgements The CCA/PLS toolkit was developed at the Machine Learning & Neuroimaging Laboratory (MLNL) , Centre for Medical Imaging Computing, Computer Science Department, University College London, UK. The development of the toolkit was supported by the Wellcome Trust (grant number WT102845/Z/13/Z). License This project is licensed under the terms of the GNU General Public License v3.0 license. Copyright \u00a9 2022 University College London","title":"Home"},{"location":"#ccapls-toolkit","text":"This is a MATLAB toolkit to incorporate Canonical Correlation Analysis (CCA) , Partial Least Squares (PLS) and their different variants to investigate multivariate associations between multiple modalities of data, e.g., brain imaging and behaviour. These models find pairs of weights (one weight for each data modality) such that the linear combination of the brain and behavioural variables maximise correlation (CCA) or covariance (PLS). The toolkit includes various options for CCA/PLS models (e.g., standard CCA, standard PLS, regularized CCA, sparse PLS) and analysis frameworks (e.g., statistical framework, machine learning framework). It can also perform Principal Component Analysis (PCA) to reduce the dimensionality of the data before entering them into standard CCA analysis ( PCA-CCA ). Although there are methods to estimate all weights (or associative effects ) for most CCA/PLS models at once, the toolkit uses an interative solution to be able to optimize the hyperparameters of the model (i.e., number of principal components or regularization parameters) for each associative effect independently. In such iterative solution, the CCA/PLS model estimates one pair of weights (one weight for each data modality) at a time. These associative effects are then removed from the data (by a process called deflation ) and the same process is repeated multiple times. The iterative solution also allows to estimate different PLS variants by choosing a specific deflation. For a short theoretical introduction to the CCA/PLS models, analytic frameworks and deflation methods used in the toolkit, see Background . For further reading, see: Shawe-Taylor J, Cristianini N (2004) Kernel Methods for Pattern Analysis. Cambridge: Cambridge University Press. Rosipal R, Kramer N (2006) Overview and Recent Advances in Partial Least Squares. In: Saunders, C., Grobelnik, M.m Gunn, S., Shawe-Taylor, J. (eds) Subspace, Latent Struct Featur Sel. Berlin, Heidelberg: Springer Berlin Heidelberg, pp 34-51. Krishnan A, Williams LJ, McIntosh AR, Abdi H (2011) Partial Least Squares (PLS) methods for neuroimaging: A tutorial and review. Neuroimage. 56: 455-475. Monteiro JM, Rao A, Shawe-Taylor J & Mourao-Miranda J (2016) A multiple hold-out framework for Sparse Partial Least Squares. J. Neurosci. Methods 271, 182-194. Mihalik A, Ferreira FS, Moutoussis M et al. (2020) Multiple Holdouts With Stability: Improving the Generalizability of Machine Learning Analyses of Brain-Behavior Relationships. Biol. Psychiatry 87, 368-376. Winkler AM, Renaud O, Smith SM, Nichols TE (2020) Permutation inference for canonical correlation analysis. Neuroimage 220, 117065 Mihalik A, Chapman J, Adams RA et al. (2022) Canonical Correlation Analysis and Partial Least Squares for identifying brain-behaviour associations: a tutorial and a comparative study. Biol. Psychiatry Cogn. Neurosci. Neuroimaging doi: https://doi.org/10.1016/j.bpsc.2022.07.012 Please use the menu on the left to get started with the toolkit.","title":"CCA/PLS Toolkit"},{"location":"#contributors","text":"Agoston Mihalik - main developer (former at UCL, now at University of Cambridge, UK) Nils Winter (University of M\u00fcnster, Germany) Fabio Ferreira (former at UCL, now at Imperial College London, UK) James Chapman (UCL, UK) Janaina Mourao-Miranda - Principal Investigator (UCL, UK) Some of the code used in the toolkit was developed by Joao Monteiro who was a PhD student at UCL (currently a data scientist at Heni). We wish to thank members and collaborators of the Machine Learning & Neuroimaging Laboratory for testing the toolkit and providing invaluable feedback. We would particularly like to acknowledge Eliana Nicolaisen, Cemre Zor, Konstantinos Tsirlis, Taiane Ramos and Richard Nguyen. Feel free to report any bugs under cca-pls-toolkit@cs.ucl.ac.uk or by creating an issue here . Pull requests are also welcome to https://github.com/anaston/cca_pls_toolkit , however, unfortunately we don't have the resources to provide general user support.","title":"Contributors"},{"location":"#acknowledgements","text":"The CCA/PLS toolkit was developed at the Machine Learning & Neuroimaging Laboratory (MLNL) , Centre for Medical Imaging Computing, Computer Science Department, University College London, UK. The development of the toolkit was supported by the Wellcome Trust (grant number WT102845/Z/13/Z).","title":"Acknowledgements"},{"location":"#license","text":"This project is licensed under the terms of the GNU General Public License v3.0 license. Copyright \u00a9 2022 University College London","title":"License"},{"location":"analysis/","text":"Dependencies There is only one necessary dependency (PALM Toolbox) to run CCA/PLS analyses. In short, the PALM toolbox allows to use restricted permutations based on the exchangeability block structure of the data (i.e., which examples are allowed to be exchanged or not). The exchangeability block structure is also used for stratified partitioning of the data (i.e., some examples are kept in the same data splits). For further information on exchangeability blocks, see the section Data or Winkler et al 2015 and the PALM toolbox . Overview As illustrated in the figure below, the analysis can be divided into eight operations. Here we describe these operations and the contents of the output files they create: Initialization : setting and saving the analysis configuration into cfg*.mat file including a cfg structure. For details on cfg , see the section Configuration for analysis . Data splitting : creating training and test sets of the data and saving outputs into outmat*.mat and inmat*.mat files. The outmat*.mat file includes: otrid : 2D logical array defining the training sets of the outer data splits (rows corresponding to examples and columns corresponding to the outer splits), oteid : 2D logical array defining the test sets of the outer data splits (rows corresponding to examples and columns corresponding to the outer splits). The inmat*.mat file includess: itrid : 3D logical array defining the training sets of the inner data splits (rows corresponding to examples, columns corresponding to the outer splits, and 3rd dimension corresponding to the inner splits), iteid : 3D logical array defining the test sets of the inner data splits (rows corresponding to examples, columns corresponding to the outer splits, and 3rd dimension corresponding to the inner splits). Preprocessing : imputing, z-scoring and deconfounding the data and saving outputs into preproc*.mat . There are separate preproc*.mat files for each data modality, outer and inner splits. For instance, preprocx_split_1_subsample_1*.mat saves resulst of the preprocessing for the first outer and inner split of the data modality \\(\\mathbf{X}\\) . Depending on the preprocessing strategy, this file can include up to three variables: mu : numeric array storing the mean values of the features, sigma : numeric array storing the standard deviations of the features, beta : 2D numeric array storing the regression coefficients from deconfounding (rows corresponding to confounds, columns corresponding to features). Singular Value Decomposition (SVD) of the data and saving outputs into *svd*.mat . There are separate *svd*.mat files for each data modality, training and test set of the outer and inner splits. For instance, tr_svdx_split_1_subsample_1*.mat saves the SVD results of the training set of the first outer and inner split of data modality \\(\\mathbf{X}\\) . The tr_svd*.mat files include: VX (or VY ): 2D numeric array storing the right singular vectors for data modality \\(\\mathbf{X}\\) (or \\(\\mathbf{Y}\\) ) (rows corresponding to examples, columns corresponding to singular vectors), RX (or RY ): 2D numeric array storing the principal components for data modality \\(\\mathbf{X}\\) (or \\(\\mathbf{Y}\\) ) (rows corresponding to examples, columns correponding to principal components), LX (or LY ): numeric array storing the squared singular values for data modality \\(\\mathbf{X}\\) (or \\(\\mathbf{Y}\\) ). The te_svd*.mat files include: RX (or RY ): 2D numeric array storing the test data transformed into principal component space for data modality \\(\\mathbf{X}\\) (or \\(\\mathbf{Y}\\) ) (rows corresponding to examples, columns corresponding to principal components). Grid search : hyperparameter optimization using a grid search and saving outputs into grid*.mat . Before cleaning up the intermediate files of the analysis (see cleanup_files ), there are separate grid*.mat files for each hyperparameter combination and outer split. For instance, grid_split_1_L1x_1_L1y_1*.mat saves the results of the first outer split for the hyperparameter combination \\(c_x=c_y=1\\) (for details, see here ). After cleaning up, all grid search results will be compiled into allgrid*.mat . This file includes all the metrics that are used for evaluating the CCA/PLS model in the inner splits (for details, see cfg.machine.metric here ), for instance: correl : 2D numeric array storing the out-of-sample correlations in the validations sets (rows corresponding to outers splits and hyperparameters, columns corresponding to inner splits), simwx : 3D numeric array storing the similarity of \\(\\mathbf{w}_x\\) between the traingin sets of the inner splits (rows corresponding to outers splits and hyperparameters, columns corresponding to inner splits, 3rd dimension corresponding to pairwise comparisons), simwy : 3D numeric array storing the similarity of \\(\\mathbf{w}_y\\) between the training sets of the inner splits (rows corresponding to outers splits and hyperparameters, columns corresponding to inner splits, 3rd dimension corresponding to pairwise comparisons). Training/testing : setting hyperparameters and saving these into param*.mat . This file includes: param : a structure array with each structure storing the best (or fixed) hyperparameter combination for a particular outer split. fitting models on optimization sets (i.e., outer split), assessing the model weights on holdout sets and saving outputs into model*.mat . This file includes the model weights, \\(\\mathbf{w}_x\\) and \\(\\mathbf{w}_y\\) and all the metrics that are used for evaluating the CCA/PLS model in the outer splits (for details, see cfg.machine.metric here ), for instance: wX and wY : 2D numeric arrays storing the models weights (rows corresponding to outer splits, columns corresponding to features), correl : numeric array storing the out-of-sample correlations in the holdout sets, simwx : 2D numeric array storing the similarity of \\(\\mathbf{w}_x\\) between the training sets of the outer splits (rows corresponding to outer splits, columns corresponding to pairwise comparisons), simwy : 2D numeric array storing the similarity of \\(\\mathbf{w}_y\\) between the training sets of the outer splits (rows corresponding to outer splits, columns corresponding to pairwise comparisons). Permutation test : permutation testing and saving outputs into perm*.mat . Before cleaning up the intermediate files of the analysis (see cleanup_files ), there are separate perm*.mat files for each permutation. For instance, perm_0001*.mat saves the results of the first permutation. After cleaning up, all permutation results will be compiled into perm*.mat . This file includes all the metrics that are used for evaluating the CCA/PLS model in the outer splits (for details, see cfg.machine.metric here ), for instance: correl : 2D numeric array storing the out-of-sample correlations (rows corresponding to outer splits, columns corresponding to permutations), simwx : 3D numeric array storing the similarity of \\(\\mathbf{w}_x\\) between the training sets of the outer splits (rows corresponding to outer splits, columns corresponding to permutations, 3rd dimension corresponding to pairwise comparisons), simwy : 3D numeric array storing the similarity of \\(\\mathbf{w}_y\\) between the training sets of the outer splits (rows corresponding to outer splits, columns corresponding to permutations, 3rd dimension corresponding to pairwise comparisons). In addition, permmat*.mat file includes: permid : cell array with each cell storing a 2D numeric array to define the indexes of the permuted examples for a particular outer split (rows corresponding to examples, columns corresponding to permutations). Saving results : evaluating significance of results and saving outputs into res*.mat as well as the summary of results into results_table.txt . The res*.mat file includes the res structure with the following fields obtained during the analysis: dir : paths to your project, analysis and main outputs, frwork : results oriented details of the framework, stat : detailed results of significance testing, env : details of the computation environment. To get a more detailed description of the fields and subfields of res , please see here . Below is an example of a results_table.txt of an SPLS analysis: split correl pval nfeatx nfeaty 1 0.4355 0.0010 12 9 2 0.3963 0.0010 12 12 3 0.3564 0.0010 33 58 4 0.3517 0.0010 29 4 5 0.4748 0.0010 11 10 The column headings refer to: split : outer data splits, correl : out-of-sample correlation in the holdout sets, pval : p-value within each data split, nfeatx and nfeaty : the number of non-zero features in \\(\\mathbf{w}_x\\) and \\(\\mathbf{w}_y\\) , respectively. In PCA-RCCA and RCCA analysess, the column headings display the hyperparameter values (i.e., amount of L2-norm regularization or number of principal components). Deflation : deflation of the data and repeating steps 4-8. for each associative effect. This operation doesn't save any output files. Configuration for analysis All details of a CCA/PLS analysis are defined in a single configuration variable. This variable is a simple MATLAB structure called cfg , which includes the following main fields: dir : paths to your project, analysis and the outputs of preprocessing, machine : name and other details of the CCA/PLS model, e.g., hyperparameter settings, frwork : details of the framework, e.g., number of data splits, defl : name and details of the deflation method, stat : details of the statistical inference, e.g., number of permutations, data : details of the data e.g., dimensionality, env : details of the computation environment, e.g., local computer or cluster. Use the cfg_defaults function to initialize and update all necessary settings to your cfg . To get a more detailed description of the fields and subfields of cfg , please see here . Please find an example of how to set these variables below: Matlab % Project folder cfg . dir . project = '/PATH/TO/PROJECT/' ; % Machine settings cfg . machine . name = 'spls' ; cfg . machine . param . crit = 'correl+simwxy' ; % Framework settings cfg . frwork . name = 'holdout' ; cfg . frwork . split . nout = 1 ; % Deflation settings cfg . defl . name = 'pls-modeA' ; % Environment settings cfg . env . comp = 'local' ; % Statistical inference settings cfg . stat . nperm = 1000 ; % Update cfg with defaults cfg = cfg_defaults ( cfg ); Data The input data used in a CCA/PLS analysis must be stored in a dedicated folder called data within the project directory (see the demo folder of the toolkit as an example structure and the details on the Getting Started page here ). The \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) matrices should be stored in a specific format inside two .mat files: X.mat including a 2D numeric array called X , which stores one of the data modalities, Y.mat including a 2D numeric array called Y , which stores the other data modality. In both cases, rows correspond to examples (e.g., subjects in a group level analysis) and columns correspond to features (e.g., behavioural measures or brain measures of voxel-wise, connectivity or region-of-interest data). In addition, you can provide other input data matrices, which should be in a similar format: C.mat including a 2D numeric array called C , which stores the confounding variables of the analysis (rows corresponding to examples, columns corresponding to confounds), EB.mat including a 2D numeric array called EB , which defines the exchangeability block structure of the data (rows corresponding to examples, columns corresponding to the exchangeability blocks). The EB matrix can be used for stratified partitioning of the data and/or using restricted permutations. For instance, you can use this to provide the genetic dependencies of your data (e.g., twins, family structure) or different cohorts (e.g., healthy vs. depressed sample). For details on how to create the EB matrix, see Winkler et al 2015 and the PALM toolbox .","title":"Analysis"},{"location":"analysis/#overview","text":"As illustrated in the figure below, the analysis can be divided into eight operations. Here we describe these operations and the contents of the output files they create: Initialization : setting and saving the analysis configuration into cfg*.mat file including a cfg structure. For details on cfg , see the section Configuration for analysis . Data splitting : creating training and test sets of the data and saving outputs into outmat*.mat and inmat*.mat files. The outmat*.mat file includes: otrid : 2D logical array defining the training sets of the outer data splits (rows corresponding to examples and columns corresponding to the outer splits), oteid : 2D logical array defining the test sets of the outer data splits (rows corresponding to examples and columns corresponding to the outer splits). The inmat*.mat file includess: itrid : 3D logical array defining the training sets of the inner data splits (rows corresponding to examples, columns corresponding to the outer splits, and 3rd dimension corresponding to the inner splits), iteid : 3D logical array defining the test sets of the inner data splits (rows corresponding to examples, columns corresponding to the outer splits, and 3rd dimension corresponding to the inner splits). Preprocessing : imputing, z-scoring and deconfounding the data and saving outputs into preproc*.mat . There are separate preproc*.mat files for each data modality, outer and inner splits. For instance, preprocx_split_1_subsample_1*.mat saves resulst of the preprocessing for the first outer and inner split of the data modality \\(\\mathbf{X}\\) . Depending on the preprocessing strategy, this file can include up to three variables: mu : numeric array storing the mean values of the features, sigma : numeric array storing the standard deviations of the features, beta : 2D numeric array storing the regression coefficients from deconfounding (rows corresponding to confounds, columns corresponding to features). Singular Value Decomposition (SVD) of the data and saving outputs into *svd*.mat . There are separate *svd*.mat files for each data modality, training and test set of the outer and inner splits. For instance, tr_svdx_split_1_subsample_1*.mat saves the SVD results of the training set of the first outer and inner split of data modality \\(\\mathbf{X}\\) . The tr_svd*.mat files include: VX (or VY ): 2D numeric array storing the right singular vectors for data modality \\(\\mathbf{X}\\) (or \\(\\mathbf{Y}\\) ) (rows corresponding to examples, columns corresponding to singular vectors), RX (or RY ): 2D numeric array storing the principal components for data modality \\(\\mathbf{X}\\) (or \\(\\mathbf{Y}\\) ) (rows corresponding to examples, columns correponding to principal components), LX (or LY ): numeric array storing the squared singular values for data modality \\(\\mathbf{X}\\) (or \\(\\mathbf{Y}\\) ). The te_svd*.mat files include: RX (or RY ): 2D numeric array storing the test data transformed into principal component space for data modality \\(\\mathbf{X}\\) (or \\(\\mathbf{Y}\\) ) (rows corresponding to examples, columns corresponding to principal components). Grid search : hyperparameter optimization using a grid search and saving outputs into grid*.mat . Before cleaning up the intermediate files of the analysis (see cleanup_files ), there are separate grid*.mat files for each hyperparameter combination and outer split. For instance, grid_split_1_L1x_1_L1y_1*.mat saves the results of the first outer split for the hyperparameter combination \\(c_x=c_y=1\\) (for details, see here ). After cleaning up, all grid search results will be compiled into allgrid*.mat . This file includes all the metrics that are used for evaluating the CCA/PLS model in the inner splits (for details, see cfg.machine.metric here ), for instance: correl : 2D numeric array storing the out-of-sample correlations in the validations sets (rows corresponding to outers splits and hyperparameters, columns corresponding to inner splits), simwx : 3D numeric array storing the similarity of \\(\\mathbf{w}_x\\) between the traingin sets of the inner splits (rows corresponding to outers splits and hyperparameters, columns corresponding to inner splits, 3rd dimension corresponding to pairwise comparisons), simwy : 3D numeric array storing the similarity of \\(\\mathbf{w}_y\\) between the training sets of the inner splits (rows corresponding to outers splits and hyperparameters, columns corresponding to inner splits, 3rd dimension corresponding to pairwise comparisons). Training/testing : setting hyperparameters and saving these into param*.mat . This file includes: param : a structure array with each structure storing the best (or fixed) hyperparameter combination for a particular outer split. fitting models on optimization sets (i.e., outer split), assessing the model weights on holdout sets and saving outputs into model*.mat . This file includes the model weights, \\(\\mathbf{w}_x\\) and \\(\\mathbf{w}_y\\) and all the metrics that are used for evaluating the CCA/PLS model in the outer splits (for details, see cfg.machine.metric here ), for instance: wX and wY : 2D numeric arrays storing the models weights (rows corresponding to outer splits, columns corresponding to features), correl : numeric array storing the out-of-sample correlations in the holdout sets, simwx : 2D numeric array storing the similarity of \\(\\mathbf{w}_x\\) between the training sets of the outer splits (rows corresponding to outer splits, columns corresponding to pairwise comparisons), simwy : 2D numeric array storing the similarity of \\(\\mathbf{w}_y\\) between the training sets of the outer splits (rows corresponding to outer splits, columns corresponding to pairwise comparisons). Permutation test : permutation testing and saving outputs into perm*.mat . Before cleaning up the intermediate files of the analysis (see cleanup_files ), there are separate perm*.mat files for each permutation. For instance, perm_0001*.mat saves the results of the first permutation. After cleaning up, all permutation results will be compiled into perm*.mat . This file includes all the metrics that are used for evaluating the CCA/PLS model in the outer splits (for details, see cfg.machine.metric here ), for instance: correl : 2D numeric array storing the out-of-sample correlations (rows corresponding to outer splits, columns corresponding to permutations), simwx : 3D numeric array storing the similarity of \\(\\mathbf{w}_x\\) between the training sets of the outer splits (rows corresponding to outer splits, columns corresponding to permutations, 3rd dimension corresponding to pairwise comparisons), simwy : 3D numeric array storing the similarity of \\(\\mathbf{w}_y\\) between the training sets of the outer splits (rows corresponding to outer splits, columns corresponding to permutations, 3rd dimension corresponding to pairwise comparisons). In addition, permmat*.mat file includes: permid : cell array with each cell storing a 2D numeric array to define the indexes of the permuted examples for a particular outer split (rows corresponding to examples, columns corresponding to permutations). Saving results : evaluating significance of results and saving outputs into res*.mat as well as the summary of results into results_table.txt . The res*.mat file includes the res structure with the following fields obtained during the analysis: dir : paths to your project, analysis and main outputs, frwork : results oriented details of the framework, stat : detailed results of significance testing, env : details of the computation environment. To get a more detailed description of the fields and subfields of res , please see here . Below is an example of a results_table.txt of an SPLS analysis: split correl pval nfeatx nfeaty 1 0.4355 0.0010 12 9 2 0.3963 0.0010 12 12 3 0.3564 0.0010 33 58 4 0.3517 0.0010 29 4 5 0.4748 0.0010 11 10 The column headings refer to: split : outer data splits, correl : out-of-sample correlation in the holdout sets, pval : p-value within each data split, nfeatx and nfeaty : the number of non-zero features in \\(\\mathbf{w}_x\\) and \\(\\mathbf{w}_y\\) , respectively. In PCA-RCCA and RCCA analysess, the column headings display the hyperparameter values (i.e., amount of L2-norm regularization or number of principal components). Deflation : deflation of the data and repeating steps 4-8. for each associative effect. This operation doesn't save any output files.","title":"Overview"},{"location":"analysis/#configuration-for-analysis","text":"All details of a CCA/PLS analysis are defined in a single configuration variable. This variable is a simple MATLAB structure called cfg , which includes the following main fields: dir : paths to your project, analysis and the outputs of preprocessing, machine : name and other details of the CCA/PLS model, e.g., hyperparameter settings, frwork : details of the framework, e.g., number of data splits, defl : name and details of the deflation method, stat : details of the statistical inference, e.g., number of permutations, data : details of the data e.g., dimensionality, env : details of the computation environment, e.g., local computer or cluster. Use the cfg_defaults function to initialize and update all necessary settings to your cfg . To get a more detailed description of the fields and subfields of cfg , please see here . Please find an example of how to set these variables below: Matlab % Project folder cfg . dir . project = '/PATH/TO/PROJECT/' ; % Machine settings cfg . machine . name = 'spls' ; cfg . machine . param . crit = 'correl+simwxy' ; % Framework settings cfg . frwork . name = 'holdout' ; cfg . frwork . split . nout = 1 ; % Deflation settings cfg . defl . name = 'pls-modeA' ; % Environment settings cfg . env . comp = 'local' ; % Statistical inference settings cfg . stat . nperm = 1000 ; % Update cfg with defaults cfg = cfg_defaults ( cfg );","title":"Configuration for analysis"},{"location":"analysis/#data","text":"The input data used in a CCA/PLS analysis must be stored in a dedicated folder called data within the project directory (see the demo folder of the toolkit as an example structure and the details on the Getting Started page here ). The \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) matrices should be stored in a specific format inside two .mat files: X.mat including a 2D numeric array called X , which stores one of the data modalities, Y.mat including a 2D numeric array called Y , which stores the other data modality. In both cases, rows correspond to examples (e.g., subjects in a group level analysis) and columns correspond to features (e.g., behavioural measures or brain measures of voxel-wise, connectivity or region-of-interest data). In addition, you can provide other input data matrices, which should be in a similar format: C.mat including a 2D numeric array called C , which stores the confounding variables of the analysis (rows corresponding to examples, columns corresponding to confounds), EB.mat including a 2D numeric array called EB , which defines the exchangeability block structure of the data (rows corresponding to examples, columns corresponding to the exchangeability blocks). The EB matrix can be used for stratified partitioning of the data and/or using restricted permutations. For instance, you can use this to provide the genetic dependencies of your data (e.g., twins, family structure) or different cohorts (e.g., healthy vs. depressed sample). For details on how to create the EB matrix, see Winkler et al 2015 and the PALM toolbox .","title":"Data"},{"location":"background/","text":"CCA/PLS models In this section, we present the formulations of the different CCA and PLS models implemented in the toolkit. In all models, \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) represent matrices for the two data modalities, each matrix containing one standardized (i.e., having zero mean and unit variance) variable/feature per column and one example/sample per row. The model weights, \\(\\mathbf{w}_x\\) and \\(\\mathbf{w}_y\\) (one weight vector for each modality) are column vectors with the same number of elements as the number of variables in their corresponding data modality. Notations for L1 and L2-norm: \\(||\\mathbf{w}||_2 = \\sqrt{\\sum_{k=1}^{n} |x_k|^2}\\) is the L2-norm of a vector \\(\\mathbf{w}\\) , i.e., square root of the sum of squares of weight values, \\(||\\mathbf{w}||_1 = \\sum_{k=1}^{n} |x_k|\\) is the L1-norm of a vector \\(\\mathbf{w}\\) , i.e., sum of absolute weight values. Canonical Correlation Analysis (CCA) CCA ( Hotelling 1936 ) finds a pair of weights, \\(\\mathbf{w}_x\\) and \\(\\mathbf{w}_y\\) , such that the correlation between the projections of \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) onto these weights are maximised: $$ max_{\\mathbf{w}_x,\\mathbf{w}_y} \\text{ } corr(\\mathbf{Xw}_x,\\mathbf{Yw}_y) $$ Most commonly though, CCA is expressed in the form of a constrained optimization problem: $$ max_{\\mathbf{w}_x,\\mathbf{w}_y} \\text{ } \\mathbf{w}_x^T\\mathbf{X}^T\\mathbf{Yw}_y $$ $$ \\text{subject to } \\mathbf{w}_x^T\\mathbf{X}^T\\mathbf{Xw}_x = 1,\\ \\mathbf{w}_y^T\\mathbf{Y}^T\\mathbf{Yw}_y = 1 $$ We highlight that it is not possible to obtain a solution for this standard CCA when the number of variables exceeds the number of examples (technically speaking, the optimization problem is ill posed). Two approaches have been proposed to address this problem: reducing the dimensionality of the data with Principal Component Analysis (PCA) (e.g., Smith et al. 2015 , Helmer et al. 2020 , Alnaes et al. 2020 ), using regularized extensions of CCA and PLS (e.g., Xia et al. 2018 , Ing et al. 2019 , Popovic et al. 2020 ). We note that although PLS always has a solution (i.e., never ill posed) irrespective of the number of variables, it might still benefit from regularization. Adding an L1-norm regularization, for example, pushes the weights of some variables to zero and therefore forcing the model to learn a sparse solution. Partial Least Squares (PLS) PLS (Wold 1985, Wegelin 2000 ) finds a pair of weights, \\(\\mathbf{w}_x\\) and \\(\\mathbf{w}_y\\) , such that the covariance between the projections of \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) onto these weights are maximised: $$ max_{\\mathbf{w}_x,\\mathbf{w}_y} \\text{ } cov(\\mathbf{Xw}_x,\\mathbf{Yw}_y) $$ Similar to CCA, PLS is most often expressed as a constrained optimization problem in the following form: $$ max_{\\mathbf{w}_x,\\mathbf{w}_y} \\text{ } \\mathbf{w}_x^T\\mathbf{X}^T\\mathbf{Yw}_y $$ $$ \\text{subject to } ||\\mathbf{w}_x||_2^2 = 1,\\ ||\\mathbf{w}_y||_2^2 = 1 $$ PLS is a family of methods. Depending on the modelling aim, PLS variants can be divided into two main groups: symmetric variants (PLS-mode A, PLS-SVD): with the aim of identifying associations between two data modalities, asymmetric variants (PLS1, PLS2): with the aim of predicting one modality from another modality. For details on these variants, see e.g., Wegelin 2000 , Rosipal & Kramer 2006 , Krishnan et al. 2011 , Mihalik et al. 2020 . All PLS variants use the same optimization problem but they differ in their deflation strategies . Therefore, whereas all variants yield the same first associative effect, the weights from the second associative effects will be different. CCA with PCA dimensionality reduction (PCA-CCA) PCA transforms each modality of multivariate data into uncorrelated principal components, such that the variance of each principal component is maximised: $$ max_{\\mathbf{w}_x} \\text{ } var(\\mathbf{Xw}_x) $$ \\[ max_{\\mathbf{w}_y} \\text{ } var(\\mathbf{Yw}_y) \\] These principal components (i.e., \\(\\mathbf{R}_x=\\mathbf{Xw}_x\\) and \\(\\mathbf{R}_y=\\mathbf{Yw}_y\\) ) are then entered into CCA resulting in the following constrained optimization problem: $$ max_{\\mathbf{v}_x,\\mathbf{v}_y} \\text{ } \\mathbf{v}_x^T\\mathbf{R}_x^T\\mathbf{R}_y\\mathbf{v}_y $$ $$ \\text{subject to } \\mathbf{v}_x^T\\mathbf{R}_x^T\\mathbf{R}_x\\mathbf{v}_x = 1, $$ $$ \\mathbf{v}_y^T\\mathbf{R}_y^T\\mathbf{R}_y\\mathbf{v}_y = 1 $$ PCA is often used as naive dimensionality reduction technique, as principal components explaining little variance are assumed to be noise and discarded, and the remaining principal components are entered into CCA. Regularized CCA (RCCA) In RCCA, L2-norm regularization is added to the CCA optimization ( Vinod 1976 ), which leads to the following constrained optimization problem: $$ max_{\\mathbf{w}_x,\\mathbf{w}_y} \\text{ } \\mathbf{w}_x^T\\mathbf{X}^T\\mathbf{Yw}_y $$ $$ \\text{subject to } (1-c_x)\\mathbf{w}_x^T\\mathbf{X}^T\\mathbf{Xw}_x+c_x||\\mathbf{w}_x||_2^2 = 1, $$ $$ (1-c_y)\\mathbf{w}_y^T\\mathbf{Y}^T\\mathbf{Yw}_y+c_y||\\mathbf{w}_y||_2^2 = 1 $$ The two hyperparameters of RCCA ( \\(c_x\\) , \\(c_y\\) ) control the amount of L2-norm regularization. We can see that these hyperparameters provide a smooth transition between CCA ( \\(c_x=c_y=0\\) , not regularized) and PLS ( \\(c_x=c_y=1\\) , most regularized), thus RCCA can be thought of as a mixture of CCA and PLS optimization. Sparse PLS (SPLS) In SPLS, L1-norm regularization is added to the PLS optimization (e.g., Le Cao et al. 2008 , Monteiro et al. 2016 ), which leads to the following constrained optimization problem: $$ max_{\\mathbf{w}_x,\\mathbf{w}_y} \\text{ } \\mathbf{w}_x^T\\mathbf{X}^T\\mathbf{Yw}_y $$ $$ \\text{subject to } ||\\mathbf{w}_x||_2^2 \\le 1,\\ ||\\mathbf{w}_x||_1 \\le c_x,\\ ||\\mathbf{w}_y||_2^2 \\le 1,\\ ||\\mathbf{w}_y||_1 \\le c_y\\ $$ The two hyperparameters of SPLS ( \\(c_x\\) , \\(c_y\\) ) control the amount of L1-norm regularization. These hyperparameters set an upper limit to the L1-norm of the weights, thus imposing sparsity on the weights (i.e., the weights of some variables will be set to 0). Similar to PLS, SPLS can also have different variants based on the deflation methods (e.g., generalized deflation in Monteiro et al. 2016 , PLS-mode A deflation in Le Cao et al. 2009 , PLS2 deflation in Le Cao et al. 2008 ). Analysis frameworks In order to optimize the hyperparameters (i.e., number of principal components or regularization parameters) and to perform statistical inference (i.e., assess the number of significant associative effects), the CCA/PLS model is embedded in an analytical framework. The figures below ( Mihalik et al. 2022 ) illustrate the two frameworks included in the toolkit. In the descriptive framework , CCA/PLS is fitted on the entire data and the resulting model weights are used to compute in-sample correlation. There is no hyperparameter optimization, i.e., the number of principal components (PCA-CCA) or regularization parameters (RCCA, SPLS) is fixed. To assess the number of significant associative effects, we use permutation testing based on the in-sample correlation. In particular, we shuffle the order of the subjects (i.e., rows) in \\(\\mathbf{Y}\\) and fit the CCA/PLS model to compute permuted in-sample correlations. Critically, we calculate the p-value as the fraction of permuted in-sample correlations exceeding the in-sample correlation obtained on the non-permuted data of the first associative effect (i.e., maximum statistics approach). In the predictive (or machine learning) framework , first we randomly split the data into optimization set (e.g., 80% of the overall data) and holdout set (e.g., 20% of the overall data). For CCA/PLS models with hyperparameters (i.e., PCA-CCA, RCCA and SPLS), we further split the optimization set multiple times into training set (e.g., 80% of the optimization set) and validation set (e.g., 20% of the optimization set). The inner split is used for hyperparameter optimization and the outer split is used for statistical inference. The best hyperparameters (one for each data modality) are selected based on the best generalizability of the model (measured as the average out-of-sample correlation in the validation sets). To assess the number of significant associative effects, we use permutation testing based on the out-of-sample correlation in the holdout set. In particular, we shuffle the order of the subjects (i.e., rows) in \\(\\mathbf{Y}\\) and fit the CCA/PLS model to compute permuted out-sample correlation. We calculate the p-value as the fraction of permuted out-of-sample correlations exceeding the out-of-sample correlation obtained on the non-permuted data. If multiple holdout sets are used then the p-value for each holdout set is corrected for multiple comparisons using Bonferroni correction (e.g., \\(\\alpha=0.05/10=0.005\\) in case of 10 holdout sets). This means that the associative effect is considered significant if \\(p \\le 0.005\\) in at least one of the holdout sets (omnibus hypothesis approach, for details see Monteiro et al. 2016 ). An important component of the CCA/PLS framework is testing the stability of the model . In the toolkit, the stability of the CCA/PLS model is measured as the average similarity of weights across different training (or optimization) sets (of course, at least two splits of data are needed for this). In addition to assessing the stability of the CCA/PLS model in the outer splits, stability can be also used as a joint criterion with generalizability for hyperparameter optimization (for details, see Mihalik et al. 2020 ). Deflation methods The toolkit includes three different deflation methods for CCA/PLS models: generalized deflation, PLS-mode A deflation, PLS regression deflation. CCA , RCCA and PLS-SVD can be all seen as subcases of the generalized eigenvalue problem. The iterative solution of the generalized eigenvalue problem uses generalized deflation, which thus will be the deflation strategy for these models. This deflation can be written as: \\[ \\mathbf{X}_{i+1} = \\mathbf{X}_{i} - \\mathbf{X}_{i} \\mathbf{w}_x \\mathbf{w}_x^T \\mathbf{B}_{x} \\] \\[ \\mathbf{Y}_{i+1} = \\mathbf{Y}_{i} - \\mathbf{Y}_{i} \\mathbf{w}_{y} \\mathbf{w}_y^T \\mathbf{B}_{y} \\] where we used the same notations as in the CCA/PLS models and \\(\\mathbf{B}_x,\\mathbf{B}_y\\) define the different subcases of the generalized eigenvalue problem. In case of RCCA, \\(\\mathbf{B}_x = (1-c_x)\\mathbf{X}_0^T\\mathbf{X}_0+c_x\\mathbf{I}\\) and \\(\\mathbf{B}_y = (1-c_y)\\mathbf{Y}_0^T\\mathbf{Y}_0+c_y\\mathbf{I}\\) , where \\(\\mathbf{X}_0\\) and \\(\\mathbf{Y}_0\\) are the original data matrices without deflation, \\(c_x\\) and \\(c_y\\) are the hyperparameters of L2-norm regularization in RCCA . CCA and PLS-SVD are specific cases of \\(c_x=c_y=0\\) and \\(c_x=c_y=1\\) , respectively. PLS-mode A uses the following deflation: \\[ \\mathbf{X}_{i+1} = \\mathbf{X}_i - \\mathbf{X}_i \\mathbf{w}_x \\mathbf{p}^T \\] \\[ \\mathbf{Y}_{i+1} = \\mathbf{Y}_i - \\mathbf{Y}_i \\mathbf{w}_y \\mathbf{q}^T \\] where \\(\\mathbf{p}=\\frac{\\mathbf{X}_i^T\\mathbf{X}_i\\mathbf{w}_x}{\\mathbf{w}_x^T\\mathbf{X}_i^T\\mathbf{X}_i\\mathbf{w}_x}\\) and \\(\\mathbf{q}=\\frac{\\mathbf{Y}_i^T\\mathbf{Y}_i\\mathbf{w}_y}{\\mathbf{w}_y^T\\mathbf{Y}_i^T\\mathbf{Y}_i\\mathbf{w}_y}\\) . PLS1 and PLS2 are regression methods (PLS1 refers to the variant with a single output variable and PLS2 refers to the variant with multiple output variables), in which case only the input data needs to be deflated as follows: $$ \\mathbf{X}_{i+1} = \\mathbf{X}_i - \\mathbf{X}_i \\mathbf{w}_x \\mathbf{p}^T $$ where the same notations are used as above. For additional details on deflations, see e.g., Wegelin 2000 , Rosipal & Kramer 2006 , Mihalik et al. 2020 , Mihalik et al. 2022 .","title":"Background"},{"location":"background/#ccapls-models","text":"In this section, we present the formulations of the different CCA and PLS models implemented in the toolkit. In all models, \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) represent matrices for the two data modalities, each matrix containing one standardized (i.e., having zero mean and unit variance) variable/feature per column and one example/sample per row. The model weights, \\(\\mathbf{w}_x\\) and \\(\\mathbf{w}_y\\) (one weight vector for each modality) are column vectors with the same number of elements as the number of variables in their corresponding data modality. Notations for L1 and L2-norm: \\(||\\mathbf{w}||_2 = \\sqrt{\\sum_{k=1}^{n} |x_k|^2}\\) is the L2-norm of a vector \\(\\mathbf{w}\\) , i.e., square root of the sum of squares of weight values, \\(||\\mathbf{w}||_1 = \\sum_{k=1}^{n} |x_k|\\) is the L1-norm of a vector \\(\\mathbf{w}\\) , i.e., sum of absolute weight values.","title":"CCA/PLS models"},{"location":"background/#canonical-correlation-analysis-cca","text":"CCA ( Hotelling 1936 ) finds a pair of weights, \\(\\mathbf{w}_x\\) and \\(\\mathbf{w}_y\\) , such that the correlation between the projections of \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) onto these weights are maximised: $$ max_{\\mathbf{w}_x,\\mathbf{w}_y} \\text{ } corr(\\mathbf{Xw}_x,\\mathbf{Yw}_y) $$ Most commonly though, CCA is expressed in the form of a constrained optimization problem: $$ max_{\\mathbf{w}_x,\\mathbf{w}_y} \\text{ } \\mathbf{w}_x^T\\mathbf{X}^T\\mathbf{Yw}_y $$ $$ \\text{subject to } \\mathbf{w}_x^T\\mathbf{X}^T\\mathbf{Xw}_x = 1,\\ \\mathbf{w}_y^T\\mathbf{Y}^T\\mathbf{Yw}_y = 1 $$ We highlight that it is not possible to obtain a solution for this standard CCA when the number of variables exceeds the number of examples (technically speaking, the optimization problem is ill posed). Two approaches have been proposed to address this problem: reducing the dimensionality of the data with Principal Component Analysis (PCA) (e.g., Smith et al. 2015 , Helmer et al. 2020 , Alnaes et al. 2020 ), using regularized extensions of CCA and PLS (e.g., Xia et al. 2018 , Ing et al. 2019 , Popovic et al. 2020 ). We note that although PLS always has a solution (i.e., never ill posed) irrespective of the number of variables, it might still benefit from regularization. Adding an L1-norm regularization, for example, pushes the weights of some variables to zero and therefore forcing the model to learn a sparse solution.","title":"Canonical Correlation Analysis (CCA)"},{"location":"background/#partial-least-squares-pls","text":"PLS (Wold 1985, Wegelin 2000 ) finds a pair of weights, \\(\\mathbf{w}_x\\) and \\(\\mathbf{w}_y\\) , such that the covariance between the projections of \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) onto these weights are maximised: $$ max_{\\mathbf{w}_x,\\mathbf{w}_y} \\text{ } cov(\\mathbf{Xw}_x,\\mathbf{Yw}_y) $$ Similar to CCA, PLS is most often expressed as a constrained optimization problem in the following form: $$ max_{\\mathbf{w}_x,\\mathbf{w}_y} \\text{ } \\mathbf{w}_x^T\\mathbf{X}^T\\mathbf{Yw}_y $$ $$ \\text{subject to } ||\\mathbf{w}_x||_2^2 = 1,\\ ||\\mathbf{w}_y||_2^2 = 1 $$ PLS is a family of methods. Depending on the modelling aim, PLS variants can be divided into two main groups: symmetric variants (PLS-mode A, PLS-SVD): with the aim of identifying associations between two data modalities, asymmetric variants (PLS1, PLS2): with the aim of predicting one modality from another modality. For details on these variants, see e.g., Wegelin 2000 , Rosipal & Kramer 2006 , Krishnan et al. 2011 , Mihalik et al. 2020 . All PLS variants use the same optimization problem but they differ in their deflation strategies . Therefore, whereas all variants yield the same first associative effect, the weights from the second associative effects will be different.","title":"Partial Least Squares (PLS)"},{"location":"background/#cca-with-pca-dimensionality-reduction-pca-cca","text":"PCA transforms each modality of multivariate data into uncorrelated principal components, such that the variance of each principal component is maximised: $$ max_{\\mathbf{w}_x} \\text{ } var(\\mathbf{Xw}_x) $$ \\[ max_{\\mathbf{w}_y} \\text{ } var(\\mathbf{Yw}_y) \\] These principal components (i.e., \\(\\mathbf{R}_x=\\mathbf{Xw}_x\\) and \\(\\mathbf{R}_y=\\mathbf{Yw}_y\\) ) are then entered into CCA resulting in the following constrained optimization problem: $$ max_{\\mathbf{v}_x,\\mathbf{v}_y} \\text{ } \\mathbf{v}_x^T\\mathbf{R}_x^T\\mathbf{R}_y\\mathbf{v}_y $$ $$ \\text{subject to } \\mathbf{v}_x^T\\mathbf{R}_x^T\\mathbf{R}_x\\mathbf{v}_x = 1, $$ $$ \\mathbf{v}_y^T\\mathbf{R}_y^T\\mathbf{R}_y\\mathbf{v}_y = 1 $$ PCA is often used as naive dimensionality reduction technique, as principal components explaining little variance are assumed to be noise and discarded, and the remaining principal components are entered into CCA.","title":"CCA with PCA dimensionality reduction (PCA-CCA)"},{"location":"background/#regularized-cca-rcca","text":"In RCCA, L2-norm regularization is added to the CCA optimization ( Vinod 1976 ), which leads to the following constrained optimization problem: $$ max_{\\mathbf{w}_x,\\mathbf{w}_y} \\text{ } \\mathbf{w}_x^T\\mathbf{X}^T\\mathbf{Yw}_y $$ $$ \\text{subject to } (1-c_x)\\mathbf{w}_x^T\\mathbf{X}^T\\mathbf{Xw}_x+c_x||\\mathbf{w}_x||_2^2 = 1, $$ $$ (1-c_y)\\mathbf{w}_y^T\\mathbf{Y}^T\\mathbf{Yw}_y+c_y||\\mathbf{w}_y||_2^2 = 1 $$ The two hyperparameters of RCCA ( \\(c_x\\) , \\(c_y\\) ) control the amount of L2-norm regularization. We can see that these hyperparameters provide a smooth transition between CCA ( \\(c_x=c_y=0\\) , not regularized) and PLS ( \\(c_x=c_y=1\\) , most regularized), thus RCCA can be thought of as a mixture of CCA and PLS optimization.","title":"Regularized CCA (RCCA)"},{"location":"background/#sparse-pls-spls","text":"In SPLS, L1-norm regularization is added to the PLS optimization (e.g., Le Cao et al. 2008 , Monteiro et al. 2016 ), which leads to the following constrained optimization problem: $$ max_{\\mathbf{w}_x,\\mathbf{w}_y} \\text{ } \\mathbf{w}_x^T\\mathbf{X}^T\\mathbf{Yw}_y $$ $$ \\text{subject to } ||\\mathbf{w}_x||_2^2 \\le 1,\\ ||\\mathbf{w}_x||_1 \\le c_x,\\ ||\\mathbf{w}_y||_2^2 \\le 1,\\ ||\\mathbf{w}_y||_1 \\le c_y\\ $$ The two hyperparameters of SPLS ( \\(c_x\\) , \\(c_y\\) ) control the amount of L1-norm regularization. These hyperparameters set an upper limit to the L1-norm of the weights, thus imposing sparsity on the weights (i.e., the weights of some variables will be set to 0). Similar to PLS, SPLS can also have different variants based on the deflation methods (e.g., generalized deflation in Monteiro et al. 2016 , PLS-mode A deflation in Le Cao et al. 2009 , PLS2 deflation in Le Cao et al. 2008 ).","title":"Sparse PLS (SPLS)"},{"location":"background/#analysis-frameworks","text":"In order to optimize the hyperparameters (i.e., number of principal components or regularization parameters) and to perform statistical inference (i.e., assess the number of significant associative effects), the CCA/PLS model is embedded in an analytical framework. The figures below ( Mihalik et al. 2022 ) illustrate the two frameworks included in the toolkit. In the descriptive framework , CCA/PLS is fitted on the entire data and the resulting model weights are used to compute in-sample correlation. There is no hyperparameter optimization, i.e., the number of principal components (PCA-CCA) or regularization parameters (RCCA, SPLS) is fixed. To assess the number of significant associative effects, we use permutation testing based on the in-sample correlation. In particular, we shuffle the order of the subjects (i.e., rows) in \\(\\mathbf{Y}\\) and fit the CCA/PLS model to compute permuted in-sample correlations. Critically, we calculate the p-value as the fraction of permuted in-sample correlations exceeding the in-sample correlation obtained on the non-permuted data of the first associative effect (i.e., maximum statistics approach). In the predictive (or machine learning) framework , first we randomly split the data into optimization set (e.g., 80% of the overall data) and holdout set (e.g., 20% of the overall data). For CCA/PLS models with hyperparameters (i.e., PCA-CCA, RCCA and SPLS), we further split the optimization set multiple times into training set (e.g., 80% of the optimization set) and validation set (e.g., 20% of the optimization set). The inner split is used for hyperparameter optimization and the outer split is used for statistical inference. The best hyperparameters (one for each data modality) are selected based on the best generalizability of the model (measured as the average out-of-sample correlation in the validation sets). To assess the number of significant associative effects, we use permutation testing based on the out-of-sample correlation in the holdout set. In particular, we shuffle the order of the subjects (i.e., rows) in \\(\\mathbf{Y}\\) and fit the CCA/PLS model to compute permuted out-sample correlation. We calculate the p-value as the fraction of permuted out-of-sample correlations exceeding the out-of-sample correlation obtained on the non-permuted data. If multiple holdout sets are used then the p-value for each holdout set is corrected for multiple comparisons using Bonferroni correction (e.g., \\(\\alpha=0.05/10=0.005\\) in case of 10 holdout sets). This means that the associative effect is considered significant if \\(p \\le 0.005\\) in at least one of the holdout sets (omnibus hypothesis approach, for details see Monteiro et al. 2016 ). An important component of the CCA/PLS framework is testing the stability of the model . In the toolkit, the stability of the CCA/PLS model is measured as the average similarity of weights across different training (or optimization) sets (of course, at least two splits of data are needed for this). In addition to assessing the stability of the CCA/PLS model in the outer splits, stability can be also used as a joint criterion with generalizability for hyperparameter optimization (for details, see Mihalik et al. 2020 ).","title":"Analysis frameworks"},{"location":"background/#deflation-methods","text":"The toolkit includes three different deflation methods for CCA/PLS models: generalized deflation, PLS-mode A deflation, PLS regression deflation. CCA , RCCA and PLS-SVD can be all seen as subcases of the generalized eigenvalue problem. The iterative solution of the generalized eigenvalue problem uses generalized deflation, which thus will be the deflation strategy for these models. This deflation can be written as: \\[ \\mathbf{X}_{i+1} = \\mathbf{X}_{i} - \\mathbf{X}_{i} \\mathbf{w}_x \\mathbf{w}_x^T \\mathbf{B}_{x} \\] \\[ \\mathbf{Y}_{i+1} = \\mathbf{Y}_{i} - \\mathbf{Y}_{i} \\mathbf{w}_{y} \\mathbf{w}_y^T \\mathbf{B}_{y} \\] where we used the same notations as in the CCA/PLS models and \\(\\mathbf{B}_x,\\mathbf{B}_y\\) define the different subcases of the generalized eigenvalue problem. In case of RCCA, \\(\\mathbf{B}_x = (1-c_x)\\mathbf{X}_0^T\\mathbf{X}_0+c_x\\mathbf{I}\\) and \\(\\mathbf{B}_y = (1-c_y)\\mathbf{Y}_0^T\\mathbf{Y}_0+c_y\\mathbf{I}\\) , where \\(\\mathbf{X}_0\\) and \\(\\mathbf{Y}_0\\) are the original data matrices without deflation, \\(c_x\\) and \\(c_y\\) are the hyperparameters of L2-norm regularization in RCCA . CCA and PLS-SVD are specific cases of \\(c_x=c_y=0\\) and \\(c_x=c_y=1\\) , respectively. PLS-mode A uses the following deflation: \\[ \\mathbf{X}_{i+1} = \\mathbf{X}_i - \\mathbf{X}_i \\mathbf{w}_x \\mathbf{p}^T \\] \\[ \\mathbf{Y}_{i+1} = \\mathbf{Y}_i - \\mathbf{Y}_i \\mathbf{w}_y \\mathbf{q}^T \\] where \\(\\mathbf{p}=\\frac{\\mathbf{X}_i^T\\mathbf{X}_i\\mathbf{w}_x}{\\mathbf{w}_x^T\\mathbf{X}_i^T\\mathbf{X}_i\\mathbf{w}_x}\\) and \\(\\mathbf{q}=\\frac{\\mathbf{Y}_i^T\\mathbf{Y}_i\\mathbf{w}_y}{\\mathbf{w}_y^T\\mathbf{Y}_i^T\\mathbf{Y}_i\\mathbf{w}_y}\\) . PLS1 and PLS2 are regression methods (PLS1 refers to the variant with a single output variable and PLS2 refers to the variant with multiple output variables), in which case only the input data needs to be deflated as follows: $$ \\mathbf{X}_{i+1} = \\mathbf{X}_i - \\mathbf{X}_i \\mathbf{w}_x \\mathbf{p}^T $$ where the same notations are used as above. For additional details on deflations, see e.g., Wegelin 2000 , Rosipal & Kramer 2006 , Mihalik et al. 2020 , Mihalik et al. 2022 .","title":"Deflation methods"},{"location":"cfg/","text":"Here you can find a description of all possible settings of the fields and subfields of cfg . First parameter always indicates the default option. dir Essential paths to your project, framework and processed data. The project folder should include a data folder where all the input data are stored. .project [ path ] full path to your project, such as 'PATH/TO/YOUR/PROJECT' .frwork [ path ] full path to your specific framework, such as 'PATH/TO/YOUR/PROJECT/framework/ANALYSIS_NAME' analysis name is generated from machine name and framework settings, for instance, an SPLS analysis with a single holdout set (20% of the data) and 10 validation sets (20% of the optimization set) and cfg.frwork.flag = '_TEST' will generate the spls_holdout1-0.20_subsamp10-0.20_TEST folder .load [ path ] full path to your processed data, such as 'PATH/TO/YOUR/PROJECT/framework/ANALYSIS_NAME/load' it possibly includes a preproc folder with the results of the preprocessing (e.g., mean, std of the features and the beta coefficients from the deconfounding) and an svd folder with the SVD results for computational efficiency of CCA , PCA-CCA and RCCA machine Algorithm to be used, its settings and information about hyperparameter optimization. Please make sure that you are familiar with the hyperparameter settings of the chosen algorithm, e.g. range and scale of hyperparameter values for grid search or number of PCA components. We strongly encourage to use RCCA , SPLS or PCA-CCA . For a discussion, see Mihalik et al. 2022 . .name [ 'cca', 'rcca', 'pls', 'spls' ] name of the algorithm PCA-CCA analysis is defined by cfg.machine.name = cca and cfg.machine.param = {'PCAx' 'PCAy'} RCCA finds a smooth solution between CCA and PLS using L2-norm regularization (see cfg.machine.param.L2x and cfg.machine.param.L2y below) .metric [ cell array ] metrics to evaluate the CCA/PLS algorithm note that each metric provided here will be saved on disc during hyperparameter optimization, training the main model and permutation testing, so they can be used for diagnostics later either they are used as criterion to evaluate a model or not options for each cell: 'trcorrel' (in-sample correlation), 'correl' (out-of-sample correlation measuring the generalizability of the model), 'trcovar' (in-sample covariance), 'covar' (out-of-sample covariance measuring the generalizability of the model), 'trexvarx' (in-sample percent explained variance by \\(\\mathbf{w}_x\\) ), 'exvarx' (out-of-sample percent explained variance by \\(\\mathbf{w}_x\\) ), 'trexvary' (in-sample percent explained variance by \\(\\mathbf{w}_y\\) ), 'exvary' (out-of-sample percent explained variance by \\(\\mathbf{w}_y\\) ), 'simwx' (similarity of \\(\\mathbf{w}_x\\) across training sets measuring the stability of the model), 'simwy' (similarity of \\(\\mathbf{w}_y\\) across training sets measuring the stability of the model), 'unsuc' (number of unsuccessful convergence in SPLS, should be a small number in general, for details, see cfg.machine.spls.maxiter ) .param.crit [ 'correl', 'correl+simwxy' ] criterion to select the best hyperparameter in general, we recommend using 'correl' (measuring the generalizability of the model) or 'correl+simwxy' (measuring the stability and the generalizability of the model) for SPLS 'correl+simwxy' calculates a 2-dimensional Euclidean distance from {1,1} based on out-of-sample correlation and the average similarity of \\(\\mathbf{w}_x\\) and \\(\\mathbf{w}_y\\) (i.e., it measures the deviation from perfect out-of-sample generalizability and perfect model stability, for details, see Mihalik et al. 2020 ) .param.name [ cell array ] name of the hyperparameters in the CCA/PLS model in each name, the first characters refers to the type of hyperparameter and the last character refers to the data modality, e.g., L1x for L1 regularization of \\(\\mathbf{w}_x\\) potential settings: {'L1x', 'L1y'} for L1 regularization in SPLS , {'L2x', 'L2y'} for L2 regularization in RCCA , {'PCAx', 'PCAy'} for number of PCA components in PCA_CCA an alternative to setting the number of PCA components is setting the explained variance by the PCA components using {'VARx', 'VARy'} in PCA_CCA .param.type [ 'factorial', 'matched' ] defines whether the grid search of hyperparameters should be based on a factorial combination of all hyperparameter values in the two data modalities (i.e., 100 combinations for 10 hyperparameter values in both data modalities) or the combination of hyperparameter values based on matching their indices (i.e., first index in one data modality is paired with first index in the other data modality) .param.L1x, .param.L1y [ int or numeric array ] amount of L1 regularization for \\(\\mathbf{w}_x\\) and \\(\\mathbf{w}_y\\) (see \\(c_x\\) and \\(c_y\\) in SPLS ) if not provided, the function generates a logarithmically scaled numeric array based on the following equation, for instance, for \\(\\mathbf{w}_x\\) : $$ c_x = logspace(a, b, n) $$ where \\(c_x\\) is the hyperparameter (i.e., cfg.machine.param.L1x ), \\(a\\) is the start of the logarithmic range (i.e., log(cfg.machine.param.rangeL1x(1)) ), \\(b\\) is the end of the logarithmic range (i.e., log(cfg.machine.param.rangeL1x(2)) ) and \\(n\\) is the number of values between \\(a\\) and \\(b\\) (i.e., cfg.machine.param.nL1x ) .param.rangeL1x, .param.rangeL1y [ numeric array ] range of the hyperparameters for L1 regularization of \\(\\mathbf{w}_x\\) and \\(\\mathbf{w}_y\\) the default range is between 1 and the square root of the number of features if a given value is outside of this default range then L1 regularization is not active and PLS is used instead of SPLS .param.nL1x, .param.nL1y [ int --> 10 ] number of values in the range of hyperparameters for L1 regularization of \\(\\mathbf{w}_x\\) and \\(\\mathbf{w}_y\\) (for details, see cfg.machine.param.L1x and cfg.machine.param.L1y above) .param.L2x, .param.L2y [ int ] amount of L2 regularization for \\(\\mathbf{w}_x\\) and \\(\\mathbf{w}_y\\) (see \\(c_x\\) and \\(c_y\\) in RCCA ) RCCA is equivalent to CCA if both values are set to \\(0\\) RCCA is equivalent to PLS if both values are set to \\(1\\) if not provided, the function generates a logarithmically scaled numeric array based on the following equation, for instance, for \\(\\mathbf{w}_x\\) : $$ c_x = 1 - logspace(a, b, n) $$ where \\(c_x\\) is the hyperparameter (i.e., cfg.machine.param.L2x ), \\(a\\) is the start of the logarithmic range (i.e., -log(cfg.machine.param.rangeL2x(1)) ), \\(b\\) is the end of the logarithmic range (i.e, -log(cfg.machine.param.rangeL2x(2)) ) and \\(n\\) is the number of values between \\(a\\) and \\(b\\) (i.e., cfg.machine.param.nL2x ) .param.rangeL2x, .param.rangeL2y [ numeric array ] range of hyperparameters for L2 regularization of \\(\\mathbf{w}_x\\) and \\(\\mathbf{w}_y\\) the default range is between 1 and the squared number of features .param.nL2x, .param.nL2y [ int ] number of values in the range of hyperparameters for L2 regularization of \\(\\mathbf{w}_x\\) and \\(\\mathbf{w}_y\\) (for details, see cfg.machine.param.L2x and cfg.machine.param.L2y above) .param.PCAx, .param.PCAy [ int ] number of principal components kept during the SVD step of CCA , PCA-CCA or RCCA if not provided, the function generates a logarithmically scaled numeric array based on the following equation, for instance, for \\(\\mathbf{X}\\) : $$ c_x = logspace(a, b, n) $$ where \\(c_x\\) is the hyperparameter (i.e., cfg.machine.param.PCAx ), \\(a\\) is the start of the logarithmic range (i.e., log(cfg.machine.param.rangePCAx(1)) ), \\(b\\) is the end of the logarithmic range (i.e, log(cfg.machine.param.rangePCAx(2)) ) and \\(n\\) is the number of values between \\(a\\) and \\(b\\) (i.e., cfg.machine.param.nPCAx ) .param.rangePCAx, .param.rangePCAy [ numeric array ] range of hyperparameters for number of principal components of data \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) the default range is between 1 and the rank of the training data .param.nPCAx, .param.nPCAy [ int ] number of values in the range of hyperparameters for principal components of \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) (for details, see cfg.machine.param.PCAx and cfg.machine.param.PCAy above) .param.VARx, .param.VARy [ int ] variance of data kept in the principal components during the SVD step of CCA , PCA-CCA or RCCA note that if variance is not sufficiently large, only very few (even 0 or 1) variables might be only kept if not provided, the function generates a linearly scaled numeric array based on the following equation, for instance, for \\(\\mathbf{X}\\) : $$ c_x = linspace(a, b, n) $$ where \\(c_x\\) is the hyperparameter (i.e., cfg.machine.param.VARx ), \\(a\\) is the start of the linear range (i.e., cfg.machine.param.rangeVARx(1)) ), \\(b\\) is the end of the linear range (i.e, cfg.machine.param.rangeVARx(2)) ) and \\(n\\) is the number of values between \\(a\\) and \\(b\\) (i.e., cfg.machine.param.nVARx ) .param.rangeVARx, .param.rangeVARy [ numeric array ] range of hyperparameters for explained variance in principal components of data \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) the default range is between 0.1 and 1 .param.nVARx, .param.nVARy [ int ] number of values in the range of hyperparameters for explained variance in principal components of \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) (for details, see cfg.machine.param.VARx and cfg.machine.param.VARy above) .svd.tol [ int --> 1e-10 ] eigenvalues smaller than tolerance are removed during the SVD step of CCA , PCA-CCA or RCCA .svd.varx [ float ] variance of \\(\\mathbf{X}\\) kept during the SVD step of CCA , PCA-CCA or RCCA default is 1 for CCA and 0.99 for RCCA note that if variance is not sufficiently large, only very few (even 0 or 1) variables might be only kept .svd.vary [ float ] variance of \\(\\mathbf{Y}\\) kept during the SVD step of CCA , PCA-CCA or RCCA default is 1 for CCA and 0.99 for RCCA note that if variance is not sufficiently large, only very few (even 0 or 1) variables might be only kept, i.e., for models with 1 output variable cfg.machine.svd.vary = 1 should be used .spls.tol [ int --> 1e-5 ] tolerance during SPLS convergence (for details, see Monteiro et al. 2016 ) .spls.maxiter [ int --> 100 ] maximum number of iterations during SPLS convergence (for details, see Monteiro et al. 2016 ) .simw [ char ] defines the type of similarity measure to assess the stability of model weights across splits 'correlation-Pearson' calculates absolute Pearson correlation between each pair of weights 'overlap-corrected' and 'overlap-uncorrected' calculate the overlap between each pair of sparse weigths in SPLS (for details, see Baldassarre et al. 2017 , Mihalik et al. 2020 ). frwork Details of framework with two main approaches. In the predictive (or machine learning) framework, the model is fitted on a training set and evaluated on a holdout set, and the statistical inference is based on out-of-sample correlation. In the descriptive framework, the model is fitted on the entire data, thus the statistical inference is based on in-sample correlation. The default values will change depending on the type of the framework. For further details on frameworks, see Analysis frameworks and Mihalik et al. 2022 . .name [ 'holdout', 'permutation' ] type of the framework note that permutation refers for the descriptive framework, even though both frameworks use permutation testing for statistical inference .flag [ char ] a short name to be appended to your analysis name which will then define the framework folder, see cfg.dir.frwork .nlevel [ int ] number of associative effects to be searched for .split.nout [ int ] number of outer splits/folds .split.propout [ float --> 0.2 ] proportion of holdout/test set in 'holdout' framework higher value is recommended for samples n<500 (e.g., 0.2-0.5), and lower value (e.g., 0.1) should be sufficient for samples n>1000 set to 0 in 'permutation' framework .split.nin [ int ] number of inner splits/folds .split.propin [ float --> 0.2 ] proportion of validation set in 'holdout' framework defl Deflation methods and strategies (for an introduction, see here in the Background page). In case we use multiple outer splits of the data, it is of interest which split to use as the basis for deflation. At the moment, we support the strategy of using the weights of the best data split (e.g., based on highest out-of-sample correlation) and deflate all other splits with it based on Monteiro et al. 2016 . .name [ 'generalized', 'pls-projection', 'pls-modeA', 'pls-regression' ] type of deflation .crit [ 'correl', 'pval+correl', 'correl+simwxy', 'correl+simwx+simwy', 'none' ] criterion to define best (i.e. most representative) data split to be used for deflation if 'none' set then each split is deflated by itself (i.e. they are treated independently) stat Statistical inference. Testing the generalizability of the models (i.e., using out-of-sample correlations) is one of our key recommendations. Furthermore, we advise to check the robustness (i.e., how many data splits are significant) and the stability (i.e., similarity of the weights) of the model instead of purely relying on a p-value. In the multiple-holdout framework, we support only statistical inference based on omnibus hypothesis proposed by Monteiro et al 2016 . .nperm [ int ] number of permutations .alpha [ float --> 0.05 ] threshold for significance testing note, that in the omnibus hypothesis approach, the threshold is adjusted by Bonferroni correction using the number of holdout sets .crit [ 'correl', 'covar' ] statistical inference within splits (i.e., one permutation test for each data split) based on given criterion .perm [ 'train', 'train+test' ] defines whether only the training examples or both the training and test examples are shuffled during permutation note, that the shuffling is performed within training and test sets, i.e., there is no leakage from training to test data data Details of the data and its properties including modalities, dimensionality and exchangeability block structure. The preprocessing strategy and the filenames with full path are also defined here. We highlight that when the data comes in a preprocessed format (e.g., imputed, z-scored), inference using holdout framework might be invalid (i.e., p-values inflated). .block [ boolean ] defines if there is a block structure in the data, i.e., examples are not independent of each other .nsubj [ int ] number of subjects/examples .conf [ boolean --> False ] defines if at least one of the data modalities should be deconfounded .preproc [ cell array --> {'impute', 'zscore'} ] data preprocessing strategy including missing value imputation ( 'impute' ), z-scoring ( 'zscore' ) and potentially deconfounding ( 'deconf' ) 'deconf' is automatically added if cfg.data.conf is True of note, data preprocessing is calculated on training data and applied to test data if 'holdout' framework used .mod [ cell array ] data modalaties to be used (e.g. {'X' 'Y'} ) .X.fname, .Y.fname, .C.fname [ filepath --> 'X.mat', 'Y.mat', 'C.mat' ] filename with full path to data \\(\\mathbf{X}\\) , \\(\\mathbf{Y}\\) , \\(\\mathbf{C}\\) .X.impute, .Y.impute, .C.impute [ 'median' ] strategy to impute missing values .X.deconf, .Y.deconf [ standard, none ] type of deconfounding 'standard' refers to regressing out confounds (i.e., removing confounds using regression) 'none' could be used if deconfounding is needed for the other modality but not the one where 'none' is set .X.nfeat, .Y.nfeat [ int ] number of features/variables in data \\(\\mathbf{X}\\) , \\(\\mathbf{Y}\\) .EB.split [ int or numeric array ] indexes of columns in EB matrix to use for defining exchangeability blocks for data partitioning if multi-level blocks are provided, most likely you need to provide 2 columns here as e.g., no cross-over across different family types (column 2) but shuffling across families (column 3) within same family type are allowed, in other words, families should be in the same data split (training or test) .EB.stat [ int or numeric array ] indexes of columns in EB matrix to use for defining exchangeability blocks for restricted permutations env Computation environment can be either local or cluster. In the latter case, we currently support SGE or SLURM scheduling systems (for details, see here in the Getting Started page). .comp [ 'local', 'cluster' ] computation environment .commit [ char ] SHA hash of the latest commit in git (i.e., toolkit vesion) for reproducibility .OS [ 'mac', 'unix', 'pc' ] operating system (OS) this information is used when transferring files between OS and updating paths (see cfg.dir fields and update_dir function) .fileend [ char --> '_1' ] suffix at the end of each file saved in the framework folder whilst running the experiment on a cluster for file storage efficiency and easier data transfer, we suggest to use the cleanup_files function after an experiment is completed to delete all intermediate and duplicate files .save.compression [ boolean --> 1 ] defines if files are saved with or without compression of note, loading an uncompressed file can be faster for very large data files .verbose [ int --> 2 ] level of verbosity to display information in the command line during the experiment 1 : detailed information with including elapsed time between time sensitive operations 2 : detailed information without elapsed time 3 : minimal information .seed.split , [ char --> 'default' or int ] seed before random data splitting value is passed as input to MATLAB's rng function .seed.model , [ int or char ] seed before training models in random order during hyperparameter optimization the default value is taken from cfg.env.fileend (converted to int) value is passed as input to MATLAB's rng function .seed.perm , [ char --> 'default' or int ] seed before training models in random order during permutation testing value is passed as input to MATLAB's rng function","title":"cfg"},{"location":"cfg/#dir","text":"Essential paths to your project, framework and processed data. The project folder should include a data folder where all the input data are stored. .project [ path ] full path to your project, such as 'PATH/TO/YOUR/PROJECT' .frwork [ path ] full path to your specific framework, such as 'PATH/TO/YOUR/PROJECT/framework/ANALYSIS_NAME' analysis name is generated from machine name and framework settings, for instance, an SPLS analysis with a single holdout set (20% of the data) and 10 validation sets (20% of the optimization set) and cfg.frwork.flag = '_TEST' will generate the spls_holdout1-0.20_subsamp10-0.20_TEST folder .load [ path ] full path to your processed data, such as 'PATH/TO/YOUR/PROJECT/framework/ANALYSIS_NAME/load' it possibly includes a preproc folder with the results of the preprocessing (e.g., mean, std of the features and the beta coefficients from the deconfounding) and an svd folder with the SVD results for computational efficiency of CCA , PCA-CCA and RCCA","title":"dir"},{"location":"cfg/#machine","text":"Algorithm to be used, its settings and information about hyperparameter optimization. Please make sure that you are familiar with the hyperparameter settings of the chosen algorithm, e.g. range and scale of hyperparameter values for grid search or number of PCA components. We strongly encourage to use RCCA , SPLS or PCA-CCA . For a discussion, see Mihalik et al. 2022 . .name [ 'cca', 'rcca', 'pls', 'spls' ] name of the algorithm PCA-CCA analysis is defined by cfg.machine.name = cca and cfg.machine.param = {'PCAx' 'PCAy'} RCCA finds a smooth solution between CCA and PLS using L2-norm regularization (see cfg.machine.param.L2x and cfg.machine.param.L2y below) .metric [ cell array ] metrics to evaluate the CCA/PLS algorithm note that each metric provided here will be saved on disc during hyperparameter optimization, training the main model and permutation testing, so they can be used for diagnostics later either they are used as criterion to evaluate a model or not options for each cell: 'trcorrel' (in-sample correlation), 'correl' (out-of-sample correlation measuring the generalizability of the model), 'trcovar' (in-sample covariance), 'covar' (out-of-sample covariance measuring the generalizability of the model), 'trexvarx' (in-sample percent explained variance by \\(\\mathbf{w}_x\\) ), 'exvarx' (out-of-sample percent explained variance by \\(\\mathbf{w}_x\\) ), 'trexvary' (in-sample percent explained variance by \\(\\mathbf{w}_y\\) ), 'exvary' (out-of-sample percent explained variance by \\(\\mathbf{w}_y\\) ), 'simwx' (similarity of \\(\\mathbf{w}_x\\) across training sets measuring the stability of the model), 'simwy' (similarity of \\(\\mathbf{w}_y\\) across training sets measuring the stability of the model), 'unsuc' (number of unsuccessful convergence in SPLS, should be a small number in general, for details, see cfg.machine.spls.maxiter ) .param.crit [ 'correl', 'correl+simwxy' ] criterion to select the best hyperparameter in general, we recommend using 'correl' (measuring the generalizability of the model) or 'correl+simwxy' (measuring the stability and the generalizability of the model) for SPLS 'correl+simwxy' calculates a 2-dimensional Euclidean distance from {1,1} based on out-of-sample correlation and the average similarity of \\(\\mathbf{w}_x\\) and \\(\\mathbf{w}_y\\) (i.e., it measures the deviation from perfect out-of-sample generalizability and perfect model stability, for details, see Mihalik et al. 2020 ) .param.name [ cell array ] name of the hyperparameters in the CCA/PLS model in each name, the first characters refers to the type of hyperparameter and the last character refers to the data modality, e.g., L1x for L1 regularization of \\(\\mathbf{w}_x\\) potential settings: {'L1x', 'L1y'} for L1 regularization in SPLS , {'L2x', 'L2y'} for L2 regularization in RCCA , {'PCAx', 'PCAy'} for number of PCA components in PCA_CCA an alternative to setting the number of PCA components is setting the explained variance by the PCA components using {'VARx', 'VARy'} in PCA_CCA .param.type [ 'factorial', 'matched' ] defines whether the grid search of hyperparameters should be based on a factorial combination of all hyperparameter values in the two data modalities (i.e., 100 combinations for 10 hyperparameter values in both data modalities) or the combination of hyperparameter values based on matching their indices (i.e., first index in one data modality is paired with first index in the other data modality) .param.L1x, .param.L1y [ int or numeric array ] amount of L1 regularization for \\(\\mathbf{w}_x\\) and \\(\\mathbf{w}_y\\) (see \\(c_x\\) and \\(c_y\\) in SPLS ) if not provided, the function generates a logarithmically scaled numeric array based on the following equation, for instance, for \\(\\mathbf{w}_x\\) : $$ c_x = logspace(a, b, n) $$ where \\(c_x\\) is the hyperparameter (i.e., cfg.machine.param.L1x ), \\(a\\) is the start of the logarithmic range (i.e., log(cfg.machine.param.rangeL1x(1)) ), \\(b\\) is the end of the logarithmic range (i.e., log(cfg.machine.param.rangeL1x(2)) ) and \\(n\\) is the number of values between \\(a\\) and \\(b\\) (i.e., cfg.machine.param.nL1x ) .param.rangeL1x, .param.rangeL1y [ numeric array ] range of the hyperparameters for L1 regularization of \\(\\mathbf{w}_x\\) and \\(\\mathbf{w}_y\\) the default range is between 1 and the square root of the number of features if a given value is outside of this default range then L1 regularization is not active and PLS is used instead of SPLS .param.nL1x, .param.nL1y [ int --> 10 ] number of values in the range of hyperparameters for L1 regularization of \\(\\mathbf{w}_x\\) and \\(\\mathbf{w}_y\\) (for details, see cfg.machine.param.L1x and cfg.machine.param.L1y above) .param.L2x, .param.L2y [ int ] amount of L2 regularization for \\(\\mathbf{w}_x\\) and \\(\\mathbf{w}_y\\) (see \\(c_x\\) and \\(c_y\\) in RCCA ) RCCA is equivalent to CCA if both values are set to \\(0\\) RCCA is equivalent to PLS if both values are set to \\(1\\) if not provided, the function generates a logarithmically scaled numeric array based on the following equation, for instance, for \\(\\mathbf{w}_x\\) : $$ c_x = 1 - logspace(a, b, n) $$ where \\(c_x\\) is the hyperparameter (i.e., cfg.machine.param.L2x ), \\(a\\) is the start of the logarithmic range (i.e., -log(cfg.machine.param.rangeL2x(1)) ), \\(b\\) is the end of the logarithmic range (i.e, -log(cfg.machine.param.rangeL2x(2)) ) and \\(n\\) is the number of values between \\(a\\) and \\(b\\) (i.e., cfg.machine.param.nL2x ) .param.rangeL2x, .param.rangeL2y [ numeric array ] range of hyperparameters for L2 regularization of \\(\\mathbf{w}_x\\) and \\(\\mathbf{w}_y\\) the default range is between 1 and the squared number of features .param.nL2x, .param.nL2y [ int ] number of values in the range of hyperparameters for L2 regularization of \\(\\mathbf{w}_x\\) and \\(\\mathbf{w}_y\\) (for details, see cfg.machine.param.L2x and cfg.machine.param.L2y above) .param.PCAx, .param.PCAy [ int ] number of principal components kept during the SVD step of CCA , PCA-CCA or RCCA if not provided, the function generates a logarithmically scaled numeric array based on the following equation, for instance, for \\(\\mathbf{X}\\) : $$ c_x = logspace(a, b, n) $$ where \\(c_x\\) is the hyperparameter (i.e., cfg.machine.param.PCAx ), \\(a\\) is the start of the logarithmic range (i.e., log(cfg.machine.param.rangePCAx(1)) ), \\(b\\) is the end of the logarithmic range (i.e, log(cfg.machine.param.rangePCAx(2)) ) and \\(n\\) is the number of values between \\(a\\) and \\(b\\) (i.e., cfg.machine.param.nPCAx ) .param.rangePCAx, .param.rangePCAy [ numeric array ] range of hyperparameters for number of principal components of data \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) the default range is between 1 and the rank of the training data .param.nPCAx, .param.nPCAy [ int ] number of values in the range of hyperparameters for principal components of \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) (for details, see cfg.machine.param.PCAx and cfg.machine.param.PCAy above) .param.VARx, .param.VARy [ int ] variance of data kept in the principal components during the SVD step of CCA , PCA-CCA or RCCA note that if variance is not sufficiently large, only very few (even 0 or 1) variables might be only kept if not provided, the function generates a linearly scaled numeric array based on the following equation, for instance, for \\(\\mathbf{X}\\) : $$ c_x = linspace(a, b, n) $$ where \\(c_x\\) is the hyperparameter (i.e., cfg.machine.param.VARx ), \\(a\\) is the start of the linear range (i.e., cfg.machine.param.rangeVARx(1)) ), \\(b\\) is the end of the linear range (i.e, cfg.machine.param.rangeVARx(2)) ) and \\(n\\) is the number of values between \\(a\\) and \\(b\\) (i.e., cfg.machine.param.nVARx ) .param.rangeVARx, .param.rangeVARy [ numeric array ] range of hyperparameters for explained variance in principal components of data \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) the default range is between 0.1 and 1 .param.nVARx, .param.nVARy [ int ] number of values in the range of hyperparameters for explained variance in principal components of \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) (for details, see cfg.machine.param.VARx and cfg.machine.param.VARy above) .svd.tol [ int --> 1e-10 ] eigenvalues smaller than tolerance are removed during the SVD step of CCA , PCA-CCA or RCCA .svd.varx [ float ] variance of \\(\\mathbf{X}\\) kept during the SVD step of CCA , PCA-CCA or RCCA default is 1 for CCA and 0.99 for RCCA note that if variance is not sufficiently large, only very few (even 0 or 1) variables might be only kept .svd.vary [ float ] variance of \\(\\mathbf{Y}\\) kept during the SVD step of CCA , PCA-CCA or RCCA default is 1 for CCA and 0.99 for RCCA note that if variance is not sufficiently large, only very few (even 0 or 1) variables might be only kept, i.e., for models with 1 output variable cfg.machine.svd.vary = 1 should be used .spls.tol [ int --> 1e-5 ] tolerance during SPLS convergence (for details, see Monteiro et al. 2016 ) .spls.maxiter [ int --> 100 ] maximum number of iterations during SPLS convergence (for details, see Monteiro et al. 2016 ) .simw [ char ] defines the type of similarity measure to assess the stability of model weights across splits 'correlation-Pearson' calculates absolute Pearson correlation between each pair of weights 'overlap-corrected' and 'overlap-uncorrected' calculate the overlap between each pair of sparse weigths in SPLS (for details, see Baldassarre et al. 2017 , Mihalik et al. 2020 ).","title":"machine"},{"location":"cfg/#frwork","text":"Details of framework with two main approaches. In the predictive (or machine learning) framework, the model is fitted on a training set and evaluated on a holdout set, and the statistical inference is based on out-of-sample correlation. In the descriptive framework, the model is fitted on the entire data, thus the statistical inference is based on in-sample correlation. The default values will change depending on the type of the framework. For further details on frameworks, see Analysis frameworks and Mihalik et al. 2022 . .name [ 'holdout', 'permutation' ] type of the framework note that permutation refers for the descriptive framework, even though both frameworks use permutation testing for statistical inference .flag [ char ] a short name to be appended to your analysis name which will then define the framework folder, see cfg.dir.frwork .nlevel [ int ] number of associative effects to be searched for .split.nout [ int ] number of outer splits/folds .split.propout [ float --> 0.2 ] proportion of holdout/test set in 'holdout' framework higher value is recommended for samples n<500 (e.g., 0.2-0.5), and lower value (e.g., 0.1) should be sufficient for samples n>1000 set to 0 in 'permutation' framework .split.nin [ int ] number of inner splits/folds .split.propin [ float --> 0.2 ] proportion of validation set in 'holdout' framework","title":"frwork"},{"location":"cfg/#defl","text":"Deflation methods and strategies (for an introduction, see here in the Background page). In case we use multiple outer splits of the data, it is of interest which split to use as the basis for deflation. At the moment, we support the strategy of using the weights of the best data split (e.g., based on highest out-of-sample correlation) and deflate all other splits with it based on Monteiro et al. 2016 . .name [ 'generalized', 'pls-projection', 'pls-modeA', 'pls-regression' ] type of deflation .crit [ 'correl', 'pval+correl', 'correl+simwxy', 'correl+simwx+simwy', 'none' ] criterion to define best (i.e. most representative) data split to be used for deflation if 'none' set then each split is deflated by itself (i.e. they are treated independently)","title":"defl"},{"location":"cfg/#stat","text":"Statistical inference. Testing the generalizability of the models (i.e., using out-of-sample correlations) is one of our key recommendations. Furthermore, we advise to check the robustness (i.e., how many data splits are significant) and the stability (i.e., similarity of the weights) of the model instead of purely relying on a p-value. In the multiple-holdout framework, we support only statistical inference based on omnibus hypothesis proposed by Monteiro et al 2016 . .nperm [ int ] number of permutations .alpha [ float --> 0.05 ] threshold for significance testing note, that in the omnibus hypothesis approach, the threshold is adjusted by Bonferroni correction using the number of holdout sets .crit [ 'correl', 'covar' ] statistical inference within splits (i.e., one permutation test for each data split) based on given criterion .perm [ 'train', 'train+test' ] defines whether only the training examples or both the training and test examples are shuffled during permutation note, that the shuffling is performed within training and test sets, i.e., there is no leakage from training to test data","title":"stat"},{"location":"cfg/#data","text":"Details of the data and its properties including modalities, dimensionality and exchangeability block structure. The preprocessing strategy and the filenames with full path are also defined here. We highlight that when the data comes in a preprocessed format (e.g., imputed, z-scored), inference using holdout framework might be invalid (i.e., p-values inflated). .block [ boolean ] defines if there is a block structure in the data, i.e., examples are not independent of each other .nsubj [ int ] number of subjects/examples .conf [ boolean --> False ] defines if at least one of the data modalities should be deconfounded .preproc [ cell array --> {'impute', 'zscore'} ] data preprocessing strategy including missing value imputation ( 'impute' ), z-scoring ( 'zscore' ) and potentially deconfounding ( 'deconf' ) 'deconf' is automatically added if cfg.data.conf is True of note, data preprocessing is calculated on training data and applied to test data if 'holdout' framework used .mod [ cell array ] data modalaties to be used (e.g. {'X' 'Y'} ) .X.fname, .Y.fname, .C.fname [ filepath --> 'X.mat', 'Y.mat', 'C.mat' ] filename with full path to data \\(\\mathbf{X}\\) , \\(\\mathbf{Y}\\) , \\(\\mathbf{C}\\) .X.impute, .Y.impute, .C.impute [ 'median' ] strategy to impute missing values .X.deconf, .Y.deconf [ standard, none ] type of deconfounding 'standard' refers to regressing out confounds (i.e., removing confounds using regression) 'none' could be used if deconfounding is needed for the other modality but not the one where 'none' is set .X.nfeat, .Y.nfeat [ int ] number of features/variables in data \\(\\mathbf{X}\\) , \\(\\mathbf{Y}\\) .EB.split [ int or numeric array ] indexes of columns in EB matrix to use for defining exchangeability blocks for data partitioning if multi-level blocks are provided, most likely you need to provide 2 columns here as e.g., no cross-over across different family types (column 2) but shuffling across families (column 3) within same family type are allowed, in other words, families should be in the same data split (training or test) .EB.stat [ int or numeric array ] indexes of columns in EB matrix to use for defining exchangeability blocks for restricted permutations","title":"data"},{"location":"cfg/#env","text":"Computation environment can be either local or cluster. In the latter case, we currently support SGE or SLURM scheduling systems (for details, see here in the Getting Started page). .comp [ 'local', 'cluster' ] computation environment .commit [ char ] SHA hash of the latest commit in git (i.e., toolkit vesion) for reproducibility .OS [ 'mac', 'unix', 'pc' ] operating system (OS) this information is used when transferring files between OS and updating paths (see cfg.dir fields and update_dir function) .fileend [ char --> '_1' ] suffix at the end of each file saved in the framework folder whilst running the experiment on a cluster for file storage efficiency and easier data transfer, we suggest to use the cleanup_files function after an experiment is completed to delete all intermediate and duplicate files .save.compression [ boolean --> 1 ] defines if files are saved with or without compression of note, loading an uncompressed file can be faster for very large data files .verbose [ int --> 2 ] level of verbosity to display information in the command line during the experiment 1 : detailed information with including elapsed time between time sensitive operations 2 : detailed information without elapsed time 3 : minimal information .seed.split , [ char --> 'default' or int ] seed before random data splitting value is passed as input to MATLAB's rng function .seed.model , [ int or char ] seed before training models in random order during hyperparameter optimization the default value is taken from cfg.env.fileend (converted to int) value is passed as input to MATLAB's rng function .seed.perm , [ char --> 'default' or int ] seed before training models in random order during permutation testing value is passed as input to MATLAB's rng function","title":"env"},{"location":"full_demo/","text":"Full demonstration using simulated data In the following, we will outline a complete demonstration of how to install the CCA/PLS toolkit, and generate some of the results that are presented in the accompanying tutorial paper ( Mihalik et al. 2022 ). Computation time on a single machine is about 1-3 hours. For more details, please see the in-depth online documentation of the toolkit available at https://mlnl.github.io/cca_pls_toolkit/ . Installation and prerequisites First, if you have Git already installed, clone the repository from Github using the following command in a terminal window under MacOS and Linux or in a power shell window under Microsoft Windows. Bash git clone https://github.com/mlnl/cca_pls_toolkit In case you don\u2019t want to clone the repository via Git, you can also go to https://github.com/mlnl/cca_pls_toolkit and download all scripts as a zip folder and unzip into a directory of your choice. After the toolkit is downloaded, open MATLAB and go to the folder containing the toolkit (e.g., double-click on the toolbox folder in MATLAB). To initialize the toolkit, run the following lines in the MATLAB command window: Matlab mkdir external set_path ; Dependencies For this demonstration, we need to add an additional MATLAB toolbox (PALM). For this, download PALM manually using this link ( https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/PALM/UserGuide ), copy the PALM folder into the external folder of the CCA/PLS toolkit, then finally add PALM to the MATLAB path using the following line in the MATLAB command window: Matlab set_path ( 'PALM' ); PALM is a toolbox that allows statistical inference using permutation testing whilst taking into account the dependencies in your data (e.g., family structure or diagnosis of subjects). Analysis setup In the following, we will run a Sparse Partial Least Squares (SPLS) analysis on low-dimensional simulated data. In general, the CCA/PLS toolkit uses a nested MATLAB structure to define all steps involved in a CCA/PLS analysis, which includes the exact algorithm, the type of deflation, the validation and statistical inference and so on. We call this configuration structure cfg and we will set its fields using the standard dot notation in MATLAB. For more information on all possible configurations, see the toolkit documentation. You can either follow this demo and copy and paste the relevant code sections into the MATLAB command window one at a time or you can simply run the demo_simul_paper.m script located in the demo folder of the toolkit. Project definition First, we specify the demo folder as our project directory in the cfg structure. Matlab % Project folder cfg . dir . project = fileparts ( mfilename ( 'fullpath' )); If you are working with your own data and from within a different folder, you need to change this variable to contain the path to your project. It is important that this folder you specify will also contain a 'data' folder where your data is located. Data The data used in this demo has already been created and saved to a folder under demo/data using the generate_data.m function with 1000 examples, 100 features in both data modalities (of which 10% include signal linking the two modalities) and noise level 2. The two modalities of data are stored in X.mat and Y.mat files. If you are using your own data, please make sure to create files that will match the simulated data structure, i.e., create X.mat and Y.mat files containing simple MATLAB arrays with rows for examples and columns for features. Machine For this demo, we will use an SPLS algorithm. All CCA/PLS models and their settings (e.g., amount or regularization) can be specified using the .machine field of the cfg structure. In this case, we will set the machine.name field to 'spls' to run SPLS. The metrics used to evaluate the CCA/PLS algorithms can be defined in the .machine.metric field of the cfg structure. Here, we specify in-sample correlation ('trcorrel'), out-of-sample correlation ('correl') measuring the generalizability of the model, similarity of the X and Y weights ('simwx', 'simwy') measuring the stability of the model across different training sets of data as well as the explained variance in the training set of X and Y ('trexvarx', 'trexvary'). To select the best hyperparameter (i.e., L1-norm regularization for SPLS), we will use generalizability (measured as average out-of-sample correlation on the validation sets) as optimization criterion. This is set by .machine.param.crit = 'correl' . Finally, .machine.simw defines the type of similarity measure used to assess the stability of model weights across splits. We set this to 'correlation-Pearson' which will calculate Pearson correlation between each pair of weights. Matlab % Machine settings cfg . machine . name = 'spls' ; cfg . machine . metric = { 'trcorrel' 'correl' 'simwx' 'simwy' ... 'trexvarx' 'trexvary' }; cfg . machine . param . crit = 'correl' ; cfg . machine . simw = 'correlation-Pearson' ; Framework The .frwork field defines the general framework used in the analysis. We support two main approaches in the CCA/PLS toolkit. In a 'holdout' predictive (or machine learning) framework, the data is divided into training and test sets by randomly subsampling subjects (see Monteiro et al. 2016 ). In a 'permutation' descriptive framework, the data is not splitted, focusing on in-sample statistical evaluation (see e.g., Smith et al. 2015 ). In this demo, we use the holdout framework with 10 inner and 10 outer data splits. For additional details, see the reference above, the accompanying tutorial paper ( Mihalik et al. 2022 ) or here . Matlab % Framework settings cfg . frwork . name = 'holdout' ; cfg . frwork . split . nout = 10 ; cfg . frwork . split . nin = 10 ; Deflation Next, we set the deflation of SPLS. In this demo, We will use PLS-mode A deflation. For more details on deflation strategies, see the accompanying tutorial paper or the online documentation of the toolkit. Matlab % Deflation settings cfg . defl . name = 'pls-modeA' ; Environment Next, we set the computational environment for the toolkit. As our data is relatively low-dimensional (i.e., number of features is not too high) SPLS will run quickly on a standard computer localy. For time-consuming analyses, this can be changed to a computer cluster environment. Matlab % Environment settings cfg . env . comp = 'local' ; Statistical inference In the last step, we define how the significance testing is performed. To reproduce the results reported in the accompanying tutorial paper, set this to 1000 permutations. If you need to save computation time, the number of permutations can be reduced. Please be aware, however, that too few permutation runs will make the precision of the calculated p-value low (e.g., 100 permutations allow to have a \\(\\textit{p}=0.01\\) at most). Matlab % Number of permutations cfg . stat . nperm = 1000 ; Run analysis To run the analysis, we simply update our cfg structure to add all necessary default values that we did not explicitly define and then run the main function. After the analysis, we clean up all the intermediate files that were saved during analysis to clean up disk space. This analysis will run for about 1-3 hours on a standard computer. Matlab % Update cfg with defaults cfg = cfg_defaults ( cfg ); % Run analysis main ( cfg ); % Clean up analysis files to save disc space cleanup_files ( cfg ); Results After running the analysis, you will notice that a framework folder has been automatically created by the toolkit inside the demo folder, containing all of the results. Inside the framework folder, another folder called spls_holdout10-0.20_subsamp10-0.20 has been created. This analysis folder should be unique to your analysis and by default it is named depending on the exact algorithm and analytic framework used. In this demo, it indicates that we have used an SPLS analysis with 10 holdout sets and 10 validation sets, each containing 20% of the data. As decribed in the accompanying tutorial paper, the associative effects identified by CCA/PLS are calculated iteratively, for instance, to be able to optimize the model\u2019s hyperparameters separately for each associative effect. These associative effects are called levels and the CCA/PLS toolkit will continue calculating additional associative effects whenever the current one reaches statistical significance. After running this demo, there are two folders under res in our analysis directory for level 1 and level 2. Since level 2 did not reach statistical significance, the toolkit stopped the computation at this associative effect. A quick overview of the results of level 1 can be found in the results_table.txt file located in the level 1 folder. In that file the correlation, associated p-value and, as this was an SPLS analysis, the number of selected features for the two data modalities are displayed for all specified splits (see Table below). split correl pval nfeatx nfeaty 1 0.4355 0.0010 12 9 2 0.3963 0.0010 12 12 3 0.3564 0.0010 33 58 4 0.3517 0.0010 29 4 5 0.4748 0.0010 11 10 6 0.4837 0.0010 9 10 7 0.3389 0.0010 11 15 8 0.3996 0.0010 12 57 9 0.3865 0.0010 10 13 10 0.4334 0.0010 10 11 All the additional results are stored automatically within corresponding .mat files. E.g., the results of the hyperparameter optimization are located in the grid folder separately for each level. Likewise, the results of the permutation testing are located in a folder called perm separately for each level. Loading the results To visualize the results of the SPLS analysis, a number of functions are provided in the toolkit to plot, e.g., the weights or latent variables of the individual associated effects. Before these functions can be called, they have to be added to the MATLAB path. To do so, run the following command in the MATLAB command window. Matlab % Set path for plotting set_path ( 'plot' ); Similar to the cfg structure that is used to define all analysis steps, a res structure can be created and used to load and visualize the results. First, we will load the results of the first level by specifying the directory of our analysis and the level we want to analyze. This is most easily done by quickly loading the cfg_1.mat file located in our results directory. The res_defaults function will then load the necessary result structure automatically. Matlab % Load res res . dir . frwork = cfg . dir . frwork ; res . frwork . level = 1 ; res = res_defaults ( res , 'load' ); Plot projections To plot the data projections (or latent variables) that has been learnt by the SPLS model, the plot_proj function can be used (see Figure @fig:proj). In this demo, we will add an x and y label to the res structure and will pass this as first argument to the plot_proj function. Next, we specify the data modalities as cell array and the level of associative effect. In this example, we plot the projections of X and Y for the first associative effect. We set the fourth input parameter to 'osplit' so that the training and test data of the outer split will be used for the plot which is paired with the fifth argument defining the specific outer split we want to use. We set this to the best data split (highest out-of-sample correlation in the holdout set). The next argument specifies the colour-coding of the data using the training and test data as groups. Then we specify the low-level function that will plot the results. In this case it is 2d_group which will call the plot_proj_2d_group function. All the other arguments of the plot_proj function are optional. In this demo, we flip the sign of the projection. Note, that that sign of the model weights or latent variables is arbitrary and can be changed for convenience (e.g., to compare results across models). Finally, we set the properties of the figure, axes and legends as Name-Value pairs. Matlab % Plot data projections plot_proj ( res , { 'X' 'Y' }, res . frwork . level , 'osplit' , ... res . frwork . split . best , 'training+test' , '2d_group' , ... 'gen.figure.ext' , '.svg' , ... 'gen.figure.Position' , [ 0 0 500 400 ], ... 'gen.axes.Position' , [ 0.1798 0.1560 0.7252 0.7690 ], ... 'gen.axes.XLim' , [ - 5 4.9 ], 'gen.axes.YLim' , [ - 4.2 5 ], ... 'gen.axes.FontSize' , 22 , 'gen.legend.FontSize' , 22 , ... 'gen.legend.Location' , 'best' , ... 'proj.scatter.SizeData' , 120 , ... 'proj.scatter.MarkerFaceColor' , [ 0.3 0.3 0.9 ; 0.9 0.3 0.3 ], ... 'proj.scatter.MarkerEdgeColor' , 'k' , 'proj.lsline' , 'on' , ... 'proj.xlabel' , 'Modality 1 latent variable' , ... 'proj.ylabel' , 'Modality 2 latent variable' ); Both the training and the test set show high correlations between the two latent variables, indicating that the learnt associative effect generalizes well. Plot weights Plotting model weights heavily depends on the kind of data that has been used in the analysis. In case of our simulated data, we are interested if the model recovered the weights that were used for generating the data (these true model weights were automatically saved in our data folder as wX.mat and wY.mat ). We will use a stem plot with the true and recovered weights in different colors (see Figure @fig:weights1 and @fig:weights2). The res structure will need to be passed as first argument. Next, we specify the data modalities and the type of the modality as strings. In this demo, we set these to X or Y and simul . The following argument defines the outer data split we want to use and will be set to the best split (as above for the data projections). Then we specify the low-level function that will plot the results. In this demo, it is set to stem to call the plot_weight_stem function and create a simple stem plot. Finally, we set the properties of the figure, axes and legends as Name-Value pairs. Matlab % Plot X weights as stem plot plot_weight ( res , 'X' , 'simul' , res . frwork . split . best , 'stem' , ... 'gen.figure.ext' , '.svg' , ... 'gen.figure.Position' , [ 0 0 500 400 ], ... 'gen.axes.Position' , [ 0.1798 0.1560 0.7252 0.7690 ], ... 'gen.axes.YLim' , [ - 1.1 1.2 ], ... 'gen.axes.YTick' , [ - 1 : 0.5 : 1.2 ], ... 'gen.axes.FontSize' , 22 , 'gen.legend.FontSize' , 22 , ... 'gen.legend.Location' , 'NorthEast' , ... 'simul.xlabel' , 'Modality 1 variables' , ... 'simul.ylabel' , 'Weight' , 'simul.weight.norm' , 'minmax' ); The same plot can be generated for the Y data modality. All plots are automatically saved inside the res/level1 folder. Matlab % Plot Y weights as stem plot plot_weight ( res , 'Y' , 'simul' , res . frwork . split . best , 'stem' , ... 'gen.figure.ext' , '.svg' , ... 'gen.figure.Position' , [ 0 0 500 400 ], ... 'gen.axes.Position' , [ 0.1798 0.1560 0.7252 0.7690 ], ... 'gen.axes.YLim' , [ - 1.1 1.2 ], ... 'gen.axes.YTick' , [ - 1 : 0.5 : 1.2 ], ... 'gen.axes.FontSize' , 22 , 'gen.legend.FontSize' , 22 , ... 'gen.legend.Location' , 'NorthEast' , ... 'simul.xlabel' , 'Modality 2 variables' , ... 'simul.ylabel' , 'Weight' , 'simul.weight.norm' , 'minmax' ); In this demo, we had only 1 significant associative effect. In case there are multiple significant associative effects, the process for plotting the results can be repeated for each level. Here, we plotted the results of the best data split, but other data splits can be also visualized in a similar manner. For more information on different algorithms, hyperparameter optimization, default parameters or additional plotting functions, see the CCA/PLS toolkit documentation.","title":"Full Demo"},{"location":"full_demo/#full-demonstration-using-simulated-data","text":"In the following, we will outline a complete demonstration of how to install the CCA/PLS toolkit, and generate some of the results that are presented in the accompanying tutorial paper ( Mihalik et al. 2022 ). Computation time on a single machine is about 1-3 hours. For more details, please see the in-depth online documentation of the toolkit available at https://mlnl.github.io/cca_pls_toolkit/ .","title":"Full demonstration using simulated data"},{"location":"full_demo/#installation-and-prerequisites","text":"First, if you have Git already installed, clone the repository from Github using the following command in a terminal window under MacOS and Linux or in a power shell window under Microsoft Windows. Bash git clone https://github.com/mlnl/cca_pls_toolkit In case you don\u2019t want to clone the repository via Git, you can also go to https://github.com/mlnl/cca_pls_toolkit and download all scripts as a zip folder and unzip into a directory of your choice. After the toolkit is downloaded, open MATLAB and go to the folder containing the toolkit (e.g., double-click on the toolbox folder in MATLAB). To initialize the toolkit, run the following lines in the MATLAB command window: Matlab mkdir external set_path ;","title":"Installation and prerequisites"},{"location":"full_demo/#dependencies","text":"For this demonstration, we need to add an additional MATLAB toolbox (PALM). For this, download PALM manually using this link ( https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/PALM/UserGuide ), copy the PALM folder into the external folder of the CCA/PLS toolkit, then finally add PALM to the MATLAB path using the following line in the MATLAB command window: Matlab set_path ( 'PALM' ); PALM is a toolbox that allows statistical inference using permutation testing whilst taking into account the dependencies in your data (e.g., family structure or diagnosis of subjects).","title":"Dependencies"},{"location":"full_demo/#analysis-setup","text":"In the following, we will run a Sparse Partial Least Squares (SPLS) analysis on low-dimensional simulated data. In general, the CCA/PLS toolkit uses a nested MATLAB structure to define all steps involved in a CCA/PLS analysis, which includes the exact algorithm, the type of deflation, the validation and statistical inference and so on. We call this configuration structure cfg and we will set its fields using the standard dot notation in MATLAB. For more information on all possible configurations, see the toolkit documentation. You can either follow this demo and copy and paste the relevant code sections into the MATLAB command window one at a time or you can simply run the demo_simul_paper.m script located in the demo folder of the toolkit.","title":"Analysis setup"},{"location":"full_demo/#project-definition","text":"First, we specify the demo folder as our project directory in the cfg structure. Matlab % Project folder cfg . dir . project = fileparts ( mfilename ( 'fullpath' )); If you are working with your own data and from within a different folder, you need to change this variable to contain the path to your project. It is important that this folder you specify will also contain a 'data' folder where your data is located.","title":"Project definition"},{"location":"full_demo/#data","text":"The data used in this demo has already been created and saved to a folder under demo/data using the generate_data.m function with 1000 examples, 100 features in both data modalities (of which 10% include signal linking the two modalities) and noise level 2. The two modalities of data are stored in X.mat and Y.mat files. If you are using your own data, please make sure to create files that will match the simulated data structure, i.e., create X.mat and Y.mat files containing simple MATLAB arrays with rows for examples and columns for features.","title":"Data"},{"location":"full_demo/#machine","text":"For this demo, we will use an SPLS algorithm. All CCA/PLS models and their settings (e.g., amount or regularization) can be specified using the .machine field of the cfg structure. In this case, we will set the machine.name field to 'spls' to run SPLS. The metrics used to evaluate the CCA/PLS algorithms can be defined in the .machine.metric field of the cfg structure. Here, we specify in-sample correlation ('trcorrel'), out-of-sample correlation ('correl') measuring the generalizability of the model, similarity of the X and Y weights ('simwx', 'simwy') measuring the stability of the model across different training sets of data as well as the explained variance in the training set of X and Y ('trexvarx', 'trexvary'). To select the best hyperparameter (i.e., L1-norm regularization for SPLS), we will use generalizability (measured as average out-of-sample correlation on the validation sets) as optimization criterion. This is set by .machine.param.crit = 'correl' . Finally, .machine.simw defines the type of similarity measure used to assess the stability of model weights across splits. We set this to 'correlation-Pearson' which will calculate Pearson correlation between each pair of weights. Matlab % Machine settings cfg . machine . name = 'spls' ; cfg . machine . metric = { 'trcorrel' 'correl' 'simwx' 'simwy' ... 'trexvarx' 'trexvary' }; cfg . machine . param . crit = 'correl' ; cfg . machine . simw = 'correlation-Pearson' ;","title":"Machine"},{"location":"full_demo/#framework","text":"The .frwork field defines the general framework used in the analysis. We support two main approaches in the CCA/PLS toolkit. In a 'holdout' predictive (or machine learning) framework, the data is divided into training and test sets by randomly subsampling subjects (see Monteiro et al. 2016 ). In a 'permutation' descriptive framework, the data is not splitted, focusing on in-sample statistical evaluation (see e.g., Smith et al. 2015 ). In this demo, we use the holdout framework with 10 inner and 10 outer data splits. For additional details, see the reference above, the accompanying tutorial paper ( Mihalik et al. 2022 ) or here . Matlab % Framework settings cfg . frwork . name = 'holdout' ; cfg . frwork . split . nout = 10 ; cfg . frwork . split . nin = 10 ;","title":"Framework"},{"location":"full_demo/#deflation","text":"Next, we set the deflation of SPLS. In this demo, We will use PLS-mode A deflation. For more details on deflation strategies, see the accompanying tutorial paper or the online documentation of the toolkit. Matlab % Deflation settings cfg . defl . name = 'pls-modeA' ;","title":"Deflation"},{"location":"full_demo/#environment","text":"Next, we set the computational environment for the toolkit. As our data is relatively low-dimensional (i.e., number of features is not too high) SPLS will run quickly on a standard computer localy. For time-consuming analyses, this can be changed to a computer cluster environment. Matlab % Environment settings cfg . env . comp = 'local' ;","title":"Environment"},{"location":"full_demo/#statistical-inference","text":"In the last step, we define how the significance testing is performed. To reproduce the results reported in the accompanying tutorial paper, set this to 1000 permutations. If you need to save computation time, the number of permutations can be reduced. Please be aware, however, that too few permutation runs will make the precision of the calculated p-value low (e.g., 100 permutations allow to have a \\(\\textit{p}=0.01\\) at most). Matlab % Number of permutations cfg . stat . nperm = 1000 ;","title":"Statistical inference"},{"location":"full_demo/#run-analysis","text":"To run the analysis, we simply update our cfg structure to add all necessary default values that we did not explicitly define and then run the main function. After the analysis, we clean up all the intermediate files that were saved during analysis to clean up disk space. This analysis will run for about 1-3 hours on a standard computer. Matlab % Update cfg with defaults cfg = cfg_defaults ( cfg ); % Run analysis main ( cfg ); % Clean up analysis files to save disc space cleanup_files ( cfg );","title":"Run analysis"},{"location":"full_demo/#results","text":"After running the analysis, you will notice that a framework folder has been automatically created by the toolkit inside the demo folder, containing all of the results. Inside the framework folder, another folder called spls_holdout10-0.20_subsamp10-0.20 has been created. This analysis folder should be unique to your analysis and by default it is named depending on the exact algorithm and analytic framework used. In this demo, it indicates that we have used an SPLS analysis with 10 holdout sets and 10 validation sets, each containing 20% of the data. As decribed in the accompanying tutorial paper, the associative effects identified by CCA/PLS are calculated iteratively, for instance, to be able to optimize the model\u2019s hyperparameters separately for each associative effect. These associative effects are called levels and the CCA/PLS toolkit will continue calculating additional associative effects whenever the current one reaches statistical significance. After running this demo, there are two folders under res in our analysis directory for level 1 and level 2. Since level 2 did not reach statistical significance, the toolkit stopped the computation at this associative effect. A quick overview of the results of level 1 can be found in the results_table.txt file located in the level 1 folder. In that file the correlation, associated p-value and, as this was an SPLS analysis, the number of selected features for the two data modalities are displayed for all specified splits (see Table below). split correl pval nfeatx nfeaty 1 0.4355 0.0010 12 9 2 0.3963 0.0010 12 12 3 0.3564 0.0010 33 58 4 0.3517 0.0010 29 4 5 0.4748 0.0010 11 10 6 0.4837 0.0010 9 10 7 0.3389 0.0010 11 15 8 0.3996 0.0010 12 57 9 0.3865 0.0010 10 13 10 0.4334 0.0010 10 11 All the additional results are stored automatically within corresponding .mat files. E.g., the results of the hyperparameter optimization are located in the grid folder separately for each level. Likewise, the results of the permutation testing are located in a folder called perm separately for each level.","title":"Results"},{"location":"full_demo/#loading-the-results","text":"To visualize the results of the SPLS analysis, a number of functions are provided in the toolkit to plot, e.g., the weights or latent variables of the individual associated effects. Before these functions can be called, they have to be added to the MATLAB path. To do so, run the following command in the MATLAB command window. Matlab % Set path for plotting set_path ( 'plot' ); Similar to the cfg structure that is used to define all analysis steps, a res structure can be created and used to load and visualize the results. First, we will load the results of the first level by specifying the directory of our analysis and the level we want to analyze. This is most easily done by quickly loading the cfg_1.mat file located in our results directory. The res_defaults function will then load the necessary result structure automatically. Matlab % Load res res . dir . frwork = cfg . dir . frwork ; res . frwork . level = 1 ; res = res_defaults ( res , 'load' );","title":"Loading the results"},{"location":"full_demo/#plot-projections","text":"To plot the data projections (or latent variables) that has been learnt by the SPLS model, the plot_proj function can be used (see Figure @fig:proj). In this demo, we will add an x and y label to the res structure and will pass this as first argument to the plot_proj function. Next, we specify the data modalities as cell array and the level of associative effect. In this example, we plot the projections of X and Y for the first associative effect. We set the fourth input parameter to 'osplit' so that the training and test data of the outer split will be used for the plot which is paired with the fifth argument defining the specific outer split we want to use. We set this to the best data split (highest out-of-sample correlation in the holdout set). The next argument specifies the colour-coding of the data using the training and test data as groups. Then we specify the low-level function that will plot the results. In this case it is 2d_group which will call the plot_proj_2d_group function. All the other arguments of the plot_proj function are optional. In this demo, we flip the sign of the projection. Note, that that sign of the model weights or latent variables is arbitrary and can be changed for convenience (e.g., to compare results across models). Finally, we set the properties of the figure, axes and legends as Name-Value pairs. Matlab % Plot data projections plot_proj ( res , { 'X' 'Y' }, res . frwork . level , 'osplit' , ... res . frwork . split . best , 'training+test' , '2d_group' , ... 'gen.figure.ext' , '.svg' , ... 'gen.figure.Position' , [ 0 0 500 400 ], ... 'gen.axes.Position' , [ 0.1798 0.1560 0.7252 0.7690 ], ... 'gen.axes.XLim' , [ - 5 4.9 ], 'gen.axes.YLim' , [ - 4.2 5 ], ... 'gen.axes.FontSize' , 22 , 'gen.legend.FontSize' , 22 , ... 'gen.legend.Location' , 'best' , ... 'proj.scatter.SizeData' , 120 , ... 'proj.scatter.MarkerFaceColor' , [ 0.3 0.3 0.9 ; 0.9 0.3 0.3 ], ... 'proj.scatter.MarkerEdgeColor' , 'k' , 'proj.lsline' , 'on' , ... 'proj.xlabel' , 'Modality 1 latent variable' , ... 'proj.ylabel' , 'Modality 2 latent variable' ); Both the training and the test set show high correlations between the two latent variables, indicating that the learnt associative effect generalizes well.","title":"Plot projections"},{"location":"full_demo/#plot-weights","text":"Plotting model weights heavily depends on the kind of data that has been used in the analysis. In case of our simulated data, we are interested if the model recovered the weights that were used for generating the data (these true model weights were automatically saved in our data folder as wX.mat and wY.mat ). We will use a stem plot with the true and recovered weights in different colors (see Figure @fig:weights1 and @fig:weights2). The res structure will need to be passed as first argument. Next, we specify the data modalities and the type of the modality as strings. In this demo, we set these to X or Y and simul . The following argument defines the outer data split we want to use and will be set to the best split (as above for the data projections). Then we specify the low-level function that will plot the results. In this demo, it is set to stem to call the plot_weight_stem function and create a simple stem plot. Finally, we set the properties of the figure, axes and legends as Name-Value pairs. Matlab % Plot X weights as stem plot plot_weight ( res , 'X' , 'simul' , res . frwork . split . best , 'stem' , ... 'gen.figure.ext' , '.svg' , ... 'gen.figure.Position' , [ 0 0 500 400 ], ... 'gen.axes.Position' , [ 0.1798 0.1560 0.7252 0.7690 ], ... 'gen.axes.YLim' , [ - 1.1 1.2 ], ... 'gen.axes.YTick' , [ - 1 : 0.5 : 1.2 ], ... 'gen.axes.FontSize' , 22 , 'gen.legend.FontSize' , 22 , ... 'gen.legend.Location' , 'NorthEast' , ... 'simul.xlabel' , 'Modality 1 variables' , ... 'simul.ylabel' , 'Weight' , 'simul.weight.norm' , 'minmax' ); The same plot can be generated for the Y data modality. All plots are automatically saved inside the res/level1 folder. Matlab % Plot Y weights as stem plot plot_weight ( res , 'Y' , 'simul' , res . frwork . split . best , 'stem' , ... 'gen.figure.ext' , '.svg' , ... 'gen.figure.Position' , [ 0 0 500 400 ], ... 'gen.axes.Position' , [ 0.1798 0.1560 0.7252 0.7690 ], ... 'gen.axes.YLim' , [ - 1.1 1.2 ], ... 'gen.axes.YTick' , [ - 1 : 0.5 : 1.2 ], ... 'gen.axes.FontSize' , 22 , 'gen.legend.FontSize' , 22 , ... 'gen.legend.Location' , 'NorthEast' , ... 'simul.xlabel' , 'Modality 2 variables' , ... 'simul.ylabel' , 'Weight' , 'simul.weight.norm' , 'minmax' ); In this demo, we had only 1 significant associative effect. In case there are multiple significant associative effects, the process for plotting the results can be repeated for each level. Here, we plotted the results of the best data split, but other data splits can be also visualized in a similar manner. For more information on different algorithms, hyperparameter optimization, default parameters or additional plotting functions, see the CCA/PLS toolkit documentation.","title":"Plot weights"},{"location":"getting_started/","text":"Installation To install the CCA/PLS Toolkit, clone the repository from Github using the following command: Bash git clone https://github.com/mlnl/cca_pls_toolkit After the toolkit is downloaded, go to the folder containing the toolkit and open MATLAB. In general, we advise you to run all commands from this toolkit folder. To initialize the toolkit, run the following line in the MATLAB command window: Matlab set_path ; Dependencies The neuroimaging community has great visualization and other tools, therefore we decided to use some of these available tools for specific purposes in the toolkit. Depending on the analysis and plots you would like to make you will need to download some of the toolboxes below. We recommend two ways of adding toolboxes to your path: you can just add the toolboxes to the path in their current location if you already have them on your computer, you can add the toolboxes in a dedicated folder (called external ) inside the toolkit. For easier management of the dependencies, all toolboxes are stored in a dedicated folder within the toolkit. To create this folder, run the following line in the MATLAB command window: Matlab mkdir external Importantly, this external folder (and its content) is not added to .gitignore and thus it is not version controlled by git. On the one side, this is to accomodate the specific needs of users and only to use toolboxes that are essential for their specific analyses and plots. On the other side, this is to avoid that the toolkit gets unnecessarily large due to potentially large external toolboxes. Here is a complete list of toolboxes that you might need for using the toolkit: PALM ( Permutation Analysis of Linear Models , used for permutation testing) SPM ( Statistical Parametric Mapping , used for opening files and reading/writing .nii files) BrainNet Viewer (used for brain data visualization) AAL ( Automated Anatomical Labelling , used for region-of-interest (ROI) analyses) To know which dependencies you will need for your specific analysis, please see the Analysis and Visualization pages. Below, an example is provided to illustrate how to add toolboxes to the toolkit using PALM. For this, download PALM manually by the provided link, copy the PALM folder into your external folder, then finally add PALM to the MATLAB path using the following line in the MATLAB command window: Matlab set_path ( 'PALM' ); Toolbox overview The toolkit consists of two main parts: Analysis Visualization The reason behind this division is that whereas an analysis can be run on a cluster without a need for a graphical output, the visualization usually takes place on a local computer with a need for a graphical output. Of course, if both the analysis and visualization are done on a local computer, the two can be easily combined as demonstrated in the examples. Analysis The figure below illustrates the inputs and outputs of each analysis. The main inputs of the analysis are: cfg structure, which is a MATLAB variable created by the user for the configuration of all the analysis settings, X.mat and Y.mat files including the two modalities of input data (i.e., \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) matrices). Other input files can be also provided, for instance, C.mat file including the confounding variables of the analysis and \u00c8B.mat file including the (exchangeability) block structure of the data. For details on the cfg structure and the input data files, see Analysis . The main outputs of the analysis are: results_table.txt including the summary of the results, various .mat files including, for instance, information about the data splits, the results of hyperparameter optimization and the trained models (for details, see below). Next, we discuss the folder structure of your analysis and list the specific folders where the main input and output files are stored. As illustrated in the figure above, a project consists of a project folder with two subfolders: a data folder including the input data files, a framework folder including the results of the analyses with each analysis in a specific framework folder. We need to create the project and data folders manually and place our input data files within the data folder (illustrated by a red box in the figure). To generate simulated data, see the generate_data function. All the other folders and output files will be created by the toolkit during analysis. Our specific framework folder will be generated by the toolkit based on your CCA/PLS model and framework choice. For instance, an SPLS analysis with a single holdout set (20% of the data) and 10 validation sets (20% of the optimization set) will generate the spls_holdout1-0.20_subsamp10-0.20 folder. If you want to specify a custom name for this analysis, you can change cfg.frwork.flag from its default empty value, which will then append a flag to your specific framwork name. For instance, cfg.frwork.flag = '_TEST' will create the spls_holdout1-0.20_subsamp10-0.20_TEST folder. Each analysis will contain the following output files in the specific framework folder: cfg*.mat file including the cfg structure you created and filled up with other necessary default by cfg_defaults , outmat*.mat file including the training and test indexes of the outer splits of the data (i.e., optimization and holdout sets), inmat*.mat file including the training and test indexes of the inner splits of the data (i.e., inner training and validation sets). The other output files are stored in specific folders with each folder having one or multiple levels of results, where each level stands for an associative effect (e.g., first associative effect in folder level1 , second associative effect in folder level2 ): grid folder including the results of the hyperparameter optimization in grid*.mat files, load folder with a preproc folder including the preprocessed (e.g., z-scored) data in preproc*.mat files and an svd folder including the Singular Value Decomposition (SVD) of the preprocessed data in *svd*.mat files (SVD is needed for the computational efficiency of the toolkit), perm folder including the results of the permutation testing in perm*.mat files, res folder including the best (or fixed) hyperparameters in param*.mat file, the results of the main trained models in model*.mat file, additional results in res*.mat file and the summary of the results in results_table.txt file. For additional details on the output data files, see Analysis . Visualization The figure below illustrates the inputs and outputs of visualizing the results. The main inputs of the visualization are: res structure, which is a MATLAB variable loaded from res*.mat and appended with settings for visualization, .mat files either as outputs of the analysis or other files including data (e.g., mask.mat including a mask for connectivity data) or other settings for visualization (e.g., options.mat including BrainNet Viewer settings), .csv files, which are label files including information about the variables in your \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) matrices, .nv file, which is a surface mesh file in BrainNet Viewer used as a template to overlay your brain weights on, .nii files, which can be an atlas file defining regions of interest (ROI) in the brain or a mask file for voxel-wise structural MRI data. The main outputs of the visualization are: images of figures, in any requested standard file format, e.g., .fig , .png , .svg , .csv files including information about the plotted results (e.g., ordered model weights). For additional details on the res structure and the other input and output data files of visualization, see Visualization . Info We also highly recommend that you go through the defaults of cfg and res so that you understand thoroughly the analysis and visualization settings. You can find a detailed documentation of each high-level function of the toolkit under the main menu Functions, for instance, see cfg_defaults . Finally, if you want to get started and run an experiment on your local computer, see Demo for a complete analysis and visualization. In addition, we provide three simple examples to generate and analyse simulated data , simulated structural MRI data , and simulated fMRI data . Next, we briefly discuss how to run an analysis on a cluster. Running on a cluster The toolkit can be run in multiple MATLAB instances, e.g., in a cluster environment. If you use SGE or SLURM scheduling systems, you simply need to send the same analysis to different nodes and the toolkit will take care of the rest. If you use a different scheduling system then a one-line modification of the cfg_defaults function is needed to account for your scheduling system and you are ready to go. Feel free to get in touch with us to help you set this up. Here is a brief description of what happens under the hood when the toolkit is running on different MATLAB instances. Although MATLAB can load the same .mat file from different MATLAB instances, it cannot save to the same .mat file . To work around this, the toolkit appends the unique ID of the computing node/job at the end of each .mat file (in a local environment, the ID is set to _1 by default), so even if different jobs are saving the same content simultaneously, they will do it into different files. In addition, there are a handful of wrapper functions to save, search and load these .mat files and the following mechanisms are in place to share computational cost: jobs save computations to .mat files regularly on disc, there is a random seed before each time consuming operation in the toolkit (e.g., grid search and permutation testing), hence different jobs will most likely perform different computations, jobs can load .mat files computed by other jobs, jobs regularly check what .mat files are available and they only start a new computation if that has not yet been performed by another job, if two MATLAB instances are saving the same computation to a .mat file then they will write to different files due to their different job ID-s. You might ask: doesn't this computational strategy create a lot of intermediate and some duplicate files? Indeed, that is the case, however, there is a cleanup_files function that allows to remove all the intermediate and duplicate files after your analysis is done. Do not forget to use this as otherwise you might exceed your disc space or have difficulties to move your results around, e.g., from a cluster to a local computer. An extra benefit of this computational strategy is that in case your analysis is aborted (e.g., you run out of allocated time on a cluster), you can always restart the same analysis and it will catch up with the computations where it was aborted.","title":"Getting Started"},{"location":"getting_started/#installation","text":"To install the CCA/PLS Toolkit, clone the repository from Github using the following command: Bash git clone https://github.com/mlnl/cca_pls_toolkit After the toolkit is downloaded, go to the folder containing the toolkit and open MATLAB. In general, we advise you to run all commands from this toolkit folder. To initialize the toolkit, run the following line in the MATLAB command window: Matlab set_path ;","title":"Installation"},{"location":"getting_started/#dependencies","text":"The neuroimaging community has great visualization and other tools, therefore we decided to use some of these available tools for specific purposes in the toolkit. Depending on the analysis and plots you would like to make you will need to download some of the toolboxes below. We recommend two ways of adding toolboxes to your path: you can just add the toolboxes to the path in their current location if you already have them on your computer, you can add the toolboxes in a dedicated folder (called external ) inside the toolkit. For easier management of the dependencies, all toolboxes are stored in a dedicated folder within the toolkit. To create this folder, run the following line in the MATLAB command window: Matlab mkdir external Importantly, this external folder (and its content) is not added to .gitignore and thus it is not version controlled by git. On the one side, this is to accomodate the specific needs of users and only to use toolboxes that are essential for their specific analyses and plots. On the other side, this is to avoid that the toolkit gets unnecessarily large due to potentially large external toolboxes. Here is a complete list of toolboxes that you might need for using the toolkit: PALM ( Permutation Analysis of Linear Models , used for permutation testing) SPM ( Statistical Parametric Mapping , used for opening files and reading/writing .nii files) BrainNet Viewer (used for brain data visualization) AAL ( Automated Anatomical Labelling , used for region-of-interest (ROI) analyses) To know which dependencies you will need for your specific analysis, please see the Analysis and Visualization pages. Below, an example is provided to illustrate how to add toolboxes to the toolkit using PALM. For this, download PALM manually by the provided link, copy the PALM folder into your external folder, then finally add PALM to the MATLAB path using the following line in the MATLAB command window: Matlab set_path ( 'PALM' );","title":"Dependencies"},{"location":"getting_started/#toolbox-overview","text":"The toolkit consists of two main parts: Analysis Visualization The reason behind this division is that whereas an analysis can be run on a cluster without a need for a graphical output, the visualization usually takes place on a local computer with a need for a graphical output. Of course, if both the analysis and visualization are done on a local computer, the two can be easily combined as demonstrated in the examples.","title":"Toolbox overview"},{"location":"getting_started/#analysis","text":"The figure below illustrates the inputs and outputs of each analysis. The main inputs of the analysis are: cfg structure, which is a MATLAB variable created by the user for the configuration of all the analysis settings, X.mat and Y.mat files including the two modalities of input data (i.e., \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) matrices). Other input files can be also provided, for instance, C.mat file including the confounding variables of the analysis and \u00c8B.mat file including the (exchangeability) block structure of the data. For details on the cfg structure and the input data files, see Analysis . The main outputs of the analysis are: results_table.txt including the summary of the results, various .mat files including, for instance, information about the data splits, the results of hyperparameter optimization and the trained models (for details, see below). Next, we discuss the folder structure of your analysis and list the specific folders where the main input and output files are stored. As illustrated in the figure above, a project consists of a project folder with two subfolders: a data folder including the input data files, a framework folder including the results of the analyses with each analysis in a specific framework folder. We need to create the project and data folders manually and place our input data files within the data folder (illustrated by a red box in the figure). To generate simulated data, see the generate_data function. All the other folders and output files will be created by the toolkit during analysis. Our specific framework folder will be generated by the toolkit based on your CCA/PLS model and framework choice. For instance, an SPLS analysis with a single holdout set (20% of the data) and 10 validation sets (20% of the optimization set) will generate the spls_holdout1-0.20_subsamp10-0.20 folder. If you want to specify a custom name for this analysis, you can change cfg.frwork.flag from its default empty value, which will then append a flag to your specific framwork name. For instance, cfg.frwork.flag = '_TEST' will create the spls_holdout1-0.20_subsamp10-0.20_TEST folder. Each analysis will contain the following output files in the specific framework folder: cfg*.mat file including the cfg structure you created and filled up with other necessary default by cfg_defaults , outmat*.mat file including the training and test indexes of the outer splits of the data (i.e., optimization and holdout sets), inmat*.mat file including the training and test indexes of the inner splits of the data (i.e., inner training and validation sets). The other output files are stored in specific folders with each folder having one or multiple levels of results, where each level stands for an associative effect (e.g., first associative effect in folder level1 , second associative effect in folder level2 ): grid folder including the results of the hyperparameter optimization in grid*.mat files, load folder with a preproc folder including the preprocessed (e.g., z-scored) data in preproc*.mat files and an svd folder including the Singular Value Decomposition (SVD) of the preprocessed data in *svd*.mat files (SVD is needed for the computational efficiency of the toolkit), perm folder including the results of the permutation testing in perm*.mat files, res folder including the best (or fixed) hyperparameters in param*.mat file, the results of the main trained models in model*.mat file, additional results in res*.mat file and the summary of the results in results_table.txt file. For additional details on the output data files, see Analysis .","title":"Analysis"},{"location":"getting_started/#visualization","text":"The figure below illustrates the inputs and outputs of visualizing the results. The main inputs of the visualization are: res structure, which is a MATLAB variable loaded from res*.mat and appended with settings for visualization, .mat files either as outputs of the analysis or other files including data (e.g., mask.mat including a mask for connectivity data) or other settings for visualization (e.g., options.mat including BrainNet Viewer settings), .csv files, which are label files including information about the variables in your \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) matrices, .nv file, which is a surface mesh file in BrainNet Viewer used as a template to overlay your brain weights on, .nii files, which can be an atlas file defining regions of interest (ROI) in the brain or a mask file for voxel-wise structural MRI data. The main outputs of the visualization are: images of figures, in any requested standard file format, e.g., .fig , .png , .svg , .csv files including information about the plotted results (e.g., ordered model weights). For additional details on the res structure and the other input and output data files of visualization, see Visualization . Info We also highly recommend that you go through the defaults of cfg and res so that you understand thoroughly the analysis and visualization settings. You can find a detailed documentation of each high-level function of the toolkit under the main menu Functions, for instance, see cfg_defaults . Finally, if you want to get started and run an experiment on your local computer, see Demo for a complete analysis and visualization. In addition, we provide three simple examples to generate and analyse simulated data , simulated structural MRI data , and simulated fMRI data . Next, we briefly discuss how to run an analysis on a cluster.","title":"Visualization"},{"location":"getting_started/#running-on-a-cluster","text":"The toolkit can be run in multiple MATLAB instances, e.g., in a cluster environment. If you use SGE or SLURM scheduling systems, you simply need to send the same analysis to different nodes and the toolkit will take care of the rest. If you use a different scheduling system then a one-line modification of the cfg_defaults function is needed to account for your scheduling system and you are ready to go. Feel free to get in touch with us to help you set this up. Here is a brief description of what happens under the hood when the toolkit is running on different MATLAB instances. Although MATLAB can load the same .mat file from different MATLAB instances, it cannot save to the same .mat file . To work around this, the toolkit appends the unique ID of the computing node/job at the end of each .mat file (in a local environment, the ID is set to _1 by default), so even if different jobs are saving the same content simultaneously, they will do it into different files. In addition, there are a handful of wrapper functions to save, search and load these .mat files and the following mechanisms are in place to share computational cost: jobs save computations to .mat files regularly on disc, there is a random seed before each time consuming operation in the toolkit (e.g., grid search and permutation testing), hence different jobs will most likely perform different computations, jobs can load .mat files computed by other jobs, jobs regularly check what .mat files are available and they only start a new computation if that has not yet been performed by another job, if two MATLAB instances are saving the same computation to a .mat file then they will write to different files due to their different job ID-s. You might ask: doesn't this computational strategy create a lot of intermediate and some duplicate files? Indeed, that is the case, however, there is a cleanup_files function that allows to remove all the intermediate and duplicate files after your analysis is done. Do not forget to use this as otherwise you might exceed your disc space or have difficulties to move your results around, e.g., from a cluster to a local computer. An extra benefit of this computational strategy is that in case your analysis is aborted (e.g., you run out of allocated time on a cluster), you can always restart the same analysis and it will catch up with the computations where it was aborted.","title":"Running on a cluster"},{"location":"res/","text":"Here you can find a description of all possible settings of the fields and subfields of res . First parameter always indicates the default option. Analysis These fields are obtained during analysis and saved into res*.mat . Some of the fields are inherited from cfg . dir Essential paths to your project, framework (subfields inherited from cfg , see here ) and the output folders of the experiment such as grid search, permutations and main results. .project [ path ] full path to your project, such as 'PATH/TO/YOUR/PROJECT' .frwork [ path ] full path to your framework, such as 'PATH/TO/YOUR/PROJECT/framework/ANALYSIS_NAME' .grid [ path ] full path to your grid search results, such as 'PATH/TO/YOUR/PROJECT/framework/ANALYSIS_NAME/grid/level<id>' .perm [ path ] full path to your permutation testing results, such as 'PATH/TO/YOUR/PROJECT/framework/ANALYSIS_NAME/perm/level<id>' .res [ path ] full path to your main results, such as 'PATH/TO/YOUR/PROJECT/framework/ANALYSIS_NAME/grid/level<id>' frwork Results oriented details of framework with different subfields as in cfg . .level [ int ] level of the multivariate associative effect in the iterative calculation process .nlevel [ int ] maximum number of levels of the multivariate associative effect by default, the maximum number is limited by the rank of data \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) .split.all [ int or numeric array ] all splits at the current level .split.nall [ int ] number of all splits at the current level .split.best best split based on the criterion defined by cfg.defl.crit (for details, see here ) stat Results of statistical inference with some subfields inherited from cfg . For further details on the type of statistical inferences, see here in the Background page. .nperm [ int ] number of permutations .pval [ int or numeric array ] uncorrected p-values of the statistical inference within splits (one p-value per split) of note, in case of omnibus hypothesis the associative effect is significant if any of the p-values is smaller than the adjusted significance threshold .sig [ boolean ] defines whether the associative effect is significant across splits env Computation environment with all subfields inherited from cfg . For further details see here . .fileend [ char --> '_1' ] suffix at the end of each file saved in the framework folder whilst running the experiment on a cluster for file storage efficiency and easier data transfer, we suggest to use the cleanup_files function after an experiment is completed to delete all intermediate and duplicate files .save.compression [ boolean --> 1 ] defines if files are saved with or without compression of note, loading an uncompressed file can be faster for very large data files .verbose [ int --> 2 ] level of verbosity to display information in the command line during the experiment 1 : detailed information with including elapsed time between time sensitive operations 2 : detailed information without elapsed time 3 : minimal information .seed.split , [ char --> 'default' or int ] seed before random data splitting value is passed as input to MATLAB's rng function .seed.model , [ int or char ] seed before training models in random order during hyperparameter optimization the default value is taken from res.env.fileend (converted to int) value is passed as input to MATLAB's rng function .seed.perm , [ char --> 'default' or int ] seed before training models in random order during permutation testing value is passed as input to MATLAB's rng function Visualization These fields are for visualization. They are added to res only temporarily and they are not saved to res*.mat . gen General options for plotting. We recommend 'interactive' file selection when new to plotting results to fully understand what files are needed for the specific plots. Later, especially when wishing to automatize plots, it might be convenient to use 'none' file selection to avoid the interactive pop-up windows and the files can be provided by overwriting defaults if needed. .selectfile [ 'none', 'interactive' ] file selection using a wrapper function over the select_file function of SPM .weight.flip [ boolean --> false ] defines whether wa want to flip the weights, i.e., change their sign .weight.type [ 'weight', 'correlation' ] defines the type of model weight we want to interpret 'weight' refers to the true model weights read from model*.mat 'correlation' refers to loadings (PLS literature) or structure correlations (CCA literature), i.e., the correlation between the input variables and the latent variables/projections .figure.ext [ char --> '.png' ] defines the file extensions when saving a figure to disc .figure.Position [ numeric array ---> [] ] defines the position of the MATLAB figure, specified as a vector of the form [left bottom width height], for details, see MATLAB's Figure Properties by default, we use MATLAB's default settings .axes.Position [ numeric array ---> [] ] defines the position of the MATLAB axes, specified as a vector of the form [left bottom width height], for details, see MATLAB's Axes Properties by default, we use MATLAB's default settings .axes.XLim, .axes.YLim [ numeric array ---> [] ] defines the x- and y-axis limits, specified as a two-element vector of the form [ymin ymax], for details, see MATLAB's Axes Properties by default, we use MATLAB's automatic axis adjustment .axes.FontSize [ int ---> [] ] defines the fontsize for the axes in the figure, for details, see MATLAB's Axes Properties by default, we use MATLAB's default settings .axes.FontName [ char ---> [] ] defines the fontname for the axes in the figure, for details, see MATLAB's Axes Properties by default, we use MATLAB's default settings .axes.XTick, .axes.YTick [ int ---> [] ] defines the position of ticks on the x- and y-axis, for details, see MATLAB's Axes Properties by default, we use MATLAB's default settings .axes.XScale, .axes.YScale [ int ---> [] ] defines the scale on the x- and y-axis, for details, see MATLAB's Axes Properties by default, we use MATLAB's default settings .legend.FontSize [ int ---> [] ] defines the fontsize for the legend in the figure, for details, see MATLAB's Legend Properties by default, we use MATLAB's default settings .legend.Location [ char ---> [] ] defines the location of the legend in the figure, for details, see MATLAB's Legend Properties data Subfields inherited from cfg . For further details see here . .X.fname, .Y.fname, .C.fname [ filepath --> 'X.mat', 'Y.mat', 'C.mat' ] filename with full path to data \\(\\mathbf{X}\\) , \\(\\mathbf{Y}\\) , \\(\\mathbf{C}\\) param Options for plotting hyperparameter optimization results from grid search. For the plotting function, see plot_paropt . .view [ numeric array ---> [-130 20] ] viewing angle of 3D plot to help assessment of global maximum proj Options for plotting projections of data (i.e., latent variables). For the plotting function, see plot_proj . .xlabel, .ylabel [ char --> 'Brain latent variable', 'Behavioural latent variable' ] label for x- and y-axis .scatter.SizeData [ int --> [] ] marker size in scatter plot, for details see MATLAB's Scatter Properties .scatter.MarkerFaceColor [ numeric array --> [] ] face colour of the marker in scatter plot, specified as a vector of 3 RGB elements, for details see MATLAB's Scatter Properties note, that multiple RGB values can be provided if we want to colour-code multiple groups .scatter.MarkerEdgeColor [ char --> [] ] edge color of the marker in scatter plot, for details see MATLAB's Scatter Properties .lsline [ 'off', 'on' ] defines whether a least-squares line is overlayed on the scatter plot .file.label [ filepath --> 'LabelsY.csv' ] label file with full path for additional colormap/group information label file and data file (see below) should be in correspondence, i.e., row i in label file (without column heading) should correspond to column i in data file .file.data [ filepath --> 'Y.mat' ] data file with full path for additional colormap/group information label file and data file (see above) should be in correspondence, i.e., row i in label file (without column heading) should correspond to column i in data file .flip [ boolean --> false ] defines whether wa want to flip the sign of projections .multi_level [ boolean --> 0 ] defines whether we want a simple plot or a multi-level plot behav Options for plotting behavioural weights. For the plotting function, see plot_weight . .xlabel, .ylabel [ char --> 'Behavioural variables', 'Weight' ] label for x- and y-axis .weight.filtzero [ boolean --> 1 ] post-process weights by removing weights with zero values .weight.numtop [ int --> Inf ] post-process weights by selecting the top weights 'Inf' refers to including all weights .weight.sorttype [ 'sign', '', 'abs' ] post-process weights by sorting them in descending order 'sign' sorts both positive and negative weights in descending order (i.e., as if they were two independent lists) 'abs' sorts weights based on absolute value '' refers to no sorting .weight.norm [ 'none', 'minmax', 'std', 'zscore' ] post-process weights by normalizing them 'none' refers to no normalization 'minmax' refers to normalization by absolute maximum value 'std' refers to normalization by standard deviation 'zscore' refers to normalization by zscore .file.label [ filepath --> 'LabelsY.csv' ] label file with full path for data \\(\\mathbf{Y}\\) .label.maxchar [ int --> Inf ] maximum number of characters for label names in figure of note, use it when some of the labels are too long to display on the figure setting it to e.g., 50 conn Options for plotting connectivity data e.g., from resting-state fMRI. For the general plotting function, see plot_weight . .file.mask [ filepath --> 'mask.mat' ] mask file with full path for connectivity data mask variable in the mask.mat file is a 2D logical array specifying the connections that are included in the brain data .weight.filtzero [ boolean --> 1 ] post-process weights by removing weights with zero values .weight.numtop [ int --> Inf ] post-process weights by selecting the top weights 'Inf' refers to including all weights .weight.sorttype [ 'sign', '', 'abs' ] post-process weights by sorting them in descending order 'sign' sorts both positive and negative weights in descending order (i.e., as if they were two independent lists) 'abs' sorts weights based on absolute value '' refers to no sorting .weight.type [ 'auto', 'strength' ] post-process weights by multiplying them by the sign of the population mean in the original data 'auto' does no post-processing 'strength' does post-processing .weight.sign [ 'all', 'positive', 'negative' ] post-process weights by selecting a subset of them (e.g., with positive or negative sign) 'all' does no post-processing .module.disp [ boolean --> 0 ] defines whether to display module weights in command line .module.type [ 'average', 'sum' ] calculate the average or sum of weights within/between modules .module.norm [ 'none', 'global', 'max' ] normalize module weights 'none' does no normalization .file.label [ filepath --> 'LabelsX.csv' ] label file with full path for connectivity data vbm Options for plotting voxel-wise structural MRI data. For the general plotting function, see plot_weight . .weight.numtop [ int --> Inf ] post-process weights by selecting the top weights 'Inf' refers to including all weights .weight.sorttype [ 'sign', '', 'abs' ] post-process weights by sorting them in descending order 'sign' sorts both positive and negative weights in descending order (i.e., as if they were two independent lists) 'abs' sorts weights based on absolute value '' refers to no sorting .weight.norm [ 'none', 'minmax', 'std', 'zscore' ] post-process weights by normalizing them 'none' refers to no normalization 'minmax' refers to normalization by absolute maximum value 'std' refers to normalization by standard deviation 'zscore' refers to normalization by zscore .file.mask [ filepath --> 'mask.nii' ] mask file with full path for VBM data .nii file includes a 3D image with booleans for the voxels that are included in the brain .file.MNI [ filepath --> 'T1_1mm_brain.nii' ] source image with full path for normalization to MNI space .transM [ numeric array --> eye(4) ] rigid body transformation matrix to reorient the weight image to MNI space before normalization occurs roi Options for plotting region-wise structural MRI data. For the general plotting function, see plot_weight . .weight.filtzero [ boolean --> 1 ] post-process weights by removing weights with zero values .weight.numtop [ int --> Inf ] post-process weights by selecting the top weights 'Inf' refers to including all weights .weight.sorttype [ 'sign', '', 'abs' ] post-process weights by sorting them in descending order 'sign' sorts both positive and negative weights in descending order (i.e., as if they were two independent lists) 'abs' sorts weights based on absolute value '' refers to no sorting .out [ numeric array --> [] ] indexes of ROIs to remove from displaying on the figure .file.label [ filepath --> 'LabelsX.csv' ] label file with full path for ROI data simul Options for plotting modality independent (e.g., simulated) weights. For the general plotting function, see plot_weight . .xlabel, .ylabel [ char --> 'Variables', 'Weight' ] label for x- and y-axis .weight.filtzero [ boolean --> 1 ] post-process weights by removing weights with zero values .weight.numtop [ int --> Inf ] post-process weights by selecting the top weights 'Inf' refers to including all weights .weight.sorttype [ 'sign', '', 'abs' ] post-process weights by sorting them in descending order 'sign' sorts both positive and negative weights in descending order (i.e., as if they were two independent lists) 'abs' sorts weights based on absolute value '' refers to no sorting .weight.norm [ 'none', 'minmax', 'std', 'zscore' ] post-process weights by normalizing them 'none' refers to no normalization 'minmax' refers to normalization by absolute maximum value 'std' refers to normalization by standard deviation 'zscore' refers to normalization by zscore .weight.file.X,.weight.file.Y [ filepath ] full path to the true weight files, so that the true weights can be overlaid on the stem plot brainnet Settings for BrainNet Viewer for automatic plotting of brain weights on a glass brain and saving it as bitmap image in file. .file.surf [ filepath --> 'BrainMesh_ICBM152.nv' ] brain mesh (glass brain) file of note, full path is not necessary if file is in path or BrainNet Viewer is in external folder .file.options [ filepath --> 'options.mat' ] options file with full path for BrainNet configuration","title":"res"},{"location":"res/#analysis","text":"These fields are obtained during analysis and saved into res*.mat . Some of the fields are inherited from cfg .","title":"Analysis"},{"location":"res/#dir","text":"Essential paths to your project, framework (subfields inherited from cfg , see here ) and the output folders of the experiment such as grid search, permutations and main results. .project [ path ] full path to your project, such as 'PATH/TO/YOUR/PROJECT' .frwork [ path ] full path to your framework, such as 'PATH/TO/YOUR/PROJECT/framework/ANALYSIS_NAME' .grid [ path ] full path to your grid search results, such as 'PATH/TO/YOUR/PROJECT/framework/ANALYSIS_NAME/grid/level<id>' .perm [ path ] full path to your permutation testing results, such as 'PATH/TO/YOUR/PROJECT/framework/ANALYSIS_NAME/perm/level<id>' .res [ path ] full path to your main results, such as 'PATH/TO/YOUR/PROJECT/framework/ANALYSIS_NAME/grid/level<id>'","title":"dir"},{"location":"res/#frwork","text":"Results oriented details of framework with different subfields as in cfg . .level [ int ] level of the multivariate associative effect in the iterative calculation process .nlevel [ int ] maximum number of levels of the multivariate associative effect by default, the maximum number is limited by the rank of data \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) .split.all [ int or numeric array ] all splits at the current level .split.nall [ int ] number of all splits at the current level .split.best best split based on the criterion defined by cfg.defl.crit (for details, see here )","title":"frwork"},{"location":"res/#stat","text":"Results of statistical inference with some subfields inherited from cfg . For further details on the type of statistical inferences, see here in the Background page. .nperm [ int ] number of permutations .pval [ int or numeric array ] uncorrected p-values of the statistical inference within splits (one p-value per split) of note, in case of omnibus hypothesis the associative effect is significant if any of the p-values is smaller than the adjusted significance threshold .sig [ boolean ] defines whether the associative effect is significant across splits","title":"stat"},{"location":"res/#env","text":"Computation environment with all subfields inherited from cfg . For further details see here . .fileend [ char --> '_1' ] suffix at the end of each file saved in the framework folder whilst running the experiment on a cluster for file storage efficiency and easier data transfer, we suggest to use the cleanup_files function after an experiment is completed to delete all intermediate and duplicate files .save.compression [ boolean --> 1 ] defines if files are saved with or without compression of note, loading an uncompressed file can be faster for very large data files .verbose [ int --> 2 ] level of verbosity to display information in the command line during the experiment 1 : detailed information with including elapsed time between time sensitive operations 2 : detailed information without elapsed time 3 : minimal information .seed.split , [ char --> 'default' or int ] seed before random data splitting value is passed as input to MATLAB's rng function .seed.model , [ int or char ] seed before training models in random order during hyperparameter optimization the default value is taken from res.env.fileend (converted to int) value is passed as input to MATLAB's rng function .seed.perm , [ char --> 'default' or int ] seed before training models in random order during permutation testing value is passed as input to MATLAB's rng function","title":"env"},{"location":"res/#visualization","text":"These fields are for visualization. They are added to res only temporarily and they are not saved to res*.mat .","title":"Visualization"},{"location":"res/#gen","text":"General options for plotting. We recommend 'interactive' file selection when new to plotting results to fully understand what files are needed for the specific plots. Later, especially when wishing to automatize plots, it might be convenient to use 'none' file selection to avoid the interactive pop-up windows and the files can be provided by overwriting defaults if needed. .selectfile [ 'none', 'interactive' ] file selection using a wrapper function over the select_file function of SPM .weight.flip [ boolean --> false ] defines whether wa want to flip the weights, i.e., change their sign .weight.type [ 'weight', 'correlation' ] defines the type of model weight we want to interpret 'weight' refers to the true model weights read from model*.mat 'correlation' refers to loadings (PLS literature) or structure correlations (CCA literature), i.e., the correlation between the input variables and the latent variables/projections .figure.ext [ char --> '.png' ] defines the file extensions when saving a figure to disc .figure.Position [ numeric array ---> [] ] defines the position of the MATLAB figure, specified as a vector of the form [left bottom width height], for details, see MATLAB's Figure Properties by default, we use MATLAB's default settings .axes.Position [ numeric array ---> [] ] defines the position of the MATLAB axes, specified as a vector of the form [left bottom width height], for details, see MATLAB's Axes Properties by default, we use MATLAB's default settings .axes.XLim, .axes.YLim [ numeric array ---> [] ] defines the x- and y-axis limits, specified as a two-element vector of the form [ymin ymax], for details, see MATLAB's Axes Properties by default, we use MATLAB's automatic axis adjustment .axes.FontSize [ int ---> [] ] defines the fontsize for the axes in the figure, for details, see MATLAB's Axes Properties by default, we use MATLAB's default settings .axes.FontName [ char ---> [] ] defines the fontname for the axes in the figure, for details, see MATLAB's Axes Properties by default, we use MATLAB's default settings .axes.XTick, .axes.YTick [ int ---> [] ] defines the position of ticks on the x- and y-axis, for details, see MATLAB's Axes Properties by default, we use MATLAB's default settings .axes.XScale, .axes.YScale [ int ---> [] ] defines the scale on the x- and y-axis, for details, see MATLAB's Axes Properties by default, we use MATLAB's default settings .legend.FontSize [ int ---> [] ] defines the fontsize for the legend in the figure, for details, see MATLAB's Legend Properties by default, we use MATLAB's default settings .legend.Location [ char ---> [] ] defines the location of the legend in the figure, for details, see MATLAB's Legend Properties","title":"gen"},{"location":"res/#data","text":"Subfields inherited from cfg . For further details see here . .X.fname, .Y.fname, .C.fname [ filepath --> 'X.mat', 'Y.mat', 'C.mat' ] filename with full path to data \\(\\mathbf{X}\\) , \\(\\mathbf{Y}\\) , \\(\\mathbf{C}\\)","title":"data"},{"location":"res/#param","text":"Options for plotting hyperparameter optimization results from grid search. For the plotting function, see plot_paropt . .view [ numeric array ---> [-130 20] ] viewing angle of 3D plot to help assessment of global maximum","title":"param"},{"location":"res/#proj","text":"Options for plotting projections of data (i.e., latent variables). For the plotting function, see plot_proj . .xlabel, .ylabel [ char --> 'Brain latent variable', 'Behavioural latent variable' ] label for x- and y-axis .scatter.SizeData [ int --> [] ] marker size in scatter plot, for details see MATLAB's Scatter Properties .scatter.MarkerFaceColor [ numeric array --> [] ] face colour of the marker in scatter plot, specified as a vector of 3 RGB elements, for details see MATLAB's Scatter Properties note, that multiple RGB values can be provided if we want to colour-code multiple groups .scatter.MarkerEdgeColor [ char --> [] ] edge color of the marker in scatter plot, for details see MATLAB's Scatter Properties .lsline [ 'off', 'on' ] defines whether a least-squares line is overlayed on the scatter plot .file.label [ filepath --> 'LabelsY.csv' ] label file with full path for additional colormap/group information label file and data file (see below) should be in correspondence, i.e., row i in label file (without column heading) should correspond to column i in data file .file.data [ filepath --> 'Y.mat' ] data file with full path for additional colormap/group information label file and data file (see above) should be in correspondence, i.e., row i in label file (without column heading) should correspond to column i in data file .flip [ boolean --> false ] defines whether wa want to flip the sign of projections .multi_level [ boolean --> 0 ] defines whether we want a simple plot or a multi-level plot","title":"proj"},{"location":"res/#behav","text":"Options for plotting behavioural weights. For the plotting function, see plot_weight . .xlabel, .ylabel [ char --> 'Behavioural variables', 'Weight' ] label for x- and y-axis .weight.filtzero [ boolean --> 1 ] post-process weights by removing weights with zero values .weight.numtop [ int --> Inf ] post-process weights by selecting the top weights 'Inf' refers to including all weights .weight.sorttype [ 'sign', '', 'abs' ] post-process weights by sorting them in descending order 'sign' sorts both positive and negative weights in descending order (i.e., as if they were two independent lists) 'abs' sorts weights based on absolute value '' refers to no sorting .weight.norm [ 'none', 'minmax', 'std', 'zscore' ] post-process weights by normalizing them 'none' refers to no normalization 'minmax' refers to normalization by absolute maximum value 'std' refers to normalization by standard deviation 'zscore' refers to normalization by zscore .file.label [ filepath --> 'LabelsY.csv' ] label file with full path for data \\(\\mathbf{Y}\\) .label.maxchar [ int --> Inf ] maximum number of characters for label names in figure of note, use it when some of the labels are too long to display on the figure setting it to e.g., 50","title":"behav"},{"location":"res/#conn","text":"Options for plotting connectivity data e.g., from resting-state fMRI. For the general plotting function, see plot_weight . .file.mask [ filepath --> 'mask.mat' ] mask file with full path for connectivity data mask variable in the mask.mat file is a 2D logical array specifying the connections that are included in the brain data .weight.filtzero [ boolean --> 1 ] post-process weights by removing weights with zero values .weight.numtop [ int --> Inf ] post-process weights by selecting the top weights 'Inf' refers to including all weights .weight.sorttype [ 'sign', '', 'abs' ] post-process weights by sorting them in descending order 'sign' sorts both positive and negative weights in descending order (i.e., as if they were two independent lists) 'abs' sorts weights based on absolute value '' refers to no sorting .weight.type [ 'auto', 'strength' ] post-process weights by multiplying them by the sign of the population mean in the original data 'auto' does no post-processing 'strength' does post-processing .weight.sign [ 'all', 'positive', 'negative' ] post-process weights by selecting a subset of them (e.g., with positive or negative sign) 'all' does no post-processing .module.disp [ boolean --> 0 ] defines whether to display module weights in command line .module.type [ 'average', 'sum' ] calculate the average or sum of weights within/between modules .module.norm [ 'none', 'global', 'max' ] normalize module weights 'none' does no normalization .file.label [ filepath --> 'LabelsX.csv' ] label file with full path for connectivity data","title":"conn"},{"location":"res/#vbm","text":"Options for plotting voxel-wise structural MRI data. For the general plotting function, see plot_weight . .weight.numtop [ int --> Inf ] post-process weights by selecting the top weights 'Inf' refers to including all weights .weight.sorttype [ 'sign', '', 'abs' ] post-process weights by sorting them in descending order 'sign' sorts both positive and negative weights in descending order (i.e., as if they were two independent lists) 'abs' sorts weights based on absolute value '' refers to no sorting .weight.norm [ 'none', 'minmax', 'std', 'zscore' ] post-process weights by normalizing them 'none' refers to no normalization 'minmax' refers to normalization by absolute maximum value 'std' refers to normalization by standard deviation 'zscore' refers to normalization by zscore .file.mask [ filepath --> 'mask.nii' ] mask file with full path for VBM data .nii file includes a 3D image with booleans for the voxels that are included in the brain .file.MNI [ filepath --> 'T1_1mm_brain.nii' ] source image with full path for normalization to MNI space .transM [ numeric array --> eye(4) ] rigid body transformation matrix to reorient the weight image to MNI space before normalization occurs","title":"vbm"},{"location":"res/#roi","text":"Options for plotting region-wise structural MRI data. For the general plotting function, see plot_weight . .weight.filtzero [ boolean --> 1 ] post-process weights by removing weights with zero values .weight.numtop [ int --> Inf ] post-process weights by selecting the top weights 'Inf' refers to including all weights .weight.sorttype [ 'sign', '', 'abs' ] post-process weights by sorting them in descending order 'sign' sorts both positive and negative weights in descending order (i.e., as if they were two independent lists) 'abs' sorts weights based on absolute value '' refers to no sorting .out [ numeric array --> [] ] indexes of ROIs to remove from displaying on the figure .file.label [ filepath --> 'LabelsX.csv' ] label file with full path for ROI data","title":"roi"},{"location":"res/#simul","text":"Options for plotting modality independent (e.g., simulated) weights. For the general plotting function, see plot_weight . .xlabel, .ylabel [ char --> 'Variables', 'Weight' ] label for x- and y-axis .weight.filtzero [ boolean --> 1 ] post-process weights by removing weights with zero values .weight.numtop [ int --> Inf ] post-process weights by selecting the top weights 'Inf' refers to including all weights .weight.sorttype [ 'sign', '', 'abs' ] post-process weights by sorting them in descending order 'sign' sorts both positive and negative weights in descending order (i.e., as if they were two independent lists) 'abs' sorts weights based on absolute value '' refers to no sorting .weight.norm [ 'none', 'minmax', 'std', 'zscore' ] post-process weights by normalizing them 'none' refers to no normalization 'minmax' refers to normalization by absolute maximum value 'std' refers to normalization by standard deviation 'zscore' refers to normalization by zscore .weight.file.X,.weight.file.Y [ filepath ] full path to the true weight files, so that the true weights can be overlaid on the stem plot","title":"simul"},{"location":"res/#brainnet","text":"Settings for BrainNet Viewer for automatic plotting of brain weights on a glass brain and saving it as bitmap image in file. .file.surf [ filepath --> 'BrainMesh_ICBM152.nv' ] brain mesh (glass brain) file of note, full path is not necessary if file is in path or BrainNet Viewer is in external folder .file.options [ filepath --> 'options.mat' ] options file with full path for BrainNet configuration","title":"brainnet"},{"location":"visualization/","text":"Dependencies The visualization functions can have multiple dependencies. In general, if you want to use an interactive file selection, (i.e., pop-up dialog box to choose input files when needed), you need SPM . You also need SPM if you work with a mask or an atlas. For details on these, please see below. If you want to plot brain weights on a glass brain, you need BrainNet Viewer . We use BrainNet Viewer in the toolkit for plotting structural MRI weights either at the voxel or ROI level as well as fMRI connectivity weights either at the edge level or summarized by nodes. We suggest that you manually edit your preferred settings in the BrainNet Viewer GUI at first usage and save these options as otions.mat then the toolkit will be able to automatically set your configurations for the BrainNet figure in successsive plots. Overview There are three high-level functions to visualize your results: plot_proj : it plots the projections (or latent variables) of the data for an associative effect in a simple plot or for multiple associative effects in a multi-level plot . It can plot training and/or test sets of specific data splits. You can colour-code the subjects in the figure by groups or a continuous variable. plot_weight : it plots the model weights in specific formats. You can plot behavioural weights using text, vertical or horizontal bar plots. The brain weights can be plotted on a glass brain using BrainNet Viewer either as a map for voxel-level weights, as nodes for region-level weights and summarized connectivity weights or as edges for connectivity weights. You can also summarize connectivity weights as within and between-module connectivities. plot_paropt : it plots the grid search results of the hyperparameter optimization and it is a useful tool for diagnostics. It can visualize each metric in cfg.machine.metric as a function of hyperparameters irrespectively whether the requested metric was used as criterion for hyperparameter selection or not. Each metric is plotted in a separate subplot. These high-level functions will call low-level functions for a specific variation of a plot. A full list of high- and low-level plotting functions is illustrated in the following figure. The low-level functions are specified by the func argument of the high-level functions. For instance: to use the plot_proj_2d_group function, you need to call plot_proj with a 2d_group argument, to use the plot_weight_behav_horz function, you need to call plot_weight with a behav_horz argument. Matlab % Plot projections plot_proj ( ... , '2d_group') % Plot weights plot_weight ( ... , 'behav_horz') For a more detailed description of the high- and low-level functions, see the sections below. Configuration for plotting The main input of the visualization is the res structure including all the settings for your visualization. You might remember that res was initialized during your analysis and saved into res*.mat (for details, see Analysis ). First, we need to load the res structure either manually or using the res_defaults function as shown below. In the latter case, you need to initialize res with res.dir.frwork and res.frwork.level to inform res_defaults which res*.mat file to load. Matlab % Load res res . dir . frwork = 'PATH/TO/YOUR/SPECIFIC/FRAMEWORK' ; res . frwork . level = 1 ; res = res_defaults ( res , 'load' ); Once loaded, res can be populated with the following fields for the visualization of the results: gen : general options (e.g., interactive file selection) and settings (e.g., position, font size) for the figure, axes and legend, param : options for plotting the results of hyperparameter optimization, proj : options for plotting the projections of the data, behav : options for plotting the behavioural weights, conn : options for plotting the fMRI connectivity weights, vbm : options for plotting the voxel-level structural MRI weights, roi : options for plotting the region-level structural MRI weights, simul : options for plotting the weights of simulated data, brainnet : options for using BrainNet Viewer . Info Note that the fields for visualization are only temporarily added to res and not saved in the res*.mat file. To get a more detailed description of the fields and subfields of res , please see here . Projections One of the first results of your analysis that you will probably want to visualize is the projections of the data (i.e., latent variables) that are learnt by the CCA/PLS model. This plot can be created by the plot_proj function, which has the following required inputs: the res structure, the data modalities and the level of the associative effect, the type (outer and/or inner) and index of the data split, label for colour-coding the plot, and finally, the low-level function to use ('2d', '2d_group' or '2d_cmap'). The most simple projection plot can be created using the low-level function plot_proj_2d , which does not require any additional input files. Matlab % Plot data projections plot_proj ( res , { 'X' 'Y' }, res . frwork . level , 'osplit' , 1 , 'none' , '2d' ); The two other low-level functions allow you to colour-code your scatter plot by a continuous variable ( plot_proj_2d_cmap ) or groups ( plot_proj_2d_group ). There are different ways to define the colour-code: providing a group.mat file in your data folder, which should include a numeric array to define the colour map or groups, using the outer training and test sets to define the groups, providing a data and label file, where the data file should include the variable(s) and the label file should include the name of the variables to define the colour map or groups (by default, Y.mat and LabelsY.csv are used for these purposes but custom data and label files are also possible). In this example, we use plot_proj_2d_group to create a more advanced plot of the same data by also defining additional figure, axes and legend settings. Matlab % Plot data projections plot_proj ( res , { 'X' 'Y' }, res . frwork . level , 'osplit' , 1 , ... 'training+test' , '2d_group' , 'gen.axes.FontSize' , 20 , ... 'gen.legend.FontSize' , 20 , 'gen.legend.Location' , 'NorthWest' , ... 'proj.scatter.SizeData' , 120 , 'proj.scatter.MarkerEdgeColor' , 'k' , ... 'proj.scatter.MarkerFaceColor' , [ 0.3 0.3 0.9 ; 0.9 0.3 0.3 ], ... 'proj.xlabel' , 'Modality 1 latent variable' , ... 'proj.ylabel' , 'Modality 2 latent variable' , ... 'gen.axes.XTick' , [ - 5 : 2 : 5 ], 'proj.lsline' , 'on' ); Weights Another important part of the analysis results are the model weights of the learnt associative effect. The visualization of the weights are data modality dependent. For instance, the behavioral weights usually have only a few dimensions and can be best captured using a vertical or horizontal bar plot. The brain weights can be higher dimensional and their visualization might change depending on whether they belong to voxel-wise or region-of-interest structural MRI data, or fMRI connectivity data. Modality independent A data modality independent visualization can be achieved using a stem plot ( plot_weight_stem ). It does not require an input file by default. However, if you use simulated data and the true model weights are available as .mat files, you can overlay them on your stem plot. Matlab % Plot Y weights as stem plot plot_weight ( res , 'Y' , 'simul' , res . frwork . split . best , 'stem' , ... 'gen.axes.YLim' , [ - 0.2 1.2 ], 'simul.weight.norm' , 'minmax' , ... 'gen.axes.FontSize' , 20 , 'gen.legend.FontSize' , 20 ); Behaviour The behavioural weights can be best plotted using a bar graph, either vertically ( plot_weight_behav_vert ) or horizontally ( plot_weight_behav_horz ). Usually, only the top weights are plotted (e.g., the top 20 variables) and they are ordered by their actual weight values. Sometimes it might also be good to color in different behavioural domains such as cognition, emotion, clinical symptoms, psychometrics, or simply different questionnaires. Alternatively, the behavioural weights can be plotted as texts using plot_weight_behav_text . In this case, the weights themselves are actually not visualized (except of the minimum and maximum values), but the labels of the corresponding variables are displayed as text, separately for the positive and negative weights. This visualization makes sense if the labels are ordered based on the weights. To create these plots, a label file (usually a .csv file but can be any delimited file) is required with the following column headings: Label : describing the name or label of the variables, Category (optional): describing the different domains/categories the variables belong to. An example table is shown below. Label Category '1' 'Domain 1' '2' 'Domain 2' '3' 'Domain 1' Matlab % Plot behavioural weights as vertical bar plot plot_weight ( res , 'Y' , 'behav' , res . frwork . split . best , 'behav_vert' , ... 'gen.axes.FontSize' , 20 , 'gen.legend.FontSize' , 20 , ... 'gen.axes.YLim' , [ - 0.4 1.2 ], 'gen.weight.flip' , 1 , ... 'behav.weight.sorttype' , 'sign' , 'behav.weight.numtop' , 20 , ... 'behav.weight.norm' , 'minmax' ); Brain Plotting brain weights is probably the most complex as the dimensionality and type of data can be quite different across data modalities (e.g., voxel-wise, region-based, connectivity). These plots also need the most additional inputs. Voxel-wise sMRI Vowel-wise structural MRI weights are plotted on a cortical surface of a glass brain using BrainNet Viewer and the plot_weight_brain_cortex function. As the brain data is stored in concatenated form in the \\(\\mathbf{X}\\) matrix, this function requires a mask file to be able to reshape the brain weights into the same format as the original structural MRI data came from. The mask file should be a 3D image in .nii format including booleans that define which voxels are used as features in your \\(\\mathbf{X}\\) data (e.g., grey matter in the cortex). By default, the brain weights will be overlaid on an MNI template provided by BrainNet Viewer ( BrainMesh_ICBM152.nv ) but you can also use our own mesh file if you like. In case your brain data is not in MNI space, you can use the built-in normalization option in the toolkit, which uses the reorientation, spatial preprocessing and normalize modules of SPM . For this, you will need to make sure that your brain data are in relative proximity to MNI space and you need to provide a source image which will be used as a basis for normalization. ROI-wise sMRI ROI-wise structural MRI weights are plotted on the cortical surface as nodes on a glass brain using BrainNet Viewer and the plot_weight_brain_node function. This function requires a label file (usually a .csv file but can be any delimited file) with the following column headings: X , Y and Z coordinates of your regions (MNI coordinates by default), Label : describing the name or label of the regions, Index (optional): listing the indexes of the regions, Color (optional): specifying the colour of the region in the figure. An example table is shown below. X Y Z Index Label -39 -6 49 1 1 41 -9 50 2 2 -20 35 35 3 3 By default, the brain weights will be overlaid on an MNI template provided by BrainNet Viewer ( BrainMesh_ICBM152.nv ) but you can also use our own mesh file. Be careful that the coordinates in your label file match the space of your template file. Matlab % Plot ROI weights on a glass brain plot_weight ( res , 'X' , 'roi' , 1 , 'brain_node' , ... 'roi.weight.sorttype' , 'sign' , 'roi.weight.numtop' , 20 , ... 'roi.out' , 9000 + [ reshape ([ 1 : 10 : 81 ; 2 : 10 : 82 ], [], 1 ); ... reshape ( 100 : 10 : 170 , [], 1 )]); fMRI connectivity edges fMRI connectivity weights can be plotted on the cortical surface as edges on a glass brain using BrainNet Viewer and the plot_weight_brain_edge function. As the brain data are stored in a concatenated form in the \\(\\mathbf{X}\\) matrix, this function requires a mask file to be able to reshape the connections into a connectivity matrix. The mask file should be a 2D logical array in .mat format including a connectivity matrix (each row/column corresponding to a node) that defines which connections are used as features in your \\(\\mathbf{X}\\) data. In addition, a label file (ususally a .csv file but can be any delimited file) should be also provided with the following column headings: X , Y and Z coordinates of your regions (MNI coordinates by default), Label : defining the name or label of the regions, Index : listing the indexes of the regions, Color (optional): specifying the colour of the regions in the figure, Size (optional): specifying the size of the regions in the figure. For an example table, see the above section here . By default, the brain weights will be overlaid on an MNI template provided by BrainNet Viewer ( BrainMesh_ICBM152.nv ) but you can also use our own mesh file. Be careful that the coordinates in your label file match the space of your template file. Matlab % Plot connectivity weights on glass brain plot_weight ( res , 'X' , 'conn' , res . frwork . split . best , 'brain_edge' , ... 'conn.weight.sorttype' , 'sign' , 'conn.weight.numtop' , 20 ); fMRI connectivity nodes fMRI connectivity weights can be also plotted on the cortical surface summarized as nodes on a glass brain using BrainNet Viewer and the plot_weight_brain_conn_node . This function requires the same mask files as the plot_weight_brain_edge function (for details, see the section above here ). In addition, a label file (ususally a .csv file but can be any delimited file) should be also provided with the following column headings: X , Y and Z coordinates of your regions (MNI coordinates by default), Label : defining the name or label of the regions, Color (optional): specifying the colour of the regions in the figure. For an example table, see the section above. By default, the brain weights will be overlaid on an MNI template provided by BrainNet Viewer ( BrainMesh_ICBM152.nv ) but you can also use our own mesh file. Be careful that the coordinates in your label file match the space of your template file. Matlab % Plot connectivity weights summarized as nodes on glass brain plot_weight ( res , 'X' , 'conn' , res . frwork . split . best , 'brain_conn_node' , ... 'conn.weight.sorttype' , 'sign' , 'conn.weight.numtop' , 20 ); fMRI connectivity modules fMRI connectivity weights can be also summarized as modules and plotted as a connectivity matrix (i.e., within and between-module connections). This functionality is provided by the plot_weight_brain_conn_module function, which requires the same mask files as the plot_weight_brain_edge function (for details, see the section above here ). In addition, a label file (ususally a .csv file but can be any delimited file) should be also provided with the following column headings: Region : defining the module the regions belongs to. An example table is shown below. Region DMN DMN VIS Hyperparameters One major advantage of the CCA/PLS tooklit is the possibility to tune the hyperparameters of the different models separately for each associative effect. It is good practice to visualize the hyperparameter surface to better understand the optimization process and the resulting final models. In this example, three surface plots are created showing the out-of-sample correlation and the similarity of \\(\\mathbf{w}_x\\) and \\(\\mathbf{w}_y\\) as a function of the two hyperparameters of the SPLS model (L1x, L1y). Matlab % Plot hyperparameter surface for grid search results plot_paropt ( res , 1 , { 'correl' , 'simwx' , 'simwy' }, ... 'gen.figure.Position' , [ 500 600 1200 400 ], 'gen.axes.FontSize' , 20 , ... 'gen.axes.XScale' , 'log' , 'gen.axes.YScale' , 'log' );","title":"Visualization"},{"location":"visualization/#overview","text":"There are three high-level functions to visualize your results: plot_proj : it plots the projections (or latent variables) of the data for an associative effect in a simple plot or for multiple associative effects in a multi-level plot . It can plot training and/or test sets of specific data splits. You can colour-code the subjects in the figure by groups or a continuous variable. plot_weight : it plots the model weights in specific formats. You can plot behavioural weights using text, vertical or horizontal bar plots. The brain weights can be plotted on a glass brain using BrainNet Viewer either as a map for voxel-level weights, as nodes for region-level weights and summarized connectivity weights or as edges for connectivity weights. You can also summarize connectivity weights as within and between-module connectivities. plot_paropt : it plots the grid search results of the hyperparameter optimization and it is a useful tool for diagnostics. It can visualize each metric in cfg.machine.metric as a function of hyperparameters irrespectively whether the requested metric was used as criterion for hyperparameter selection or not. Each metric is plotted in a separate subplot. These high-level functions will call low-level functions for a specific variation of a plot. A full list of high- and low-level plotting functions is illustrated in the following figure. The low-level functions are specified by the func argument of the high-level functions. For instance: to use the plot_proj_2d_group function, you need to call plot_proj with a 2d_group argument, to use the plot_weight_behav_horz function, you need to call plot_weight with a behav_horz argument. Matlab % Plot projections plot_proj ( ... , '2d_group') % Plot weights plot_weight ( ... , 'behav_horz') For a more detailed description of the high- and low-level functions, see the sections below.","title":"Overview"},{"location":"visualization/#configuration-for-plotting","text":"The main input of the visualization is the res structure including all the settings for your visualization. You might remember that res was initialized during your analysis and saved into res*.mat (for details, see Analysis ). First, we need to load the res structure either manually or using the res_defaults function as shown below. In the latter case, you need to initialize res with res.dir.frwork and res.frwork.level to inform res_defaults which res*.mat file to load. Matlab % Load res res . dir . frwork = 'PATH/TO/YOUR/SPECIFIC/FRAMEWORK' ; res . frwork . level = 1 ; res = res_defaults ( res , 'load' ); Once loaded, res can be populated with the following fields for the visualization of the results: gen : general options (e.g., interactive file selection) and settings (e.g., position, font size) for the figure, axes and legend, param : options for plotting the results of hyperparameter optimization, proj : options for plotting the projections of the data, behav : options for plotting the behavioural weights, conn : options for plotting the fMRI connectivity weights, vbm : options for plotting the voxel-level structural MRI weights, roi : options for plotting the region-level structural MRI weights, simul : options for plotting the weights of simulated data, brainnet : options for using BrainNet Viewer . Info Note that the fields for visualization are only temporarily added to res and not saved in the res*.mat file. To get a more detailed description of the fields and subfields of res , please see here .","title":"Configuration for plotting"},{"location":"visualization/#projections","text":"One of the first results of your analysis that you will probably want to visualize is the projections of the data (i.e., latent variables) that are learnt by the CCA/PLS model. This plot can be created by the plot_proj function, which has the following required inputs: the res structure, the data modalities and the level of the associative effect, the type (outer and/or inner) and index of the data split, label for colour-coding the plot, and finally, the low-level function to use ('2d', '2d_group' or '2d_cmap'). The most simple projection plot can be created using the low-level function plot_proj_2d , which does not require any additional input files. Matlab % Plot data projections plot_proj ( res , { 'X' 'Y' }, res . frwork . level , 'osplit' , 1 , 'none' , '2d' ); The two other low-level functions allow you to colour-code your scatter plot by a continuous variable ( plot_proj_2d_cmap ) or groups ( plot_proj_2d_group ). There are different ways to define the colour-code: providing a group.mat file in your data folder, which should include a numeric array to define the colour map or groups, using the outer training and test sets to define the groups, providing a data and label file, where the data file should include the variable(s) and the label file should include the name of the variables to define the colour map or groups (by default, Y.mat and LabelsY.csv are used for these purposes but custom data and label files are also possible). In this example, we use plot_proj_2d_group to create a more advanced plot of the same data by also defining additional figure, axes and legend settings. Matlab % Plot data projections plot_proj ( res , { 'X' 'Y' }, res . frwork . level , 'osplit' , 1 , ... 'training+test' , '2d_group' , 'gen.axes.FontSize' , 20 , ... 'gen.legend.FontSize' , 20 , 'gen.legend.Location' , 'NorthWest' , ... 'proj.scatter.SizeData' , 120 , 'proj.scatter.MarkerEdgeColor' , 'k' , ... 'proj.scatter.MarkerFaceColor' , [ 0.3 0.3 0.9 ; 0.9 0.3 0.3 ], ... 'proj.xlabel' , 'Modality 1 latent variable' , ... 'proj.ylabel' , 'Modality 2 latent variable' , ... 'gen.axes.XTick' , [ - 5 : 2 : 5 ], 'proj.lsline' , 'on' );","title":"Projections"},{"location":"visualization/#weights","text":"Another important part of the analysis results are the model weights of the learnt associative effect. The visualization of the weights are data modality dependent. For instance, the behavioral weights usually have only a few dimensions and can be best captured using a vertical or horizontal bar plot. The brain weights can be higher dimensional and their visualization might change depending on whether they belong to voxel-wise or region-of-interest structural MRI data, or fMRI connectivity data.","title":"Weights"},{"location":"visualization/#modality-independent","text":"A data modality independent visualization can be achieved using a stem plot ( plot_weight_stem ). It does not require an input file by default. However, if you use simulated data and the true model weights are available as .mat files, you can overlay them on your stem plot. Matlab % Plot Y weights as stem plot plot_weight ( res , 'Y' , 'simul' , res . frwork . split . best , 'stem' , ... 'gen.axes.YLim' , [ - 0.2 1.2 ], 'simul.weight.norm' , 'minmax' , ... 'gen.axes.FontSize' , 20 , 'gen.legend.FontSize' , 20 );","title":"Modality independent"},{"location":"visualization/#behaviour","text":"The behavioural weights can be best plotted using a bar graph, either vertically ( plot_weight_behav_vert ) or horizontally ( plot_weight_behav_horz ). Usually, only the top weights are plotted (e.g., the top 20 variables) and they are ordered by their actual weight values. Sometimes it might also be good to color in different behavioural domains such as cognition, emotion, clinical symptoms, psychometrics, or simply different questionnaires. Alternatively, the behavioural weights can be plotted as texts using plot_weight_behav_text . In this case, the weights themselves are actually not visualized (except of the minimum and maximum values), but the labels of the corresponding variables are displayed as text, separately for the positive and negative weights. This visualization makes sense if the labels are ordered based on the weights. To create these plots, a label file (usually a .csv file but can be any delimited file) is required with the following column headings: Label : describing the name or label of the variables, Category (optional): describing the different domains/categories the variables belong to. An example table is shown below. Label Category '1' 'Domain 1' '2' 'Domain 2' '3' 'Domain 1' Matlab % Plot behavioural weights as vertical bar plot plot_weight ( res , 'Y' , 'behav' , res . frwork . split . best , 'behav_vert' , ... 'gen.axes.FontSize' , 20 , 'gen.legend.FontSize' , 20 , ... 'gen.axes.YLim' , [ - 0.4 1.2 ], 'gen.weight.flip' , 1 , ... 'behav.weight.sorttype' , 'sign' , 'behav.weight.numtop' , 20 , ... 'behav.weight.norm' , 'minmax' );","title":"Behaviour"},{"location":"visualization/#brain","text":"Plotting brain weights is probably the most complex as the dimensionality and type of data can be quite different across data modalities (e.g., voxel-wise, region-based, connectivity). These plots also need the most additional inputs.","title":"Brain"},{"location":"visualization/#voxel-wise-smri","text":"Vowel-wise structural MRI weights are plotted on a cortical surface of a glass brain using BrainNet Viewer and the plot_weight_brain_cortex function. As the brain data is stored in concatenated form in the \\(\\mathbf{X}\\) matrix, this function requires a mask file to be able to reshape the brain weights into the same format as the original structural MRI data came from. The mask file should be a 3D image in .nii format including booleans that define which voxels are used as features in your \\(\\mathbf{X}\\) data (e.g., grey matter in the cortex). By default, the brain weights will be overlaid on an MNI template provided by BrainNet Viewer ( BrainMesh_ICBM152.nv ) but you can also use our own mesh file if you like. In case your brain data is not in MNI space, you can use the built-in normalization option in the toolkit, which uses the reorientation, spatial preprocessing and normalize modules of SPM . For this, you will need to make sure that your brain data are in relative proximity to MNI space and you need to provide a source image which will be used as a basis for normalization.","title":"Voxel-wise sMRI"},{"location":"visualization/#roi-wise-smri","text":"ROI-wise structural MRI weights are plotted on the cortical surface as nodes on a glass brain using BrainNet Viewer and the plot_weight_brain_node function. This function requires a label file (usually a .csv file but can be any delimited file) with the following column headings: X , Y and Z coordinates of your regions (MNI coordinates by default), Label : describing the name or label of the regions, Index (optional): listing the indexes of the regions, Color (optional): specifying the colour of the region in the figure. An example table is shown below. X Y Z Index Label -39 -6 49 1 1 41 -9 50 2 2 -20 35 35 3 3 By default, the brain weights will be overlaid on an MNI template provided by BrainNet Viewer ( BrainMesh_ICBM152.nv ) but you can also use our own mesh file. Be careful that the coordinates in your label file match the space of your template file. Matlab % Plot ROI weights on a glass brain plot_weight ( res , 'X' , 'roi' , 1 , 'brain_node' , ... 'roi.weight.sorttype' , 'sign' , 'roi.weight.numtop' , 20 , ... 'roi.out' , 9000 + [ reshape ([ 1 : 10 : 81 ; 2 : 10 : 82 ], [], 1 ); ... reshape ( 100 : 10 : 170 , [], 1 )]);","title":"ROI-wise sMRI"},{"location":"visualization/#fmri-connectivity-edges","text":"fMRI connectivity weights can be plotted on the cortical surface as edges on a glass brain using BrainNet Viewer and the plot_weight_brain_edge function. As the brain data are stored in a concatenated form in the \\(\\mathbf{X}\\) matrix, this function requires a mask file to be able to reshape the connections into a connectivity matrix. The mask file should be a 2D logical array in .mat format including a connectivity matrix (each row/column corresponding to a node) that defines which connections are used as features in your \\(\\mathbf{X}\\) data. In addition, a label file (ususally a .csv file but can be any delimited file) should be also provided with the following column headings: X , Y and Z coordinates of your regions (MNI coordinates by default), Label : defining the name or label of the regions, Index : listing the indexes of the regions, Color (optional): specifying the colour of the regions in the figure, Size (optional): specifying the size of the regions in the figure. For an example table, see the above section here . By default, the brain weights will be overlaid on an MNI template provided by BrainNet Viewer ( BrainMesh_ICBM152.nv ) but you can also use our own mesh file. Be careful that the coordinates in your label file match the space of your template file. Matlab % Plot connectivity weights on glass brain plot_weight ( res , 'X' , 'conn' , res . frwork . split . best , 'brain_edge' , ... 'conn.weight.sorttype' , 'sign' , 'conn.weight.numtop' , 20 );","title":"fMRI connectivity edges"},{"location":"visualization/#fmri-connectivity-nodes","text":"fMRI connectivity weights can be also plotted on the cortical surface summarized as nodes on a glass brain using BrainNet Viewer and the plot_weight_brain_conn_node . This function requires the same mask files as the plot_weight_brain_edge function (for details, see the section above here ). In addition, a label file (ususally a .csv file but can be any delimited file) should be also provided with the following column headings: X , Y and Z coordinates of your regions (MNI coordinates by default), Label : defining the name or label of the regions, Color (optional): specifying the colour of the regions in the figure. For an example table, see the section above. By default, the brain weights will be overlaid on an MNI template provided by BrainNet Viewer ( BrainMesh_ICBM152.nv ) but you can also use our own mesh file. Be careful that the coordinates in your label file match the space of your template file. Matlab % Plot connectivity weights summarized as nodes on glass brain plot_weight ( res , 'X' , 'conn' , res . frwork . split . best , 'brain_conn_node' , ... 'conn.weight.sorttype' , 'sign' , 'conn.weight.numtop' , 20 );","title":"fMRI connectivity nodes"},{"location":"visualization/#fmri-connectivity-modules","text":"fMRI connectivity weights can be also summarized as modules and plotted as a connectivity matrix (i.e., within and between-module connections). This functionality is provided by the plot_weight_brain_conn_module function, which requires the same mask files as the plot_weight_brain_edge function (for details, see the section above here ). In addition, a label file (ususally a .csv file but can be any delimited file) should be also provided with the following column headings: Region : defining the module the regions belongs to. An example table is shown below. Region DMN DMN VIS","title":"fMRI connectivity modules"},{"location":"visualization/#hyperparameters","text":"One major advantage of the CCA/PLS tooklit is the possibility to tune the hyperparameters of the different models separately for each associative effect. It is good practice to visualize the hyperparameter surface to better understand the optimization process and the resulting final models. In this example, three surface plots are created showing the out-of-sample correlation and the similarity of \\(\\mathbf{w}_x\\) and \\(\\mathbf{w}_y\\) as a function of the two hyperparameters of the SPLS model (L1x, L1y). Matlab % Plot hyperparameter surface for grid search results plot_paropt ( res , 1 , { 'correl' , 'simwx' , 'simwy' }, ... 'gen.figure.Position' , [ 500 600 1200 400 ], 'gen.axes.FontSize' , 20 , ... 'gen.axes.XScale' , 'log' , 'gen.axes.YScale' , 'log' );","title":"Hyperparameters"},{"location":"mfiles/cfg_defaults/","text":"cfg_defaults Set defaults in your configuration ( cfg ) structure which will define the settings of your analysis (e.g., machine, framework, statistical inference). Use this function to update and add all necessary defaults to your cfg . If you defined anything in your cfg before calling the function, it won't overwrite those values. The path to the project folder should be always defined in your cfg or passed as varargin, otherwise the function throws an error. All the other fields are optional and can be filled up by cfg_defaults . No results will be stored in the cfg structure. See res_defaults for more information on results. Warning We strongly advise to inspect the output of cfg_defaults to make sure that the defaults are set as expected. Syntax cfg = cfg_defaults ( cfg , varargin ) Inputs cfg [ struct ] varargin [ name-value pairs ] additional parameters can be set via name-value pairs with dot notation supported (e.g., 'frwork.split.nout', 5) Outputs cfg [ struct ] configuration structure that has been updated with defaults Examples % Example 1 [ X , Y , wX , wY ] = generate_data ( 1000 , 100 , 100 , 10 , 10 , 1 ); See also: cfg , res_defaults","title":"cfg_defaults"},{"location":"mfiles/cfg_defaults/#syntax","text":"cfg = cfg_defaults ( cfg , varargin )","title":"Syntax"},{"location":"mfiles/cfg_defaults/#inputs","text":"cfg [ struct ] varargin [ name-value pairs ] additional parameters can be set via name-value pairs with dot notation supported (e.g., 'frwork.split.nout', 5)","title":"Inputs"},{"location":"mfiles/cfg_defaults/#outputs","text":"cfg [ struct ] configuration structure that has been updated with defaults","title":"Outputs"},{"location":"mfiles/cfg_defaults/#examples","text":"% Example 1 [ X , Y , wX , wY ] = generate_data ( 1000 , 100 , 100 , 10 , 10 , 1 ); See also: cfg , res_defaults","title":"Examples"},{"location":"mfiles/cleanup_files/","text":"cleanup_files Cleans up unnecessary duplicate and intermediate files created during analysis to save disc space. Syntax cleanup_files(cfg) Inputs cfg [ struct ] Examples % Example 1 load cfg ; cleanup_files ( cfg ); See also: cfg","title":"cleanup_files"},{"location":"mfiles/cleanup_files/#syntax","text":"cleanup_files(cfg)","title":"Syntax"},{"location":"mfiles/cleanup_files/#inputs","text":"cfg [ struct ]","title":"Inputs"},{"location":"mfiles/cleanup_files/#examples","text":"% Example 1 load cfg ; cleanup_files ( cfg ); See also: cfg","title":"Examples"},{"location":"mfiles/example_fmri/","text":"example_fmri This is a demo for simulated fMRI connectivity data. We will discuss step by step how to generate data, run an analysis as well as how to visualize the results. Copy and paste the code chunks into a function to create your own experiment or copy the function from the examples folder of the toolkit. Generate data First, we generate the simulated data using the generate_data function of the toolkit. We will use 1000 examples and 100 features in both data modalities. We set the signal to be sparse with 10% of the features in each modality that are relevant to capture the association across modalities. The noise parameter of the model is set to 1. For further details on the generative model, see Mihalik et al. 2022 . Matlab %----- Generate data % Data folder data_dir = fullfile ( fileparts ( mfilename ( 'fullpath' )), 'example_fmri' , 'data' ); if ~ exist ( fullfile ( data_dir , 'X.mat' ), 'file' ) || ... ~ exist ( fullfile ( data_dir , 'Y.mat' ), 'file' ) % Generate simulated fMRI connectivity data [ X , Y , wX , wY ] = generate_data ( 1000 , 100 , 100 , 10 , 10 , 1 ); % Save simulated data and true model weights if ~ isfolder ( 'data_dir' ) mkdir ( data_dir ); end save ( fullfile ( data_dir , 'X.mat' ), 'X' ); save ( fullfile ( data_dir , 'Y.mat' ), 'Y' ); save ( fullfile ( data_dir , 'wX.mat' ), 'wX' ); save ( fullfile ( data_dir , 'wY.mat' ), 'wY' ); end Create mask and label files As we use simulated fMRI connectivity and behavioural data, we need to create mask and label files. Although we recommend to use a multi-modal or functional atlas for fMRI connectivity data, for simplicity, we will use the AAL atlas to create a subset of 100 connections of the full connectivity matrix between all AAL regions. For our behavioural label file, we will simply use indexes for Label and 2 domains as Category . Matlab % Add the AAL2 (https://www.gin.cnrs.fr/en/tools/aal/) and the BrainNet % Viewer (https://www.nitrc.org/projects/bnv/) toolboxes to the path set_path ( 'aal' , 'brainnet' ); % Create AAL labels for full simulated fMRI connectivity data if ~ exist ( fullfile ( data_dir , 'LabelsX.csv' ), 'file' ) BrainNet_GenCoord ( which ( 'AAL2.nii' ), 'AAL.txt' ); T = readtable ( 'AAL.txt' ); nROI = size ( T , 1 ); T . Properties . VariableNames ([ 1 : 3 6 ]) = { 'X' 'Y' 'Z' 'Index' }; % we will need only these variables T . Label = sprintfc ( 'Region-%d' , [ 1 : nROI ] ' ); % we need characters for this label writetable ( T (:,[ 1 : 3 6 : 7 ]), fullfile ( data_dir , 'LabelsX.csv' )); delete AAL.txt ; % clean up end % Create mask for subset of 100 connections used as features in input data if ~ exist ( fullfile ( data_dir , 'mask.mat' ), 'file' ) mask = false ( nROI ); full_mask_id = find ( tril ( true ( nROI ), - 1 )); rand_id = randperm ( numel ( full_mask_id )); mask ( rand_id ( 1 : 100 )) = 1 ; save ( fullfile ( data_dir , 'mask.mat' ), 'mask' ); end % Create labels for behavioural data if ~ exist ( fullfile ( data_dir , 'LabelsY.csv' ), 'file' ) T = table ([ 1 : 100 ] ' , [ repmat ({ 'Domain 1' }, 50 , 1 ); repmat ({ 'Domain 2' }, 50 , 1 )], ... 'VariableNames' , { 'Label' 'Category' }); writetable ( T , fullfile ( data_dir , 'LabelsY.csv' )); end Analysis Now we are ready to set up the analysis. We start by running set_path to add the necessary paths of the toolkit to your MATLAB path. Matlab %----- Analysis % Set path for analysis set_path ; Project folder Next, we specify the folder to our project. Make sure to specify the correct path. We recommend to use a full path, but a relative path should also work. Matlab % Project folder cfg . dir . project = fullfile ( fileparts ( data_dir )); Machine Now, we configure the CCA/PLS model we would like to use. We set machine.name to cca and machine.param.name to {'PCAx' 'PCAy'} for PCA-CCA . For quicker results, we fix the number of principal components. However, in general we recommend to determine the optimal number of components based on a grid search similar to demo_smri . Matlab % Machine settings cfg . machine . name = 'cca' ; cfg . machine . param . name = { 'PCAx' 'PCAy' }; cfg . machine . param . PCAx = 95 ; cfg . machine . param . PCAy = 95 ; For more information on the CCA/PLS models and the hyperparameter choices, see here . For further details on the choices of data settings, see here . Framework Next, we set the framework name to holdout and the number of outer data splits to 5 to perform a multiple holdout approach. Matlab % Framework settings cfg . frwork . name = 'holdout' ; cfg . frwork . split . nout = 5 ; For further details on the framework choices, see here . Environment Next, we set the computational environment for the toolkit. As our PCA-CCA implementation is computationally efficient, most of the times we can run it locally on our computer. Matlab % Environment settings cfg . env . comp = 'local' ; For further details on the environmental settings, see here . Statistical inference Finally, we need to define how the statistical inference is performed. As we use a multiple holdout approach, we have additional options here too. We will perform statistical inference in two steps. First, for each outer split we do permutation testing based on out-of-sample correlation. Second, to infer if the associative effect is significant across splits, omnibus hypothesis is used, which tests if any outer split is significant after adjusting the threshold with Bonferroni correction (e.g., p=0.01 in case of 5 splits). This approach is based on Monteiro et al. 2016 . For quicker results, we set the number of permutations to 100, however, we recommend using at least 1000 permutations in general. Matlab % Statistical inference settings cfg . stat . nperm = 100 ; For further details on the statistical inference, see here . Run Analysis To run the analysis, we simply update our cfg structure to add all necessary default values that we did not explicitly define and then run the main function. After the analysis, we clean up all the duplicate and intermediate files to save disc space. Note that if you run the analysis in a cluster environment then you will need to comment out the last line and run it manually once the analysis is completed as the cleanup_files function does not work in a parallel environment. Matlab % Update cfg with defaults cfg = cfg_defaults ( cfg ); % Run analysis main ( cfg ); % Clean up analysis files to save disc space cleanup_files ( cfg ); Visualization Now that we have run our first analysis, let's plot some of the results. Before we can do any plotting, we need to make sure that we have called set_path('plot') to add the plotting folder. Then we load the res structure. In general, we advise you to plot your results on a local computer as it is often cumbersome and slow in a cluster environment. If you move your results from a cluster to a local computer, you need update the paths in your cfg*.mat and res*.mat files using update_dir . This should be called once the res structure is loaded either manually or by res_defaults . Matlab %----- Visualization % Set path for plotting and the BrainNet Viewer toolbox set_path ( 'plot' , 'brainnet' ); % Load res res . dir . frwork = cfg . dir . frwork ; res . frwork . level = 1 ; res . gen . selectfile = 'none' ; res . gen . weight . flip = 1 ; res = res_defaults ( res , 'load' ); Plot projections To plot the data projections (or latent variables) that has been learnt by the model, simply run plot_proj . As first argument, we need to pass the res structure. Then, we specify the data modalities as cell array and the level of associative effect. In this example, we plot the projections of X and Y for the first associative effect. We set the fourth input parameter to 'osplit' so that the training and test data of the outer split will be used for the plot. The following argument defines the outer data split we want to use (in this demo, we have only one split). We use the second to last argument to specify the colour-coding of the data using the training and test data as groups ( teid ). Finally, we specify the low-level function that will plot the results. In this case it is plot_proj_2d_group . Please see the documentation of plot_proj for more details. Matlab % Plot data projections plot_proj ( res , { 'X' 'Y' }, res . frwork . level , 'osplit' , ... res . frwork . split . best , 'training+test' , '2d_group' , 'gen.axes.FontSize' , 20 , ... 'gen.legend.FontSize' , 20 , 'gen.legend.Location' , 'NorthWest' , ... 'proj.scatter.SizeData' , 120 , 'proj.scatter.MarkerEdgeColor' , 'k' , ... 'proj.scatter.MarkerFaceColor' , [ 0.3 0.3 0.9 ; 0.9 0.3 0.3 ]); Plot weights Plotting model weights heavily depends on the kind of data that has been used in the analysis. In case of our fake functional MRI connectivity data, we will plot the weights as edges on a glass brain. We will use only the top 20 most positive and top 20 most negative weights for the figure. We set this by first sorting the weights by their sign ( roi.weight.sorttype = sign ) then taking the top 20 from both ends ( roi.weight.numtop = 20 ). In case of our fake behavioural data, we will plot the weights as a vertical bar plot, again using only the top 20 most positive and top 20 most negative weights. As first argument, we need to pass the res function, in which we define our custom processing for the weights. Next, we specify the data modality and the type of the modality as strings. In this example, we use brain connectivity and behavioural data, so we set these to X and conn for one and Y and behav for the other. The following argument defines the outer data split we want to use. Finally, we specify the low-level function that will plot the results. In this example, it will be plot_weight_brain_edge and plot_weight_behav_vert . Please see the documentation of plot_weight for more details. Matlab % Plot connectivity weights on glass brain plot_weight ( res , 'X' , 'conn' , res . frwork . split . best , 'brain_edge' , ... 'conn.weight.sorttype' , 'sign' , 'conn.weight.numtop' , 20 ); Matlab % Plot behavioural weights as vertical bar plot plot_weight ( res , 'Y' , 'behav' , res . frwork . split . best , 'behav_vert' , ... 'gen.axes.FontSize' , 20 , 'gen.legend.FontSize' , 20 , ... 'gen.axes.YLim' , [ - 0.004 0.013 ], ... 'behav.weight.sorttype' , 'sign' , 'behav.weight.numtop' , 20 );","title":"Functional MRI data"},{"location":"mfiles/example_fmri/#generate-data","text":"First, we generate the simulated data using the generate_data function of the toolkit. We will use 1000 examples and 100 features in both data modalities. We set the signal to be sparse with 10% of the features in each modality that are relevant to capture the association across modalities. The noise parameter of the model is set to 1. For further details on the generative model, see Mihalik et al. 2022 . Matlab %----- Generate data % Data folder data_dir = fullfile ( fileparts ( mfilename ( 'fullpath' )), 'example_fmri' , 'data' ); if ~ exist ( fullfile ( data_dir , 'X.mat' ), 'file' ) || ... ~ exist ( fullfile ( data_dir , 'Y.mat' ), 'file' ) % Generate simulated fMRI connectivity data [ X , Y , wX , wY ] = generate_data ( 1000 , 100 , 100 , 10 , 10 , 1 ); % Save simulated data and true model weights if ~ isfolder ( 'data_dir' ) mkdir ( data_dir ); end save ( fullfile ( data_dir , 'X.mat' ), 'X' ); save ( fullfile ( data_dir , 'Y.mat' ), 'Y' ); save ( fullfile ( data_dir , 'wX.mat' ), 'wX' ); save ( fullfile ( data_dir , 'wY.mat' ), 'wY' ); end","title":"Generate data"},{"location":"mfiles/example_fmri/#create-mask-and-label-files","text":"As we use simulated fMRI connectivity and behavioural data, we need to create mask and label files. Although we recommend to use a multi-modal or functional atlas for fMRI connectivity data, for simplicity, we will use the AAL atlas to create a subset of 100 connections of the full connectivity matrix between all AAL regions. For our behavioural label file, we will simply use indexes for Label and 2 domains as Category . Matlab % Add the AAL2 (https://www.gin.cnrs.fr/en/tools/aal/) and the BrainNet % Viewer (https://www.nitrc.org/projects/bnv/) toolboxes to the path set_path ( 'aal' , 'brainnet' ); % Create AAL labels for full simulated fMRI connectivity data if ~ exist ( fullfile ( data_dir , 'LabelsX.csv' ), 'file' ) BrainNet_GenCoord ( which ( 'AAL2.nii' ), 'AAL.txt' ); T = readtable ( 'AAL.txt' ); nROI = size ( T , 1 ); T . Properties . VariableNames ([ 1 : 3 6 ]) = { 'X' 'Y' 'Z' 'Index' }; % we will need only these variables T . Label = sprintfc ( 'Region-%d' , [ 1 : nROI ] ' ); % we need characters for this label writetable ( T (:,[ 1 : 3 6 : 7 ]), fullfile ( data_dir , 'LabelsX.csv' )); delete AAL.txt ; % clean up end % Create mask for subset of 100 connections used as features in input data if ~ exist ( fullfile ( data_dir , 'mask.mat' ), 'file' ) mask = false ( nROI ); full_mask_id = find ( tril ( true ( nROI ), - 1 )); rand_id = randperm ( numel ( full_mask_id )); mask ( rand_id ( 1 : 100 )) = 1 ; save ( fullfile ( data_dir , 'mask.mat' ), 'mask' ); end % Create labels for behavioural data if ~ exist ( fullfile ( data_dir , 'LabelsY.csv' ), 'file' ) T = table ([ 1 : 100 ] ' , [ repmat ({ 'Domain 1' }, 50 , 1 ); repmat ({ 'Domain 2' }, 50 , 1 )], ... 'VariableNames' , { 'Label' 'Category' }); writetable ( T , fullfile ( data_dir , 'LabelsY.csv' )); end","title":"Create mask and label files"},{"location":"mfiles/example_fmri/#analysis","text":"Now we are ready to set up the analysis. We start by running set_path to add the necessary paths of the toolkit to your MATLAB path. Matlab %----- Analysis % Set path for analysis set_path ;","title":"Analysis"},{"location":"mfiles/example_fmri/#project-folder","text":"Next, we specify the folder to our project. Make sure to specify the correct path. We recommend to use a full path, but a relative path should also work. Matlab % Project folder cfg . dir . project = fullfile ( fileparts ( data_dir ));","title":"Project folder"},{"location":"mfiles/example_fmri/#machine","text":"Now, we configure the CCA/PLS model we would like to use. We set machine.name to cca and machine.param.name to {'PCAx' 'PCAy'} for PCA-CCA . For quicker results, we fix the number of principal components. However, in general we recommend to determine the optimal number of components based on a grid search similar to demo_smri . Matlab % Machine settings cfg . machine . name = 'cca' ; cfg . machine . param . name = { 'PCAx' 'PCAy' }; cfg . machine . param . PCAx = 95 ; cfg . machine . param . PCAy = 95 ; For more information on the CCA/PLS models and the hyperparameter choices, see here . For further details on the choices of data settings, see here .","title":"Machine"},{"location":"mfiles/example_fmri/#framework","text":"Next, we set the framework name to holdout and the number of outer data splits to 5 to perform a multiple holdout approach. Matlab % Framework settings cfg . frwork . name = 'holdout' ; cfg . frwork . split . nout = 5 ; For further details on the framework choices, see here .","title":"Framework"},{"location":"mfiles/example_fmri/#environment","text":"Next, we set the computational environment for the toolkit. As our PCA-CCA implementation is computationally efficient, most of the times we can run it locally on our computer. Matlab % Environment settings cfg . env . comp = 'local' ; For further details on the environmental settings, see here .","title":"Environment"},{"location":"mfiles/example_fmri/#statistical-inference","text":"Finally, we need to define how the statistical inference is performed. As we use a multiple holdout approach, we have additional options here too. We will perform statistical inference in two steps. First, for each outer split we do permutation testing based on out-of-sample correlation. Second, to infer if the associative effect is significant across splits, omnibus hypothesis is used, which tests if any outer split is significant after adjusting the threshold with Bonferroni correction (e.g., p=0.01 in case of 5 splits). This approach is based on Monteiro et al. 2016 . For quicker results, we set the number of permutations to 100, however, we recommend using at least 1000 permutations in general. Matlab % Statistical inference settings cfg . stat . nperm = 100 ; For further details on the statistical inference, see here .","title":"Statistical inference"},{"location":"mfiles/example_fmri/#run-analysis","text":"To run the analysis, we simply update our cfg structure to add all necessary default values that we did not explicitly define and then run the main function. After the analysis, we clean up all the duplicate and intermediate files to save disc space. Note that if you run the analysis in a cluster environment then you will need to comment out the last line and run it manually once the analysis is completed as the cleanup_files function does not work in a parallel environment. Matlab % Update cfg with defaults cfg = cfg_defaults ( cfg ); % Run analysis main ( cfg ); % Clean up analysis files to save disc space cleanup_files ( cfg );","title":"Run Analysis"},{"location":"mfiles/example_fmri/#visualization","text":"Now that we have run our first analysis, let's plot some of the results. Before we can do any plotting, we need to make sure that we have called set_path('plot') to add the plotting folder. Then we load the res structure. In general, we advise you to plot your results on a local computer as it is often cumbersome and slow in a cluster environment. If you move your results from a cluster to a local computer, you need update the paths in your cfg*.mat and res*.mat files using update_dir . This should be called once the res structure is loaded either manually or by res_defaults . Matlab %----- Visualization % Set path for plotting and the BrainNet Viewer toolbox set_path ( 'plot' , 'brainnet' ); % Load res res . dir . frwork = cfg . dir . frwork ; res . frwork . level = 1 ; res . gen . selectfile = 'none' ; res . gen . weight . flip = 1 ; res = res_defaults ( res , 'load' );","title":"Visualization"},{"location":"mfiles/example_fmri/#plot-projections","text":"To plot the data projections (or latent variables) that has been learnt by the model, simply run plot_proj . As first argument, we need to pass the res structure. Then, we specify the data modalities as cell array and the level of associative effect. In this example, we plot the projections of X and Y for the first associative effect. We set the fourth input parameter to 'osplit' so that the training and test data of the outer split will be used for the plot. The following argument defines the outer data split we want to use (in this demo, we have only one split). We use the second to last argument to specify the colour-coding of the data using the training and test data as groups ( teid ). Finally, we specify the low-level function that will plot the results. In this case it is plot_proj_2d_group . Please see the documentation of plot_proj for more details. Matlab % Plot data projections plot_proj ( res , { 'X' 'Y' }, res . frwork . level , 'osplit' , ... res . frwork . split . best , 'training+test' , '2d_group' , 'gen.axes.FontSize' , 20 , ... 'gen.legend.FontSize' , 20 , 'gen.legend.Location' , 'NorthWest' , ... 'proj.scatter.SizeData' , 120 , 'proj.scatter.MarkerEdgeColor' , 'k' , ... 'proj.scatter.MarkerFaceColor' , [ 0.3 0.3 0.9 ; 0.9 0.3 0.3 ]);","title":"Plot projections"},{"location":"mfiles/example_fmri/#plot-weights","text":"Plotting model weights heavily depends on the kind of data that has been used in the analysis. In case of our fake functional MRI connectivity data, we will plot the weights as edges on a glass brain. We will use only the top 20 most positive and top 20 most negative weights for the figure. We set this by first sorting the weights by their sign ( roi.weight.sorttype = sign ) then taking the top 20 from both ends ( roi.weight.numtop = 20 ). In case of our fake behavioural data, we will plot the weights as a vertical bar plot, again using only the top 20 most positive and top 20 most negative weights. As first argument, we need to pass the res function, in which we define our custom processing for the weights. Next, we specify the data modality and the type of the modality as strings. In this example, we use brain connectivity and behavioural data, so we set these to X and conn for one and Y and behav for the other. The following argument defines the outer data split we want to use. Finally, we specify the low-level function that will plot the results. In this example, it will be plot_weight_brain_edge and plot_weight_behav_vert . Please see the documentation of plot_weight for more details. Matlab % Plot connectivity weights on glass brain plot_weight ( res , 'X' , 'conn' , res . frwork . split . best , 'brain_edge' , ... 'conn.weight.sorttype' , 'sign' , 'conn.weight.numtop' , 20 ); Matlab % Plot behavioural weights as vertical bar plot plot_weight ( res , 'Y' , 'behav' , res . frwork . split . best , 'behav_vert' , ... 'gen.axes.FontSize' , 20 , 'gen.legend.FontSize' , 20 , ... 'gen.axes.YLim' , [ - 0.004 0.013 ], ... 'behav.weight.sorttype' , 'sign' , 'behav.weight.numtop' , 20 );","title":"Plot weights"},{"location":"mfiles/example_simulation/","text":"example_simulation This is a demo for simulated data. We will discuss step by step how to generate data, run an analysis as well as how to visualize the results. Copy and paste the code chunks into a function to create your own experiment or copy the function from the examples folder of the toolkit. Generate data First, we generate the simulated data using the generate_data function of the toolkit. We will use 1000 examples and 100 features in both data modalities. We set the signal to be sparse with 10% of the features in each modality that are relevant to capture the association across modalities. The noise parameter of the model is set to 1. For further details on the generative model, see Mihalik et al. 2022 . Matlab %----- Generate data % Data folder data_dir = fullfile ( fileparts ( mfilename ( 'fullpath' )), 'example_simulation' , 'data' ); if ~ exist ( fullfile ( data_dir , 'X.mat' ), 'file' ) || ... ~ exist ( fullfile ( data_dir , 'Y.mat' ), 'file' ) % Generate simulated data [ X , Y , wX , wY ] = generate_data ( 1000 , 100 , 100 , 10 , 10 , 1 ); % Save simulated data and true model weights if ~ isfolder ( 'data_dir' ) mkdir ( data_dir ); end save ( fullfile ( data_dir , 'X.mat' ), 'X' ); save ( fullfile ( data_dir , 'Y.mat' ), 'Y' ); save ( fullfile ( data_dir , 'wX.mat' ), 'wX' ); save ( fullfile ( data_dir , 'wY.mat' ), 'wY' ); end Analysis Now we are ready to set up the analysis. We start by running set_path to add the necessary paths of the toolkit to your MATLAB path. Matlab %----- Analysis % Set path for analysis set_path ; Project folder Next, we specify the folder to our project. Make sure to specify the correct path. We recommend to use a full path, but a relative path should also work. Matlab % Project folder cfg . dir . project = fullfile ( fileparts ( data_dir )); Machine Now, we configure the CCA/PLS model we would like to use. We set machine.name to spls for Sparse PLS . To select the best hyperparameter (L1 regularization for SPLS), we will use generalizability (measured as average out-of-sample corretion on the validation sets) and stability (measured as the avarage similarity of weights across the inner training sets) as a joint optimization criterion. This is set by machine.param.crit = correl+simwxy . For further details on this criterion, see Mihalik et al. (2020) . Matlab % Machine settings cfg . machine . name = 'spls' ; cfg . machine . param . crit = 'correl+simwxy' ; For more information on the CCA/PLS models and the hyperparameter choices, see here . Framework Next, we set the framework name to holdout and the number of outer data splits to 1 to perform a single holdout approach. Matlab % Framework settings cfg . frwork . name = 'holdout' ; cfg . frwork . split . nout = 1 ; For further details on the framework choices, see here . Deflation Next, we set the deflation of SPLS. We will use PLS-mode A deflation. Matlab % Deflation settings cfg . defl . name = 'pls-modeA' ; For further details on the deflation choices, see here . Environment Next, we set the computational environment for the toolkit. As our data is relatively low-dimensional (i.e., number of features is not too high) SPLS will run quiclky locally on our computer. Matlab % Environment settings cfg . env . comp = 'local' ; For further details on the environmental settings, see here . Statistical inference Finally, we need to define how the statistical inference is performed. For quicker results, we set the number of permutations to 100, however, we recommend using at least 1000 permutations in general. Matlab % Statistical inference settings cfg . stat . nperm = 100 ; For further details on the statistical inference, see here . Run analysis To run the analysis, we simply update our cfg structure to add all necessary default values that we did not explicitly define and then run the main function. After the analysis, we clean up all the duplicate and intermediate files to save disc space. Note that if you run the analysis in a cluster environment then you will need to comment out the last line and run it manually once the analysis is completed as the cleanup_files function does not work in a parallel environment. Matlab % Update cfg with defaults cfg = cfg_defaults ( cfg ); % Run analysis main ( cfg ); % Clean up analysis files to save disc space cleanup_files ( cfg ); Visualization Now that we have run our first analysis, let's plot some of the results. Before we can do any plotting, we need to make sure that we have called set_path('plot') to add the plotting folder. Then we load the res structure. In general, we advise you to plot your results on a local computer as it is often cumbersome and slow in a cluster environment. If you move your results from a cluster to a local computer, you need update the paths in your cfg*.mat and res*.mat files using update_dir . This should be called once the res structure is loaded either manually or by res_defaults . Matlab %----- Visualization % Set path for plotting set_path ( 'plot' ); % Load res res . dir . frwork = cfg . dir . frwork ; res . frwork . level = 1 ; res = res_defaults ( res , 'load' ); Plot grid search results First, we plot the grid search results of the hyperparameter optimization. As first argument, we need to pass the res structure. Then we specify the data modality as string. The last argument is a varargin to define an optional number of metrics. Each metric will be plotted as a function of the hyperparameter grid and in a separate subplot. In this example, we plot the test (out-of-sample) correlation and the joint generalizability-stability criterion ( dist2 ), which was used for selecting the best hyperparameter. For more details, see Mihalik et al. (2020) . Matlab % Plot hyperparameter surface for grid search results plot_paropt ( res , 1 , { 'correl' , 'simwx' , 'simwy' }, ... 'gen.figure.Position' , [ 500 600 1200 400 ], 'gen.axes.FontSize' , 20 , ... 'gen.axes.XScale' , 'log' , 'gen.axes.YScale' , 'log' ); Plot projections To plot the data projections (or latent variables) that has been learnt by the model, simply run plot_proj . As first argument, we need to pass the res structure, in which we define a custom xlabel and ylabel . Then, we specify the data modalities as cell array and the level of associative effect. In this example, we plot the projections of X and Y for the first associative effect. We set the fourth input parameter to 'osplit' so that the training and test data of the outer split will be used for the plot. The following argument defines the outer data split we want to use (in this demo, we have only one split). We use the second to last argument to specify the colour-coding of the data using the training and test data as groups ( teid ). Finally, we specify the low-level function that will plot the results. In this case it is plot_proj_2d_group . Please see the documentation of plot_proj for more details. Matlab % Plot data projections plot_proj ( res , { 'X' 'Y' }, res . frwork . level , 'osplit' , 1 , ... 'training+test' , '2d_group' , 'gen.axes.FontSize' , 20 , ... 'gen.legend.FontSize' , 20 , 'gen.legend.Location' , 'NorthWest' , ... 'proj.scatter.SizeData' , 120 , 'proj.scatter.MarkerEdgeColor' , 'k' , ... 'proj.scatter.MarkerFaceColor' , [ 0.3 0.3 0.9 ; 0.9 0.3 0.3 ], ... 'proj.xlabel' , 'Modality 1 latent variable' , ... 'proj.ylabel' , 'Modality 2 latent variable' ); Plot weights Plotting model weights heavily depends on the kind of data that has been used in the analysis. In case of our simulated data, we are interested if the model can recover the weights that were used for generating the data (these true model weights were automatically saved in our data folder as wx.mat and wy.mat ). We we use a stem plot with the recovered weights in blue, and the true weights in red. Again, we use a costum xlabel in the figures. As first argument, we need to pass the res function, in which we define our custom xlabel for the figures. Next, we specify the data modality and the type of the modality as strings. In this example, we set these to X or Y and simul . The following argument defines the outer data split we want to use. Finally, we specify the low-level function that will plot the results. In this example, it will be plot_weight_stem . Please see the documentation of plot_weight for more details. Matlab % Plot modality 1 weights as stem plot plot_weight ( res , 'X' , 'simul' , 1 , 'stem' , ... 'gen.axes.FontSize' , 20 , 'gen.legend.FontSize' , 20 , ... 'gen.axes.YLim' , [ - 0.2 1.2 ], 'simul.weight.norm' , 'minmax' , ... 'simul.xlabel' , 'Modality 1 variables' ); Matlab % Plot modality 2 weights as stem plot plot_weight ( res , 'Y' , 'simul' , 1 , 'stem' , ... 'gen.axes.FontSize' , 20 , 'gen.legend.FontSize' , 20 , ... 'gen.axes.YLim' , [ - 0.2 1.2 ], 'simul.weight.norm' , 'minmax' , ... 'simul.xlabel' , 'Modality 2 variables' );","title":"Simulated data"},{"location":"mfiles/example_simulation/#generate-data","text":"First, we generate the simulated data using the generate_data function of the toolkit. We will use 1000 examples and 100 features in both data modalities. We set the signal to be sparse with 10% of the features in each modality that are relevant to capture the association across modalities. The noise parameter of the model is set to 1. For further details on the generative model, see Mihalik et al. 2022 . Matlab %----- Generate data % Data folder data_dir = fullfile ( fileparts ( mfilename ( 'fullpath' )), 'example_simulation' , 'data' ); if ~ exist ( fullfile ( data_dir , 'X.mat' ), 'file' ) || ... ~ exist ( fullfile ( data_dir , 'Y.mat' ), 'file' ) % Generate simulated data [ X , Y , wX , wY ] = generate_data ( 1000 , 100 , 100 , 10 , 10 , 1 ); % Save simulated data and true model weights if ~ isfolder ( 'data_dir' ) mkdir ( data_dir ); end save ( fullfile ( data_dir , 'X.mat' ), 'X' ); save ( fullfile ( data_dir , 'Y.mat' ), 'Y' ); save ( fullfile ( data_dir , 'wX.mat' ), 'wX' ); save ( fullfile ( data_dir , 'wY.mat' ), 'wY' ); end","title":"Generate data"},{"location":"mfiles/example_simulation/#analysis","text":"Now we are ready to set up the analysis. We start by running set_path to add the necessary paths of the toolkit to your MATLAB path. Matlab %----- Analysis % Set path for analysis set_path ;","title":"Analysis"},{"location":"mfiles/example_simulation/#project-folder","text":"Next, we specify the folder to our project. Make sure to specify the correct path. We recommend to use a full path, but a relative path should also work. Matlab % Project folder cfg . dir . project = fullfile ( fileparts ( data_dir ));","title":"Project folder"},{"location":"mfiles/example_simulation/#machine","text":"Now, we configure the CCA/PLS model we would like to use. We set machine.name to spls for Sparse PLS . To select the best hyperparameter (L1 regularization for SPLS), we will use generalizability (measured as average out-of-sample corretion on the validation sets) and stability (measured as the avarage similarity of weights across the inner training sets) as a joint optimization criterion. This is set by machine.param.crit = correl+simwxy . For further details on this criterion, see Mihalik et al. (2020) . Matlab % Machine settings cfg . machine . name = 'spls' ; cfg . machine . param . crit = 'correl+simwxy' ; For more information on the CCA/PLS models and the hyperparameter choices, see here .","title":"Machine"},{"location":"mfiles/example_simulation/#framework","text":"Next, we set the framework name to holdout and the number of outer data splits to 1 to perform a single holdout approach. Matlab % Framework settings cfg . frwork . name = 'holdout' ; cfg . frwork . split . nout = 1 ; For further details on the framework choices, see here .","title":"Framework"},{"location":"mfiles/example_simulation/#deflation","text":"Next, we set the deflation of SPLS. We will use PLS-mode A deflation. Matlab % Deflation settings cfg . defl . name = 'pls-modeA' ; For further details on the deflation choices, see here .","title":"Deflation"},{"location":"mfiles/example_simulation/#environment","text":"Next, we set the computational environment for the toolkit. As our data is relatively low-dimensional (i.e., number of features is not too high) SPLS will run quiclky locally on our computer. Matlab % Environment settings cfg . env . comp = 'local' ; For further details on the environmental settings, see here .","title":"Environment"},{"location":"mfiles/example_simulation/#statistical-inference","text":"Finally, we need to define how the statistical inference is performed. For quicker results, we set the number of permutations to 100, however, we recommend using at least 1000 permutations in general. Matlab % Statistical inference settings cfg . stat . nperm = 100 ; For further details on the statistical inference, see here .","title":"Statistical inference"},{"location":"mfiles/example_simulation/#run-analysis","text":"To run the analysis, we simply update our cfg structure to add all necessary default values that we did not explicitly define and then run the main function. After the analysis, we clean up all the duplicate and intermediate files to save disc space. Note that if you run the analysis in a cluster environment then you will need to comment out the last line and run it manually once the analysis is completed as the cleanup_files function does not work in a parallel environment. Matlab % Update cfg with defaults cfg = cfg_defaults ( cfg ); % Run analysis main ( cfg ); % Clean up analysis files to save disc space cleanup_files ( cfg );","title":"Run analysis"},{"location":"mfiles/example_simulation/#visualization","text":"Now that we have run our first analysis, let's plot some of the results. Before we can do any plotting, we need to make sure that we have called set_path('plot') to add the plotting folder. Then we load the res structure. In general, we advise you to plot your results on a local computer as it is often cumbersome and slow in a cluster environment. If you move your results from a cluster to a local computer, you need update the paths in your cfg*.mat and res*.mat files using update_dir . This should be called once the res structure is loaded either manually or by res_defaults . Matlab %----- Visualization % Set path for plotting set_path ( 'plot' ); % Load res res . dir . frwork = cfg . dir . frwork ; res . frwork . level = 1 ; res = res_defaults ( res , 'load' );","title":"Visualization"},{"location":"mfiles/example_simulation/#plot-grid-search-results","text":"First, we plot the grid search results of the hyperparameter optimization. As first argument, we need to pass the res structure. Then we specify the data modality as string. The last argument is a varargin to define an optional number of metrics. Each metric will be plotted as a function of the hyperparameter grid and in a separate subplot. In this example, we plot the test (out-of-sample) correlation and the joint generalizability-stability criterion ( dist2 ), which was used for selecting the best hyperparameter. For more details, see Mihalik et al. (2020) . Matlab % Plot hyperparameter surface for grid search results plot_paropt ( res , 1 , { 'correl' , 'simwx' , 'simwy' }, ... 'gen.figure.Position' , [ 500 600 1200 400 ], 'gen.axes.FontSize' , 20 , ... 'gen.axes.XScale' , 'log' , 'gen.axes.YScale' , 'log' );","title":"Plot grid search results"},{"location":"mfiles/example_simulation/#plot-projections","text":"To plot the data projections (or latent variables) that has been learnt by the model, simply run plot_proj . As first argument, we need to pass the res structure, in which we define a custom xlabel and ylabel . Then, we specify the data modalities as cell array and the level of associative effect. In this example, we plot the projections of X and Y for the first associative effect. We set the fourth input parameter to 'osplit' so that the training and test data of the outer split will be used for the plot. The following argument defines the outer data split we want to use (in this demo, we have only one split). We use the second to last argument to specify the colour-coding of the data using the training and test data as groups ( teid ). Finally, we specify the low-level function that will plot the results. In this case it is plot_proj_2d_group . Please see the documentation of plot_proj for more details. Matlab % Plot data projections plot_proj ( res , { 'X' 'Y' }, res . frwork . level , 'osplit' , 1 , ... 'training+test' , '2d_group' , 'gen.axes.FontSize' , 20 , ... 'gen.legend.FontSize' , 20 , 'gen.legend.Location' , 'NorthWest' , ... 'proj.scatter.SizeData' , 120 , 'proj.scatter.MarkerEdgeColor' , 'k' , ... 'proj.scatter.MarkerFaceColor' , [ 0.3 0.3 0.9 ; 0.9 0.3 0.3 ], ... 'proj.xlabel' , 'Modality 1 latent variable' , ... 'proj.ylabel' , 'Modality 2 latent variable' );","title":"Plot projections"},{"location":"mfiles/example_simulation/#plot-weights","text":"Plotting model weights heavily depends on the kind of data that has been used in the analysis. In case of our simulated data, we are interested if the model can recover the weights that were used for generating the data (these true model weights were automatically saved in our data folder as wx.mat and wy.mat ). We we use a stem plot with the recovered weights in blue, and the true weights in red. Again, we use a costum xlabel in the figures. As first argument, we need to pass the res function, in which we define our custom xlabel for the figures. Next, we specify the data modality and the type of the modality as strings. In this example, we set these to X or Y and simul . The following argument defines the outer data split we want to use. Finally, we specify the low-level function that will plot the results. In this example, it will be plot_weight_stem . Please see the documentation of plot_weight for more details. Matlab % Plot modality 1 weights as stem plot plot_weight ( res , 'X' , 'simul' , 1 , 'stem' , ... 'gen.axes.FontSize' , 20 , 'gen.legend.FontSize' , 20 , ... 'gen.axes.YLim' , [ - 0.2 1.2 ], 'simul.weight.norm' , 'minmax' , ... 'simul.xlabel' , 'Modality 1 variables' ); Matlab % Plot modality 2 weights as stem plot plot_weight ( res , 'Y' , 'simul' , 1 , 'stem' , ... 'gen.axes.FontSize' , 20 , 'gen.legend.FontSize' , 20 , ... 'gen.axes.YLim' , [ - 0.2 1.2 ], 'simul.weight.norm' , 'minmax' , ... 'simul.xlabel' , 'Modality 2 variables' );","title":"Plot weights"},{"location":"mfiles/example_smri/","text":"example_smri This is a demo for simulated sMRI data. We will discuss step by step how to generate data, run an analysis as well as how to visualize the results. Copy and paste the code chunks into a function to create your own experiment or copy the function from the examples folder of the toolkit. Generate data First, we generate the simulated data using the generate_data function of the toolkit. We will use 1000 examples, 120 features in the brain modality and 100 features in the behavioural modality. We set the signal to be sparse with 10% of the features in each modality that are relevant to capture the association across modalities. The noise parameter of the model is set to 1. For further details on the generative model, see Mihalik et al. 2022 . Matlab %----- Generate data % Data folder data_dir = fullfile ( fileparts ( mfilename ( 'fullpath' )), 'example_smri' , 'data' ); if ~ exist ( fullfile ( data_dir , 'X.mat' ), 'file' ) || ... ~ exist ( fullfile ( data_dir , 'Y.mat' ), 'file' ) % Generate simulated sMRI data [ X , Y , wX , wY ] = generate_data ( 1000 , 120 , 100 , 10 , 10 , 1 ); % Save simulated data and true model weights if ~ isfolder ( 'data_dir' ) mkdir ( data_dir ); end save ( fullfile ( data_dir , 'X.mat' ), 'X' ); save ( fullfile ( data_dir , 'Y.mat' ), 'Y' ); save ( fullfile ( data_dir , 'wX.mat' ), 'wX' ); save ( fullfile ( data_dir , 'wY.mat' ), 'wY' ); end Create label files As we use simulated sMRI and behavioural data, we need to create label files. For the brain features, we will use the AAL atlas to create 120 regions-level labels. For our behavioural label file, we will simply use indexes for Label and 2 domains as Category . Matlab % Add the AAL2 (https://www.gin.cnrs.fr/en/tools/aal/) and the BrainNet % Viewer (https://www.nitrc.org/projects/bnv/) toolboxes to the path set_path ( 'aal' , 'brainnet' ); % Create AAL labels for simulated sMRI data if ~ exist ( fullfile ( data_dir , 'LabelsX.csv' ), 'file' ) BrainNet_GenCoord ( which ( 'AAL2.nii' ), 'AAL.txt' ); T = readtable ( 'AAL.txt' ); nROI = size ( T , 1 ); T . Properties . VariableNames ([ 1 : 3 6 ]) = { 'X' 'Y' 'Z' 'Index' }; T . Label = [ 1 : nROI ] ' ; writetable ( T (:,[ 1 : 3 6 : 7 ]), fullfile ( data_dir , 'LabelsX.csv' )); delete AAL.txt ; end % Create labels for fake behavioural data if ~ exist ( fullfile ( data_dir , 'LabelsY.csv' ), 'file' ) T = table ([ 1 : 100 ] ' , [ repmat ({ 'Domain 1' }, 50 , 1 ); repmat ({ 'Domain 2' }, 50 , 1 )], ... 'VariableNames' , { 'Label' 'Category' }); writetable ( T , fullfile ( data_dir , 'LabelsY.csv' )); end Analysis First, run set_path to add the necessary paths of the toolkit to your MATLAB path. Matlab %----- Analysis % Set path for analysis set_path ; Project folder Next, we specify the folder to our project. Since we will use the toolkit to generate fake structural MRI data, we do not need to provide input data ( X.mat and Y.mat ). Make sure to specify the correct path. We recommend to use a full path, but a relative path should also work. Matlab % Project folder cfg . dir . project = fullfile ( fileparts ( data_dir )); Machine Now, we configure the CCA/PLS model we would like to use. We set machine.name to rcca for Regularized CCA . To select the best hyperparameter (L2 regularization for RCCA), we will use a generalizability (measured as average out-of-sample corretion on the validation sets) criterion. This is set by machine.param.crit = 'correl' . Matlab % Machine settings cfg . machine . name = 'rcca' ; For more information on the CCA/PLS models and the hyperparameter choices, see here . Framework Next, we set the framework name to holdout and the number of outer data splits to 1 to perform a single holdout approach. The frwork.flag field defines a custom name for this analysis. Make sure to give it a name that will help you organize different analyses you might run on your data. Matlab % Framework settings cfg . frwork . name = 'holdout' ; cfg . frwork . split . nout = 1 ; For further details on the framework choices, see here . Environment Next, we set the computational environment for the toolkit. As our RCCA implementation is computationally efficient, most of the times we can run it locally on our computer. Matlab % Environment settings cfg . env . comp = 'local' ; For further details on the environmental settings, see here . Statistical inference Finally, we need to define how the significance testing is performed. For quicker results, we set the number of permutations to 100, however, we recommend using at least 1000 permutations in general. Matlab % Statistical inference settings cfg . stat . nperm = 100 ; For further details on the statistical inference, see here . Run analysis To run the analysis, we simply update our cfg structure to add all necessary default values that we did not explicitly define and then run the main function. After the analysis, we clean up all the duplicate and intermediate files to save disc space. Matlab % Update cfg with defaults cfg = cfg_defaults ( cfg ); % Run analysis main ( cfg ); % Clean up analysis files to save disc space cleanup_files ( cfg ); Visualization Now that we have run our first analysis, let's plot some of the results. Before we can do any plotting, we need to make sure that we have called set_path('plot') to add the plotting folder. Then we load the res structure. In general, we advise you to plot your results on a local computer as it is often cumbersome and slow in a cluster environment. If you move your results from a cluster to a local computer, you need update the paths in your cfg*.mat and res*.mat files using update_dir . This should be called once the res structure is loaded either manually or by res_defaults . Matlab %----- Visualization % Set path for plotting and the BrainNet Viewer toolbox set_path ( 'plot' , 'brainnet' ); % Load res res . dir . frwork = cfg . dir . frwork ; res . frwork . level = 1 ; res . gen . selectfile = 'none' ; res = res_defaults ( res , 'load' ); Plot grid search results First, we plot the grid search results of the hyperparameter optimization. As first argument, we need to pass the res structure. Then we specify the data modality as string. The last argument is a varargin to define an optional number of metrics. Each metric will be plotted as a function of the hyperparameter grid and in a separate subplot. In this example, we plot the training (in-sample) and test (out-of-sample) correlations as metrics. Matlab plot_paropt ( res , 1 , { 'trcorrel' , 'correl' }, ... 'gen.figure.Position' , [ 500 600 800 400 ], 'gen.axes.FontSize' , 20 , ... 'gen.axes.XScale' , 'log' , 'gen.axes.YScale' , 'log' ); Plot projections To plot the data projections (or latent variables) that has been learnt by the model, simply run plot_proj . As first argument, we need to pass the res structure. Then, we specify the data modalities as cell array and the level of associative effect. In this example, we plot the projections of X and Y for the first associative effect. We set the fourth input parameter to 'osplit' so that the training and test data of the outer split will be used for the plot. The following argument defines the outer data split we want to use (in this demo, we have only one split). We use the second to last argument to specify the colour-coding of the data using the training and test data as groups ( teid ). Finally, we specify the low-level function that will plot the results. In this case it is plot_proj_2d_group . Please see the documentation of plot_proj for more details. Matlab % Plot data projections plot_proj ( res , { 'X' 'Y' }, res . frwork . level , 'osplit' , 1 , 'training+test' , ... '2d_group' , 'gen.axes.FontSize' , 20 , ... 'gen.legend.FontSize' , 20 , 'gen.legend.Location' , 'NorthWest' , ... 'proj.scatter.SizeData' , 120 , 'proj.scatter.MarkerEdgeColor' , 'k' , ... 'proj.scatter.MarkerFaceColor' , [ 0.3 0.3 0.9 ; 0.9 0.3 0.3 ]); Plot weights Plotting model weights heavily depends on the kind of data that has been used in the analysis. In case of our fake structural MRI data, we will plot the weights as nodes on a glass brain. We will use only the top 20 most positive and top 20 most negative weights for the figure. We set this by first sorting the weights by their sign ( roi.weight.sorttype = sign ) then taking the top 20 from both ends ( roi.weight.numtop = 20 ). In case of our fake behavioural data, we will plot the weights as a vertical bar plot, again using only the top 20 most positive and top 20 most negative weights. As first argument, we need to pass the res function, in which we define our custom processing for the weights. We also add the indexes of cerebellum (based on the label file) to the res settings as do not want to include cerebellum weights on the glass brain. Next, we specify the data modality and the type of the modality as strings. In this example, we use region-level brain and behavioural data, so we set these to X and roi for one and Y and behav for the other. The following argument defines the outer data split we want to use. Finally, we specify the low-level function that will plot the results. In this example, it will be plot_weight_brain_node and plot_weight_behav_vert . Please see the documentation of plot_weight for more details. Matlab % Plot ROI weights on a glass brain plot_weight ( res , 'X' , 'roi' , 1 , 'brain_node' , ... 'roi.weight.sorttype' , 'sign' , 'roi.weight.numtop' , 20 , ... 'roi.out' , 9000 + [ reshape ([ 1 : 10 : 81 ; 2 : 10 : 82 ], [], 1 ); ... reshape ( 100 : 10 : 170 , [], 1 )]); Matlab % Plot behavioural weights as vertical bar plot plot_weight ( res , 'Y' , 'behav' , 1 , 'behav_vert' , ... 'gen.axes.FontSize' , 20 , 'gen.legend.FontSize' , 20 , ... 'behav.weight.sorttype' , 'sign' , 'behav.weight.numtop' , 20 );","title":"Structural MRI data"},{"location":"mfiles/example_smri/#generate-data","text":"First, we generate the simulated data using the generate_data function of the toolkit. We will use 1000 examples, 120 features in the brain modality and 100 features in the behavioural modality. We set the signal to be sparse with 10% of the features in each modality that are relevant to capture the association across modalities. The noise parameter of the model is set to 1. For further details on the generative model, see Mihalik et al. 2022 . Matlab %----- Generate data % Data folder data_dir = fullfile ( fileparts ( mfilename ( 'fullpath' )), 'example_smri' , 'data' ); if ~ exist ( fullfile ( data_dir , 'X.mat' ), 'file' ) || ... ~ exist ( fullfile ( data_dir , 'Y.mat' ), 'file' ) % Generate simulated sMRI data [ X , Y , wX , wY ] = generate_data ( 1000 , 120 , 100 , 10 , 10 , 1 ); % Save simulated data and true model weights if ~ isfolder ( 'data_dir' ) mkdir ( data_dir ); end save ( fullfile ( data_dir , 'X.mat' ), 'X' ); save ( fullfile ( data_dir , 'Y.mat' ), 'Y' ); save ( fullfile ( data_dir , 'wX.mat' ), 'wX' ); save ( fullfile ( data_dir , 'wY.mat' ), 'wY' ); end","title":"Generate data"},{"location":"mfiles/example_smri/#create-label-files","text":"As we use simulated sMRI and behavioural data, we need to create label files. For the brain features, we will use the AAL atlas to create 120 regions-level labels. For our behavioural label file, we will simply use indexes for Label and 2 domains as Category . Matlab % Add the AAL2 (https://www.gin.cnrs.fr/en/tools/aal/) and the BrainNet % Viewer (https://www.nitrc.org/projects/bnv/) toolboxes to the path set_path ( 'aal' , 'brainnet' ); % Create AAL labels for simulated sMRI data if ~ exist ( fullfile ( data_dir , 'LabelsX.csv' ), 'file' ) BrainNet_GenCoord ( which ( 'AAL2.nii' ), 'AAL.txt' ); T = readtable ( 'AAL.txt' ); nROI = size ( T , 1 ); T . Properties . VariableNames ([ 1 : 3 6 ]) = { 'X' 'Y' 'Z' 'Index' }; T . Label = [ 1 : nROI ] ' ; writetable ( T (:,[ 1 : 3 6 : 7 ]), fullfile ( data_dir , 'LabelsX.csv' )); delete AAL.txt ; end % Create labels for fake behavioural data if ~ exist ( fullfile ( data_dir , 'LabelsY.csv' ), 'file' ) T = table ([ 1 : 100 ] ' , [ repmat ({ 'Domain 1' }, 50 , 1 ); repmat ({ 'Domain 2' }, 50 , 1 )], ... 'VariableNames' , { 'Label' 'Category' }); writetable ( T , fullfile ( data_dir , 'LabelsY.csv' )); end","title":"Create label files"},{"location":"mfiles/example_smri/#analysis","text":"First, run set_path to add the necessary paths of the toolkit to your MATLAB path. Matlab %----- Analysis % Set path for analysis set_path ;","title":"Analysis"},{"location":"mfiles/example_smri/#project-folder","text":"Next, we specify the folder to our project. Since we will use the toolkit to generate fake structural MRI data, we do not need to provide input data ( X.mat and Y.mat ). Make sure to specify the correct path. We recommend to use a full path, but a relative path should also work. Matlab % Project folder cfg . dir . project = fullfile ( fileparts ( data_dir ));","title":"Project folder"},{"location":"mfiles/example_smri/#machine","text":"Now, we configure the CCA/PLS model we would like to use. We set machine.name to rcca for Regularized CCA . To select the best hyperparameter (L2 regularization for RCCA), we will use a generalizability (measured as average out-of-sample corretion on the validation sets) criterion. This is set by machine.param.crit = 'correl' . Matlab % Machine settings cfg . machine . name = 'rcca' ; For more information on the CCA/PLS models and the hyperparameter choices, see here .","title":"Machine"},{"location":"mfiles/example_smri/#framework","text":"Next, we set the framework name to holdout and the number of outer data splits to 1 to perform a single holdout approach. The frwork.flag field defines a custom name for this analysis. Make sure to give it a name that will help you organize different analyses you might run on your data. Matlab % Framework settings cfg . frwork . name = 'holdout' ; cfg . frwork . split . nout = 1 ; For further details on the framework choices, see here .","title":"Framework"},{"location":"mfiles/example_smri/#environment","text":"Next, we set the computational environment for the toolkit. As our RCCA implementation is computationally efficient, most of the times we can run it locally on our computer. Matlab % Environment settings cfg . env . comp = 'local' ; For further details on the environmental settings, see here .","title":"Environment"},{"location":"mfiles/example_smri/#statistical-inference","text":"Finally, we need to define how the significance testing is performed. For quicker results, we set the number of permutations to 100, however, we recommend using at least 1000 permutations in general. Matlab % Statistical inference settings cfg . stat . nperm = 100 ; For further details on the statistical inference, see here .","title":"Statistical inference"},{"location":"mfiles/example_smri/#run-analysis","text":"To run the analysis, we simply update our cfg structure to add all necessary default values that we did not explicitly define and then run the main function. After the analysis, we clean up all the duplicate and intermediate files to save disc space. Matlab % Update cfg with defaults cfg = cfg_defaults ( cfg ); % Run analysis main ( cfg ); % Clean up analysis files to save disc space cleanup_files ( cfg );","title":"Run analysis"},{"location":"mfiles/example_smri/#visualization","text":"Now that we have run our first analysis, let's plot some of the results. Before we can do any plotting, we need to make sure that we have called set_path('plot') to add the plotting folder. Then we load the res structure. In general, we advise you to plot your results on a local computer as it is often cumbersome and slow in a cluster environment. If you move your results from a cluster to a local computer, you need update the paths in your cfg*.mat and res*.mat files using update_dir . This should be called once the res structure is loaded either manually or by res_defaults . Matlab %----- Visualization % Set path for plotting and the BrainNet Viewer toolbox set_path ( 'plot' , 'brainnet' ); % Load res res . dir . frwork = cfg . dir . frwork ; res . frwork . level = 1 ; res . gen . selectfile = 'none' ; res = res_defaults ( res , 'load' );","title":"Visualization"},{"location":"mfiles/example_smri/#plot-grid-search-results","text":"First, we plot the grid search results of the hyperparameter optimization. As first argument, we need to pass the res structure. Then we specify the data modality as string. The last argument is a varargin to define an optional number of metrics. Each metric will be plotted as a function of the hyperparameter grid and in a separate subplot. In this example, we plot the training (in-sample) and test (out-of-sample) correlations as metrics. Matlab plot_paropt ( res , 1 , { 'trcorrel' , 'correl' }, ... 'gen.figure.Position' , [ 500 600 800 400 ], 'gen.axes.FontSize' , 20 , ... 'gen.axes.XScale' , 'log' , 'gen.axes.YScale' , 'log' );","title":"Plot grid search results"},{"location":"mfiles/example_smri/#plot-projections","text":"To plot the data projections (or latent variables) that has been learnt by the model, simply run plot_proj . As first argument, we need to pass the res structure. Then, we specify the data modalities as cell array and the level of associative effect. In this example, we plot the projections of X and Y for the first associative effect. We set the fourth input parameter to 'osplit' so that the training and test data of the outer split will be used for the plot. The following argument defines the outer data split we want to use (in this demo, we have only one split). We use the second to last argument to specify the colour-coding of the data using the training and test data as groups ( teid ). Finally, we specify the low-level function that will plot the results. In this case it is plot_proj_2d_group . Please see the documentation of plot_proj for more details. Matlab % Plot data projections plot_proj ( res , { 'X' 'Y' }, res . frwork . level , 'osplit' , 1 , 'training+test' , ... '2d_group' , 'gen.axes.FontSize' , 20 , ... 'gen.legend.FontSize' , 20 , 'gen.legend.Location' , 'NorthWest' , ... 'proj.scatter.SizeData' , 120 , 'proj.scatter.MarkerEdgeColor' , 'k' , ... 'proj.scatter.MarkerFaceColor' , [ 0.3 0.3 0.9 ; 0.9 0.3 0.3 ]);","title":"Plot projections"},{"location":"mfiles/example_smri/#plot-weights","text":"Plotting model weights heavily depends on the kind of data that has been used in the analysis. In case of our fake structural MRI data, we will plot the weights as nodes on a glass brain. We will use only the top 20 most positive and top 20 most negative weights for the figure. We set this by first sorting the weights by their sign ( roi.weight.sorttype = sign ) then taking the top 20 from both ends ( roi.weight.numtop = 20 ). In case of our fake behavioural data, we will plot the weights as a vertical bar plot, again using only the top 20 most positive and top 20 most negative weights. As first argument, we need to pass the res function, in which we define our custom processing for the weights. We also add the indexes of cerebellum (based on the label file) to the res settings as do not want to include cerebellum weights on the glass brain. Next, we specify the data modality and the type of the modality as strings. In this example, we use region-level brain and behavioural data, so we set these to X and roi for one and Y and behav for the other. The following argument defines the outer data split we want to use. Finally, we specify the low-level function that will plot the results. In this example, it will be plot_weight_brain_node and plot_weight_behav_vert . Please see the documentation of plot_weight for more details. Matlab % Plot ROI weights on a glass brain plot_weight ( res , 'X' , 'roi' , 1 , 'brain_node' , ... 'roi.weight.sorttype' , 'sign' , 'roi.weight.numtop' , 20 , ... 'roi.out' , 9000 + [ reshape ([ 1 : 10 : 81 ; 2 : 10 : 82 ], [], 1 ); ... reshape ( 100 : 10 : 170 , [], 1 )]); Matlab % Plot behavioural weights as vertical bar plot plot_weight ( res , 'Y' , 'behav' , 1 , 'behav_vert' , ... 'gen.axes.FontSize' , 20 , 'gen.legend.FontSize' , 20 , ... 'behav.weight.sorttype' , 'sign' , 'behav.weight.numtop' , 20 );","title":"Plot weights"},{"location":"mfiles/generate_data/","text":"generate_data Generates data via a sparse latent variable model based on Witten et al. 2009 . The generated data has two modalities with nexamples samples and nfeatx and nfeaty variables, respectively. The activex and activey inputs define the number of variables in the two data modalities that are associated with a Gaussian latent variable. Syntax [X, Y, wX, wY] = generate_data(nexamples, nfeatx, nfeaty, activex, activey, noise) Inputs nexamples [ int ] number of examples in generated data nfeatx [ int ] number of features in generated data \\(\\mathbf{X}\\) nfeaty [ int ] number of features in generated data \\(\\mathbf{Y}\\) activex [ int ] number of active features in generated data \\(\\mathbf{X}\\) associated with the latent variable activey [ int ] number of active features in generated data \\(\\mathbf{Y}\\) associated with the latent variable noise [ float ] noise level in the generative model Outputs X [ 2D numeric array ] generated data \\(\\mathbf{X}\\) with nexamples rows and nfeatx columns Y [ 2D numeric array ] generated data \\(\\mathbf{Y}\\) with nexamples rows and nfeaty columns wX [ numeric array ] true weights used to generate data \\(\\mathbf{X}\\) from the latent variable, which has activex non-zero values wY [ numeric array ] true weights used to generate data \\(\\mathbf{Y}\\) from the latent variable, which has activey non-zero values Examples % Example 1 [ X , Y , wX , wY ] = generate_data ( 1000 , 100 , 100 , 10 , 10 , 1 );","title":"generate_data"},{"location":"mfiles/generate_data/#syntax","text":"[X, Y, wX, wY] = generate_data(nexamples, nfeatx, nfeaty, activex, activey, noise)","title":"Syntax"},{"location":"mfiles/generate_data/#inputs","text":"nexamples [ int ] number of examples in generated data nfeatx [ int ] number of features in generated data \\(\\mathbf{X}\\) nfeaty [ int ] number of features in generated data \\(\\mathbf{Y}\\) activex [ int ] number of active features in generated data \\(\\mathbf{X}\\) associated with the latent variable activey [ int ] number of active features in generated data \\(\\mathbf{Y}\\) associated with the latent variable noise [ float ] noise level in the generative model","title":"Inputs"},{"location":"mfiles/generate_data/#outputs","text":"X [ 2D numeric array ] generated data \\(\\mathbf{X}\\) with nexamples rows and nfeatx columns Y [ 2D numeric array ] generated data \\(\\mathbf{Y}\\) with nexamples rows and nfeaty columns wX [ numeric array ] true weights used to generate data \\(\\mathbf{X}\\) from the latent variable, which has activex non-zero values wY [ numeric array ] true weights used to generate data \\(\\mathbf{Y}\\) from the latent variable, which has activey non-zero values","title":"Outputs"},{"location":"mfiles/generate_data/#examples","text":"% Example 1 [ X , Y , wX , wY ] = generate_data ( 1000 , 100 , 100 , 10 , 10 , 1 );","title":"Examples"},{"location":"mfiles/plot_paropt/","text":"plot_paropt It plots the grid search results of the hyperparameter optimization. Syntax plot_paropt ( res , split , metrics , varargin ) Inputs res [ struct ] res structure containing information about results and plot specifications split [ int ] index of data split to be used metrics [ 'trcorrel', 'correl', 'trcovar', 'covar', 'trexvarx', 'exvarx', 'trexvary', 'exvary', 'simwx', 'simwy', 'simwxy', 'correl+simwxy' ] metrics to be plotted as a function of hyperparameter grid, each metric in a separate subplot varargin [ name-value pairs ] additional options can be passed via name-value pairs with dot notation supported Examples % Plot hyperparameter surface for grid search results plot_paropt ( res , 1 , { 'correl' , 'simwx' , 'simwy' }, ... 'gen.figure.Position' , [ 500 600 1200 400 ], 'gen.axes.FontSize' , 20 , ... 'gen.axes.XScale' , 'log' , 'gen.axes.YScale' , 'log' ); See also: plot_proj , plot_weight","title":"plot_paropt"},{"location":"mfiles/plot_paropt/#syntax","text":"plot_paropt ( res , split , metrics , varargin )","title":"Syntax"},{"location":"mfiles/plot_paropt/#inputs","text":"res [ struct ] res structure containing information about results and plot specifications split [ int ] index of data split to be used metrics [ 'trcorrel', 'correl', 'trcovar', 'covar', 'trexvarx', 'exvarx', 'trexvary', 'exvary', 'simwx', 'simwy', 'simwxy', 'correl+simwxy' ] metrics to be plotted as a function of hyperparameter grid, each metric in a separate subplot varargin [ name-value pairs ] additional options can be passed via name-value pairs with dot notation supported","title":"Inputs"},{"location":"mfiles/plot_paropt/#examples","text":"% Plot hyperparameter surface for grid search results plot_paropt ( res , 1 , { 'correl' , 'simwx' , 'simwy' }, ... 'gen.figure.Position' , [ 500 600 1200 400 ], 'gen.axes.FontSize' , 20 , ... 'gen.axes.XScale' , 'log' , 'gen.axes.YScale' , 'log' ); See also: plot_proj , plot_weight","title":"Examples"},{"location":"mfiles/plot_proj/","text":"plot_proj It plots the projections of the data (i.e., latent variables). Syntax plot_proj ( res , mod , level , sidvar , split , colour , func , varargin ) Inputs res [ struct ] res structure containing information about results and plot specifications mod [ cell array ] modality of data to be used for plotting (i.e., {'X', 'Y'} ) level [ int or numeric array ] level of associative effect with same dimensionality as mod or automatically extended (e.g., from int to numeric array) sidvar [ 'osplit', 'otrid', 'oteid', 'isplit', 'itrid', 'iteid' ] specifies subjects to be used for plotting first letter can be 'o' for outer or 'i' for inner split, followed by either 'trid' for training, 'teid' for test or 'split' for both training and test data split [ int or numeric array ] index of data split to be used with same dimensionality as mod or automatically extended (e.g., from int to numeric array) colour [ 'none', char ] 'none' for scatterplot with same colour for all subjects or it can be used as a continuous colormap or for colouring different groups; there are three ways to define the colour specify a variable, which can be loaded from a data file (e.g., Y.mat ) using the name of the variable defined in a label file (e.g., LabelsY.csv ) use 'training+test' to colour-code the training and test sets use any other string with a '+' sign (e.g., 'MDD+HC') to define the colour-code based on group.mat func [ '2d', '2d_group', '2d_cmap' ] name of the specific plotting function (after plot_proj_* prefix) to be called varargin [ name-value pairs ] additional options can be passed via name-value pairs with dot notation supported Examples Simple plots Most often, we plot a brain latent variable vs. a behavioural latent variable for a specific level (i.e., associative effect). % Plot data projections coloured by groups plot_proj ( res , { 'X' 'Y' }, res . frwork . level , 'osplit' , res . frwork . split . best , ... 'training+test' , '2d_group' , 'gen.axes.FontSize' , 20 , ... 'gen.legend.FontSize' , 20 , 'gen.legend.Location' , 'NorthWest' , ... 'proj.scatter.SizeData' , 120 , 'proj.scatter.MarkerEdgeColor' , 'k' , ... 'proj.scatter.MarkerFaceColor' , [ 0.3 0.3 0.9 ; 0.9 0.3 0.3 ]); Multi-level plots To plot projections aggregated over multiple levels, all you need to specify is res.proj.multi_level = 1 and provide a 2D cell array of input variable 'mod'. Input variables 'level' and 'split' should have the same dimensionality or they will be extended automatically from 1-D or 2-D arrays (e.g. level = repmat(level, size(mod))). % Plot data projections across levels (and averaged across modalities % in a level after standardization) plot_proj ( res , { 'X' 'Y' ; 'X' 'Y' }, [ 1 1 ; 2 2], 'osplit', res.frwork.split.best, ... 'none' , '2d' , 'proj.multi_label' , 1 ); See also: plot_paropt , plot_weight","title":"plot_proj"},{"location":"mfiles/plot_proj/#syntax","text":"plot_proj ( res , mod , level , sidvar , split , colour , func , varargin )","title":"Syntax"},{"location":"mfiles/plot_proj/#inputs","text":"res [ struct ] res structure containing information about results and plot specifications mod [ cell array ] modality of data to be used for plotting (i.e., {'X', 'Y'} ) level [ int or numeric array ] level of associative effect with same dimensionality as mod or automatically extended (e.g., from int to numeric array) sidvar [ 'osplit', 'otrid', 'oteid', 'isplit', 'itrid', 'iteid' ] specifies subjects to be used for plotting first letter can be 'o' for outer or 'i' for inner split, followed by either 'trid' for training, 'teid' for test or 'split' for both training and test data split [ int or numeric array ] index of data split to be used with same dimensionality as mod or automatically extended (e.g., from int to numeric array) colour [ 'none', char ] 'none' for scatterplot with same colour for all subjects or it can be used as a continuous colormap or for colouring different groups; there are three ways to define the colour specify a variable, which can be loaded from a data file (e.g., Y.mat ) using the name of the variable defined in a label file (e.g., LabelsY.csv ) use 'training+test' to colour-code the training and test sets use any other string with a '+' sign (e.g., 'MDD+HC') to define the colour-code based on group.mat func [ '2d', '2d_group', '2d_cmap' ] name of the specific plotting function (after plot_proj_* prefix) to be called varargin [ name-value pairs ] additional options can be passed via name-value pairs with dot notation supported","title":"Inputs"},{"location":"mfiles/plot_proj/#examples","text":"","title":"Examples"},{"location":"mfiles/plot_proj/#simple-plots","text":"Most often, we plot a brain latent variable vs. a behavioural latent variable for a specific level (i.e., associative effect). % Plot data projections coloured by groups plot_proj ( res , { 'X' 'Y' }, res . frwork . level , 'osplit' , res . frwork . split . best , ... 'training+test' , '2d_group' , 'gen.axes.FontSize' , 20 , ... 'gen.legend.FontSize' , 20 , 'gen.legend.Location' , 'NorthWest' , ... 'proj.scatter.SizeData' , 120 , 'proj.scatter.MarkerEdgeColor' , 'k' , ... 'proj.scatter.MarkerFaceColor' , [ 0.3 0.3 0.9 ; 0.9 0.3 0.3 ]);","title":"Simple plots"},{"location":"mfiles/plot_proj/#multi-level-plots","text":"To plot projections aggregated over multiple levels, all you need to specify is res.proj.multi_level = 1 and provide a 2D cell array of input variable 'mod'. Input variables 'level' and 'split' should have the same dimensionality or they will be extended automatically from 1-D or 2-D arrays (e.g. level = repmat(level, size(mod))). % Plot data projections across levels (and averaged across modalities % in a level after standardization) plot_proj ( res , { 'X' 'Y' ; 'X' 'Y' }, [ 1 1 ; 2 2], 'osplit', res.frwork.split.best, ... 'none' , '2d' , 'proj.multi_label' , 1 ); See also: plot_paropt , plot_weight","title":"Multi-level plots"},{"location":"mfiles/plot_weight/","text":"plot_weight It plots the model weights in specific figures based on the modality of the data. Syntax plot_weight ( res , mod , modtype , split , func , varargin ) Inputs res [ struct ] res structure containing information about results and plot specifications mod [ 'X', 'Y' ] modality of data to be used for plotting modtype [ 'behav', 'conn', 'vbm', 'roi', 'simul' ] type of data split [ int ] index of data split to be used func [ 'behav_horz', 'behav_vert', 'behav_text', 'brain_conn_node', 'brain_cortex', 'brain_edge', 'brain_module', 'brain_node', 'stem' ] name of the specific plotting function (after plot_weight_* prefix) to be called varargin [ name-value pairs ] additional options can be passed via name-value pairs with dot notation supported (e.g., 'behav.weight.numtop', 20) Examples Modality independent % Plot Y weights as stem plot plot_weight ( res , 'Y' , 'simul' , res . frwork . split . best , 'stem' , ... 'gen.axes.YLim' , [ - 0.2 1.2 ], 'simul.weight.norm' , 'minmax' , ... 'gen.axes.FontSize' , 20 , 'gen.legend.FontSize' , 20 ); Behaviour % Plot behavioural weights as vertical bar plot plot_weight ( res , 'Y' , 'behav' , res . frwork . split . best , 'behav_vert' , ... 'gen.axes.FontSize' , 20 , 'gen.legend.FontSize' , 20 , ... 'gen.axes.YLim' , [ - 0.4 1.2 ], 'gen.weight.flip' , 1 , ... 'behav.weight.sorttype' , 'sign' , 'behav.weight.numtop' , 20 , ... 'behav.weight.norm' , 'minmax' ); ROI-wise sMRI % Plot ROI weights on a glass brain plot_weight ( res , 'X' , 'roi' , 1 , 'brain_node' , ... 'roi.weight.sorttype' , 'sign' , 'roi.weight.numtop' , 20 , ... 'roi.out' , 9000 + [ reshape ([ 1 : 10 : 81 ; 2 : 10 : 82 ], [], 1 ); ... reshape ( 100 : 10 : 170 , [], 1 )]); fMRI connectivity edges % Plot connectivity weights on glass brain plot_weight ( res , 'X' , 'conn' , res . frwork . split . best , 'brain_edge' , ... 'conn.weight.sorttype' , 'sign' , 'conn.weight.numtop' , 20 ); See also: plot_paropt , plot_proj","title":"plot_weight"},{"location":"mfiles/plot_weight/#syntax","text":"plot_weight ( res , mod , modtype , split , func , varargin )","title":"Syntax"},{"location":"mfiles/plot_weight/#inputs","text":"res [ struct ] res structure containing information about results and plot specifications mod [ 'X', 'Y' ] modality of data to be used for plotting modtype [ 'behav', 'conn', 'vbm', 'roi', 'simul' ] type of data split [ int ] index of data split to be used func [ 'behav_horz', 'behav_vert', 'behav_text', 'brain_conn_node', 'brain_cortex', 'brain_edge', 'brain_module', 'brain_node', 'stem' ] name of the specific plotting function (after plot_weight_* prefix) to be called varargin [ name-value pairs ] additional options can be passed via name-value pairs with dot notation supported (e.g., 'behav.weight.numtop', 20)","title":"Inputs"},{"location":"mfiles/plot_weight/#examples","text":"","title":"Examples"},{"location":"mfiles/plot_weight/#modality-independent","text":"% Plot Y weights as stem plot plot_weight ( res , 'Y' , 'simul' , res . frwork . split . best , 'stem' , ... 'gen.axes.YLim' , [ - 0.2 1.2 ], 'simul.weight.norm' , 'minmax' , ... 'gen.axes.FontSize' , 20 , 'gen.legend.FontSize' , 20 );","title":"Modality independent"},{"location":"mfiles/plot_weight/#behaviour","text":"% Plot behavioural weights as vertical bar plot plot_weight ( res , 'Y' , 'behav' , res . frwork . split . best , 'behav_vert' , ... 'gen.axes.FontSize' , 20 , 'gen.legend.FontSize' , 20 , ... 'gen.axes.YLim' , [ - 0.4 1.2 ], 'gen.weight.flip' , 1 , ... 'behav.weight.sorttype' , 'sign' , 'behav.weight.numtop' , 20 , ... 'behav.weight.norm' , 'minmax' );","title":"Behaviour"},{"location":"mfiles/plot_weight/#roi-wise-smri","text":"% Plot ROI weights on a glass brain plot_weight ( res , 'X' , 'roi' , 1 , 'brain_node' , ... 'roi.weight.sorttype' , 'sign' , 'roi.weight.numtop' , 20 , ... 'roi.out' , 9000 + [ reshape ([ 1 : 10 : 81 ; 2 : 10 : 82 ], [], 1 ); ... reshape ( 100 : 10 : 170 , [], 1 )]);","title":"ROI-wise sMRI"},{"location":"mfiles/plot_weight/#fmri-connectivity-edges","text":"% Plot connectivity weights on glass brain plot_weight ( res , 'X' , 'conn' , res . frwork . split . best , 'brain_edge' , ... 'conn.weight.sorttype' , 'sign' , 'conn.weight.numtop' , 20 ); See also: plot_paropt , plot_proj","title":"fMRI connectivity edges"},{"location":"mfiles/res_defaults/","text":"res_defaults Set defaults in your results ( res ) structure including information about the results and settings for plotting. Use this function to update and add all necessary defaults to your res . If you have defined anything in res before calling the function, it won't overwrite those values. The path to the framework folder should be always defined in your res or passed as varargin, otherwise the function throws an error. All the other fields are optional and can be filled up by res_defaults . This function can be also called to load an existing res*.mat file. Syntax res = res_defaults ( res , mode , varargin ) Inputs res [ struct ] results structure (more information below) mode [ 'init', 'load', 'projection', 'simul', 'behav', 'conn', 'vbm', 'roi', 'brainnet' ] mode of calling res_defaults, either referring to initialization ('init'), loading ('load'), type of plot ('projection', 'simul', 'behav', 'conn', 'vbm', 'roi') or settings for toolbox ('brainnet') varargin [ name-value pairs ] additional parameters can be set via name-value pairs with dot notation supported (e.g., 'behav.weight.numtop', 20) Outputs res [ struct ] result structure that has been updated with defaults Examples % Example 1 res . dir . frwork = 'PATH/TO/YOUR/PROJECT/framework/ANALYSIS_NAME' ; res . frwork . level = 1 ; res . env . fileend = '_1' ; res = res_defaults ( res , 'load' ); % Example 2 res = res_defaults ([], 'load' , 'dir.frwork' , ... 'PATH/TO/YOUR/PROJECT/framework/ANALYSIS_NAME' ); % Example 3 res = res_defaults ([], 'load' , 'dir.frwork' , ... 'PATH/TO/YOUR/PROJECT/framework/ANALYSIS_NAME' ); res = res_defaults ( res , 'behav' ); See also: res , cfg_defaults","title":"res_defaults"},{"location":"mfiles/res_defaults/#syntax","text":"res = res_defaults ( res , mode , varargin )","title":"Syntax"},{"location":"mfiles/res_defaults/#inputs","text":"res [ struct ] results structure (more information below) mode [ 'init', 'load', 'projection', 'simul', 'behav', 'conn', 'vbm', 'roi', 'brainnet' ] mode of calling res_defaults, either referring to initialization ('init'), loading ('load'), type of plot ('projection', 'simul', 'behav', 'conn', 'vbm', 'roi') or settings for toolbox ('brainnet') varargin [ name-value pairs ] additional parameters can be set via name-value pairs with dot notation supported (e.g., 'behav.weight.numtop', 20)","title":"Inputs"},{"location":"mfiles/res_defaults/#outputs","text":"res [ struct ] result structure that has been updated with defaults","title":"Outputs"},{"location":"mfiles/res_defaults/#examples","text":"% Example 1 res . dir . frwork = 'PATH/TO/YOUR/PROJECT/framework/ANALYSIS_NAME' ; res . frwork . level = 1 ; res . env . fileend = '_1' ; res = res_defaults ( res , 'load' ); % Example 2 res = res_defaults ([], 'load' , 'dir.frwork' , ... 'PATH/TO/YOUR/PROJECT/framework/ANALYSIS_NAME' ); % Example 3 res = res_defaults ([], 'load' , 'dir.frwork' , ... 'PATH/TO/YOUR/PROJECT/framework/ANALYSIS_NAME' ); res = res_defaults ( res , 'behav' ); See also: res , cfg_defaults","title":"Examples"},{"location":"mfiles/set_path/","text":"set_path Adds essential folders to the path to initialize the toolkit for an analysis. For visualization, it needs to be called with specific folders to add plotting and other toolbox folders to the path. Syntax dir_toolkit = set_path ( varargin ) Inputs varargin [ char ] folders passed as arguments will be added to the path set_path looks for folders under the toolkit folder and under the external folder. In the latter case it is sufficient to use the first few characters of the toolbox, e.g., spm instead of spm12 . Outputs dir_toolkit [ char ] full path to the toolkit folder Examples % Example 1 set_path ; % Example 2 set_path ( 'plot' ); % Example 3 set_path ( 'plot' , 'spm' , 'brainnet' );","title":"set_path"},{"location":"mfiles/set_path/#syntax","text":"dir_toolkit = set_path ( varargin )","title":"Syntax"},{"location":"mfiles/set_path/#inputs","text":"varargin [ char ] folders passed as arguments will be added to the path set_path looks for folders under the toolkit folder and under the external folder. In the latter case it is sufficient to use the first few characters of the toolbox, e.g., spm instead of spm12 .","title":"Inputs"},{"location":"mfiles/set_path/#outputs","text":"dir_toolkit [ char ] full path to the toolkit folder","title":"Outputs"},{"location":"mfiles/set_path/#examples","text":"% Example 1 set_path ; % Example 2 set_path ( 'plot' ); % Example 3 set_path ( 'plot' , 'spm' , 'brainnet' );","title":"Examples"},{"location":"mfiles/update_dir/","text":"update_dir Updates the paths in cfg and all res files for the current computer. It is needed to run when switching between computers e.g., moving data from a cluster to a local computer. Syntax update_dir(dir_frwork, fileend) Inputs dir_frwork [ char ] full path to the specific framework folder fileend [ char ] suffix at the end of the res*.mat file from cfg.env.fileend Example % Example 1 update_dir ( < specific framework folder > , '_1' ); See also: cfg","title":"update_dir"},{"location":"mfiles/update_dir/#syntax","text":"update_dir(dir_frwork, fileend)","title":"Syntax"},{"location":"mfiles/update_dir/#inputs","text":"dir_frwork [ char ] full path to the specific framework folder fileend [ char ] suffix at the end of the res*.mat file from cfg.env.fileend","title":"Inputs"},{"location":"mfiles/update_dir/#example","text":"% Example 1 update_dir ( < specific framework folder > , '_1' ); See also: cfg","title":"Example"}]}